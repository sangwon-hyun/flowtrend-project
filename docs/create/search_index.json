[["index.html", "Creating the flowtrend R package 1 Introduction", " Creating the flowtrend R package Sangwon Hyun, Tim Coleman, Francois Ribalet, Jacob Bien 2025-07-23 1 Introduction This package implements flowtrend, a model used for smooth estimation of mixture models across time. The paper preprint is here: This bookdown uses literate programming to define the r params$package_name R package, which implements the procedures described in the paper. "],["overview.html", "2 Overview 2.1 Background", " 2 Overview 2.1 Background The documentation and package are both created using simple commands: ## To render with MINIMUM evaluation -- not running code, and only defining functions. litr::render(&quot;index.Rmd&quot;, output_format = litr::litr_gitbook(minimal_eval = TRUE), output_dir = &quot;docs/create&quot;) ## To render full evaluation -- running all code. litr::render(&quot;~/repos/flowtrend-project/index.Rmd&quot;, output_format = litr::litr_gitbook(minimal_eval = FALSE), output_dir = &quot;docs/create&quot;) ## Installing the package locally ## litr::render(&quot;index.Rmd&quot;, output_format = litr::litr_gitbook(minimal_eval = TRUE)) install.packages(&quot;~/repos/flowtrend-project/flowtrend&quot;, repos = NULL, type = &quot;source&quot;) ## Loading the packaging library(flowtrend) ## This is helpful for package building litr::load_all(&quot;index.Rmd&quot;) ## Somewhat equivalent to the above my_load &lt;- function(){ litr::render(&quot;~/repos/flowtrend/index.Rmd&quot;, output_format = litr::litr_gitbook(minimal_eval = TRUE)) devtools::load_all(&quot;~/repos/flowtrend/flowtrend&quot;) } my_load() "],["package-setup.html", "3 Package setup", " 3 Package setup The DESCRIPTION file is created using this code. usethis::create_package( path = &quot;.&quot;, fields = list( Package = params$package_name, Version = &quot;0.0.0.9000&quot;, Title = &quot;flowtrend&quot;, Description = &quot;Time-smooth mixture modeling for flow cytometry data (Hyun et al. 2025).&quot;, `Authors@R` = c( person( given = &quot;Sangwon&quot;, family = &quot;Hyun&quot;, email = &quot;sangwonh@ucsc.edu&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;)), person( given = &quot;Tim&quot;, family = &quot;Coleman&quot;, email = &quot;tscoleman226@gmail.com&quot;, role = c(&quot;aut&quot;) ), person( given = &quot;Francois&quot;, family = &quot;Ribalet&quot;, email = &quot;ribalet@uw.edu&quot;, role = c(&quot;aut&quot;) ), person( given = &quot;Jacob&quot;, family = &quot;Bien&quot;, email = &quot;jbien@usc.edu&quot;, role = c(&quot;aut&quot;) ) ) ) ) usethis::use_mit_license(copyright_holder = &quot;Sangwon Hyun&quot;) The following is what will show up when someone types package?flowtrend in the console. #&#39; flowtrend #&#39; #&#39; This package implements the `flowtrend` method for automatic gating of flow cytometry data using trend filtering. #&#39; It was proposed in &lt;https://arxiv.org/abs/2504.12287&gt;. To learn #&#39; more about this package, please visit its website #&#39; &lt;https://sangwon-hyun/flowtrend-project&gt;. #&#39; #&#39; @docType package This package will have some dependancies: library(tidyverse) library(ggplot2) usethis::use_package(&quot;tidyverse&quot;, type = &quot;depends&quot;) usethis::use_package(&quot;ggplot2&quot;) usethis::use_pipe() usethis::use_package(&quot;glmnet&quot;) usethis::use_package(&quot;clue&quot;) "],["generating-and-plotting-data.html", "4 Generating and plotting data 4.1 1d data 4.2 2d data 4.3 3d data", " 4 Generating and plotting data 4.1 1d data 4.1.1 Generating 1d data This function generates synthetic 1-dimensional data, and returns it in a “long” format matrix, with columns time, y, mu, and cluster. The latter two are the true underlying parameters. #&#39; Generates some synthetic 1-dimensional data with three clusters. Returns a #&#39; data frame with (1) time, (2) Y (3) mu (4) cluster assignments. #&#39; #&#39; @param TT Number of time points. #&#39; @param nt Number of particles at each time. #&#39; @param offset Defaults to 0. How much to push up cluster 1. #&#39; @param return_model If true, return true cluster means and probabilities #&#39; instead of data. #&#39; #&#39; @return long matrix with data or model. #&#39; @export gendat_1d &lt;- function(TT, ntlist, offset = 0, return_model = FALSE, sd3=NULL){ ## Basic checks stopifnot(length(ntlist) == TT) prob_link1 = rep(NA, TT) prob_link1[1] = 2 for(tt in 2:floor(TT/2)){ prob_link1[tt] = prob_link1[tt-1] + (1/TT) } for(tt in (floor(TT/2)+1):TT){ prob_link1[tt] = prob_link1[tt-1] - .5*(1/TT) } prob_link2 = sapply(1:TT, function(tt) 3 - 0.25*(tt/TT)) prob_link3 = sapply(1:TT, function(tt) 2.5) linkmat = cbind(prob_link1, prob_link2, prob_link3) ##%&gt;% matplot() mysum = exp(linkmat) %&gt;% rowSums() cluster_prob1 = exp(prob_link1) / mysum cluster_prob2 = exp(prob_link2) / mysum cluster_prob3 = exp(prob_link3) / mysum probs = cbind(cluster_prob1, cluster_prob2, cluster_prob3) colnames(probs) = 1:3 sd1 = 0.4 sd2 = 0.5 if(is.null(sd3)) sd3 = 0.35 ## if(!is.null(sd3)) sd3 = 1/1.5 ## The ratio of amplitude:data-standard-deviation is about ## ## $1.50695$. probs_long = as_tibble(probs) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) probs_long %&gt;% ggplot() + geom_line(aes(x=time,y=prob, group=cluster, col=cluster)) + ylim(c(0,1)) ## Make cluster means, by time means &lt;- matrix(NA, TT, 3) for(ii in 1:3){ for(tt in 1:TT){ means[tt, 1] &lt;- offset + tt/TT - 1.5 means[tt, 2] &lt;- sin(seq(-1, 1, length.out = TT)[tt] * 3.1415) means[tt, 3] &lt;- -3+sin(seq(-1, 1, length.out = TT)[tt] * 6.282) } } colnames(means) = c(1:3) means_long = as_tibble(means) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) model = full_join(means_long, probs_long, by = c(&quot;time&quot;, &quot;cluster&quot;)) model = model %&gt;% left_join(tibble(cluster = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;), sd = c(sd1, sd2, sd3)), by = &quot;cluster&quot;) ## Does this work? if(return_model) return(model) ys &lt;- lapply(1:TT, FUN = function(tt){ Y &lt;- vector(mode = &quot;list&quot;, length = 2) mu &lt;- vector(mode = &quot;list&quot;, length = 2) clusters_count &lt;- rmultinom(n = 1, size = ntlist[tt], prob = probs[tt,]) for(ii in 1:3){ if(ii == 1){ mn = means[tt,ii, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + rnorm(1, mean=0, sd = sd1)) mu[[ii]] &lt;- rep(mn, clusters_count[ii,1]) } if(ii == 2){ mn = means[tt,ii, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + rnorm(1, mean=0, sd = sd2)) mu[[ii]] &lt;- rep(mn, clusters_count[ii,1]) } if(ii == 3){ mn = means[tt,ii, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + rnorm(1, mean=0, sd = sd3)) mu[[ii]] &lt;- rep(mn, clusters_count[ii,1]) } } Y &lt;- unlist(Y) mu &lt;- unlist(mu) cluster &lt;- rep(1:3, times = clusters_count) one_df = tibble(time = tt, Y = Y, mu = mu, cluster = cluster) return(one_df) }) %&gt;% bind_rows() return(ys) } dt2ylist() is a helper that takes the output generated from gendat_1d(), and splits it by the time column to create a ylist object, which is a \\(T\\)-length list of \\(n_t \\times d\\) matrices. #&#39; Converting to a list of matrices, \\code{ylist}, to input to \\code{flowtrend()}. #&#39; #&#39; @param dt Output from \\code{gendat_1d()}. #&#39; #&#39; @return List of matrices #&#39; @export dt2ylist &lt;- function(dt){ dt%&gt;% dplyr::select(time, Y) %&gt;% arrange(time) %&gt;% group_by(time) %&gt;% group_split(.keep = FALSE) %&gt;% lapply(as.matrix) } Let’s generate some data using these functions. dt = gendat_1d(TT = 100, ntlist =rep(100,100)) print(dt) ## # A tibble: 10,000 × 4 ## time Y mu cluster ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 -1.41 -1.49 1 ## 2 1 -2.14 -1.49 1 ## 3 1 -1.25 -1.49 1 ## 4 1 -1.19 -1.49 1 ## 5 1 -1.53 -1.49 1 ## 6 1 -1.72 -1.49 1 ## 7 1 -1.00 -1.49 1 ## 8 1 -2.04 -1.49 1 ## 9 1 -2.14 -1.49 1 ## 10 1 -1.23 -1.49 1 ## # ℹ 9,990 more rows ylist = dt2ylist(dt) print(head(str(ylist[1:5]))) ## List of 5 ## $ : num [1:100, 1] -1.41 -2.14 -1.25 -1.19 -1.53 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;Y&quot; ## $ : num [1:100, 1] -1.915 -0.775 -1.49 -1.807 -1.587 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;Y&quot; ## $ : num [1:100, 1] -1.78 -1.42 -1.57 -1.47 -2.08 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;Y&quot; ## $ : num [1:100, 1] -1.15 -1.06 -1.25 -1.61 -1.09 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;Y&quot; ## $ : num [1:100, 1] -1.02 -1.104 -1.216 -1.504 -0.743 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;Y&quot; ## NULL print(head(ylist[[1]])) ## Y ## [1,] -1.406954 ## [2,] -2.141103 ## [3,] -1.249702 ## [4,] -1.188994 ## [5,] -1.529449 ## [6,] -1.719625 Next, we’ll make some plotting functions 1d model and data. 4.1.2 Plotting 1d data Given 1d data ylist and an estimated model flowtrend object obj, we want to visualize these in a single plot. plot_1d() lets you do this. #&#39; Makes 1d plot of data and model #&#39; #&#39; @param ylist Data. A list of (|nt| by |dimdat|) matrices #&#39; @param obj A flowtrend (or flowmix) object. Defaults to NULL. #&#39; @param x Time points. Defaults to NULL. #&#39; @param alpha Between 0 and 1, how transparent to plot the data #&#39; points. Defaults to 0.1. #&#39; @param bin If TRUE, the data is binned. #&#39; #&#39; @return ggplot object with data, and optionally, a flowtrend model overlaid. #&#39; @export plot_1d &lt;- function(ylist, countslist=NULL, obj=NULL, x = NULL, alpha = .1, bin = FALSE, plot_band = TRUE){ ## Basic checks if(!is.null(obj))stopifnot(class(obj) %in% c(&quot;flowmix&quot;, &quot;flowtrend&quot;)) if(!is.null(x)){ stopifnot(length(x) == length(ylist)) times = x } else { times = 1:length(ylist) } dimdat = ncol(ylist[[1]]) assertthat::assert_that(dimdat == 1) ## If countslist is not provided, make a dummy if(is.null(countslist)) countslist = lapply(ylist, function(y) rep(1, nrow(y))) ## Make data into long matrix ymat &lt;- lapply(1:length(ylist), FUN = function(tt){ data.frame(time = times[tt], Y = ylist[[tt]], counts = countslist[[tt]]) }) %&gt;% bind_rows() %&gt;% as_tibble() colnames(ymat) = c(&quot;time&quot;, &quot;Y&quot;, &quot;counts&quot;) ## when ylist[[tt]] already has a column name, this is needed. if(bin){ gg = ymat %&gt;% ggplot() + geom_raster(aes(x = time, y = Y, fill = counts)) + theme_bw() + ylab(&quot;Data&quot;) + xlab(&quot;Time&quot;) + scale_fill_gradientn(colours = c(&quot;white&quot;, &quot;black&quot;)) } if(!bin){ gg = ymat %&gt;% ggplot() + geom_point(aes(x = time, y = Y), alpha = alpha) + theme_bw() + ylab(&quot;Data&quot;) + xlab(&quot;Time&quot;) } ## If there is a flowtrend object to plot, do it. if(is.null(obj)){ return(gg) } else { ## Add the model numclust = obj$numclust mnmat = obj$mn %&gt;% .[,1,] %&gt;% `colnames&lt;-`(1:numclust) %&gt;% as_tibble() %&gt;% add_column(time = times) probmat = obj$prob %&gt;% as_tibble() %&gt;% setNames(1:numclust) %&gt;% add_column(time = times) mn_long = mnmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) prob_long = probmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) est_long = full_join(mn_long, prob_long, by = c(&quot;time&quot;,&quot;cluster&quot;)) gg = gg + geom_path(aes(x = time, y = mean, linewidth = prob, group = cluster, color = cluster), data = est_long, lineend = &quot;round&quot;, linejoin=&quot;mitre&quot;) + scale_linewidth(range = c(0.05, 5), limits = c(0, 1)) if(plot_band){ ## Add the estimated 95% probability regions for data. stdev = obj$sigma %&gt;% .[,,1] %&gt;% sqrt() mn_long_by_clust = mn_long %&gt;% group_by(cluster) %&gt;% group_split() band_long_by_clust = lapply(1:numclust, function(iclust){ mn_long_by_clust[[iclust]] %&gt;% mutate(upper = mean + 1.96 * stdev[iclust]) %&gt;% mutate(lower = mean - 1.96 * stdev[iclust]) }) band_long = band_long_by_clust %&gt;% bind_rows() gg = gg + geom_line(aes(x = time, y = upper, group = cluster, color = cluster), data = band_long, size = rel(.7), alpha = .5) + geom_line(aes(x = time, y = lower, group = cluster, color = cluster), data = band_long, size = rel(.7), alpha = .5) + guides(size = &quot;none&quot;) # To turn off line size from legend } return(gg) } } The plotting function plot_1d() will be even more useful when we have a model, but can also simply plot the data ylist. Let’s try this out. set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100)) dt_model &lt;- gendat_1d(100, rep(100, 100), return_model = TRUE) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() plot_1d(ylist = ylist, x = x) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model, linetype = &quot;dashed&quot;, size=2, alpha = .7) Voilà! Adding a model is easy, you can use: plot_1d(ylist, countslist) %&gt;% plot_1d_add_model(obj = obj, idim = 1) #&#39; Add a model to an existing plot from \\code{plot_1d()}. #&#39; This actually can take a model \\code{obj}, and just plot the 1-dimensional projections of the model in the \\code{idim} dimension. #&#39; #&#39; @param gg0 ggplot object from running \\code{plot_1d}. #&#39; @param obj Model #&#39; @param idim dimension #&#39; @param plot_band If TRUE, plot a +1.96 standard deviation band. #&#39; #&#39; @return ggplot object #&#39; #&#39; @export plot_1d_add_model &lt;- function(gg0, obj, idim, plot_band = TRUE){ times = obj$x numclust = obj$numclust mnmat = obj$mn %&gt;% .[,idim,] %&gt;% `colnames&lt;-`(1:numclust) %&gt;% as_tibble() %&gt;% add_column(time = times) probmat = obj$prob %&gt;% as_tibble() %&gt;% setNames(1:numclust) %&gt;% add_column(time = times) mn_long = mnmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) %&gt;% mutate(cluster = factor(cluster, levels=sapply(1:10, toString))) prob_long = probmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) %&gt;% mutate(cluster = factor(cluster, levels=sapply(1:10, toString))) est_long = full_join(mn_long, prob_long, by = c(&quot;time&quot;,&quot;cluster&quot;)) gg = gg0 + geom_path(aes(x = time, y = mean, linewidth = prob, group = cluster, color = cluster), data = est_long, lineend = &quot;round&quot;, linejoin=&quot;mitre&quot;) + scale_linewidth(range = c(0.05, 5), limits = c(0, 1)) ## Add the estimated 95% probability regions for data. if(plot_band){ stdev = obj$sigma %&gt;% .[,idim,idim] %&gt;% sqrt() names(stdev) = sapply(1:numclust, toString) mn_long_by_clust = mn_long %&gt;% group_by(cluster) %&gt;% group_split() band_long_by_clust = lapply(1:numclust, function(iclust){ mn_long_by_clust[[iclust]] %&gt;% mutate(upper = mean + 1.96 * stdev[iclust]) %&gt;% mutate(lower = mean - 1.96 * stdev[iclust]) }) band_long = band_long_by_clust %&gt;% bind_rows() gg = gg + geom_line(aes(x = time, y = upper, group = cluster, color = cluster), data = band_long, size = rel(.7), alpha = .5) + geom_line(aes(x = time, y = lower, group = cluster, color = cluster), data = band_long, size = rel(.7), alpha = .5) + guides(size = &quot;none&quot;) } return(gg) } Also, we will want to plot the estimated cluster probabilities of a model obj of class flowtrend. #&#39; Makes cluster probability plot (lines over time). #&#39; #&#39; @param obj Estimated model (from e.g. \\code{flowtrend()}) #&#39; #&#39; @export plot_prob &lt;- function(obj, x = NULL){ ## Basic checks if(!is.null(obj)) stopifnot(class(obj) %in% c(&quot;flowmix&quot;, &quot;flowtrend&quot;)) if(!is.null(x)){ times = x } else { ##stop(&quot;must provide x&quot;) times = 1:(obj$TT) } numclust = obj$numclust probmat = obj$prob %&gt;% as_tibble() %&gt;% setNames(1:numclust) %&gt;% add_column(time = times) prob_long = probmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) prob_long %&gt;% ggplot() + geom_line(aes(x=time, y = prob, group = cluster, col = cluster), size = rel(1)) + ggtitle(&quot;Estimated cluster probability&quot;) } We can’t test it out now, but we’ll use it later in 1d-example. Here is a 1d plotting function if you have cluster memberships memlist. #&#39; Makes 1d plot of data when #&#39; #&#39; @param ylist Particlel-level data. A list of (|nt| by |dimdat|) matrices. #&#39; @param memlist List of memberships for the particles. #&#39; @param x Time points. Defaults to NULL. #&#39; @param alpha Between 0 and 1, how transparent to plot the data #&#39; points. Defaults to 0.1. #&#39; #&#39; @return ggplot object with data, and optionally, a flowtrend model overlaid. #&#39; @export plot_1d_with_membership &lt;- function(ylist, memlist, countslist = NULL, x = NULL, alpha = .01){ ## Basic checks if(!is.null(x)){ stopifnot(length(x) == length(ylist)) times = x } else { times = 1:length(ylist) } dimdat = ncol(ylist[[1]]) ## If countslist is not provided, make a dummy if(is.null(countslist)) countslist = lapply(ylist, function(y) rep(1, nrow(y))) ## Make data into long matrix ymat &lt;- lapply(1:length(ylist), FUN = function(tt){ data.frame(time = times[tt], Y = ylist[[tt]], counts = countslist[[tt]], cluster = memlist[[tt]]) }) %&gt;% bind_rows() %&gt;% as_tibble() ymat = ymat %&gt;% mutate(cluster = as.factor(cluster)) colnames(ymat) = c(&quot;time&quot;, &quot;Y&quot;, &quot;counts&quot;, &quot;cluster&quot;) ## when ylist[[tt]] already has a column name, this is needed. gg = ymat %&gt;% ggplot() + geom_point(aes(x = time, y = Y, group=cluster, col = cluster),##, shape = cluster), alpha = alpha) + ##facet_wrap(~cluster) + theme_bw() + ylab(&quot;Data&quot;) + xlab(&quot;Time&quot;) return(gg) } 4.2 2d data 4.2.1 Generating 2d data #&#39; Generates some synthetic 2-dimensional data with three clusters. #&#39; #&#39; @param TT Number of time points. #&#39; @param nt Number of particles at each time. #&#39; #&#39; @return List containing (1) ylist, (2) mnlist, (3) clusterlist. #&#39; @export gendat_2d &lt;- function(TT, ntlist){ ## Basic checks stopifnot(length(ntlist) == TT) ## Make cluster probabilities, by time cluster_prob1 = sapply(1:TT, function(tt) sin(tt/24 * 2 * pi)/3 + 1 + (tt/TT)*5) cluster_prob2 = sapply(1:TT, function(tt) cos(tt/24 * 2 * pi)/3 + 8 - (tt/TT)*5) cluster_prob3 = rep(3, TT) probs = cbind(cluster_prob1, cluster_prob2, cluster_prob3) probs = probs/rowSums(probs) colnames(probs) = 1:3 probs_long = as_tibble(probs) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) probs_long %&gt;% ggplot() + geom_line(aes(x=time,y=prob, group=cluster, col=cluster)) + ylim(c(0,1)) ## Make cluster means, by time means &lt;- array(NA, dim = c(TT, 3, 2)) for(ii in 1:3){ for(tt in 1:TT){ means[tt, 1, 1] = means[tt, 1, 2] = tt/TT + 0.5 means[tt, 2, 1] = sin(tt/24 * 2 * pi)##seq(-1, 1, length.out = TT)[tt]*3.1415) means[tt, 2, 2] = 0 means[tt, 3, 1] = means[tt, 3, 2] = -3+cos(tt/24 * 2 * pi)##seq(-1, 1, length.out = TT)[tt]*6.282) } } dimnames(means)[[2]] = c(1:3) means_long = as_tibble(means) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) model = full_join(means_long, probs_long, by = c(&quot;time&quot;, &quot;cluster&quot;)) ylist = list() mulist = list() clusterlist = list() for(tt in 1:TT){ Y &lt;- vector(mode = &quot;list&quot;, length = 2) mu &lt;- vector(mode = &quot;list&quot;, length = 2) clusters_count &lt;- rmultinom(n = 1, size = ntlist[tt], prob = probs[tt,]) for(ii in 1:3){ if(ii == 1){ mn = means[tt,ii,,drop=TRUE] Sigma1 = matrix(c(0.4, 0.3, 0.3, 0.4), ncol = 2) Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0), Sigma= Sigma1)) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } if(ii == 2){ mn = means[tt,ii,, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0), Sigma = diag(c(0.5, 0.1)))) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } if(ii == 3){ mn = means[tt,ii,, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0), Sigma = diag(c(0.35, 0.35)))) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } } Y &lt;- Y %&gt;% purrr::compact() %&gt;% do.call(rbind, .) mu &lt;- mu %&gt;% purrr::compact() %&gt;% do.call(rbind, .) cluster &lt;- rep(1:3, times = clusters_count) ylist[[tt]] = Y mulist[[tt]] = mu clusterlist[[tt]] = cluster } return(list(ylist = ylist, mulist = mulist, clusterlist = clusterlist, probs = probs, means = means)) } 4.2.2 Plotting 2d data Here’s a simpler plotting function for 2d data at the particle level. #&#39; Simple plotter for 2d particle data. #&#39; #&#39; @param ylist Data. A list of (|nt| by |dimdat|) matrices #&#39; @param countslist Count data. #&#39; @param obj flowtrend (or flowmix) object. #&#39; @param time Out of 1 through \\code{lengthy(list)}, which time point to plot. #&#39; @param zero_one_list_censored A list of zeros and 1s marking the censored #&#39; points. #&#39; #&#39; @export #&#39; @return ggplot object. plot_2d &lt;- function(ylist, countslist = NULL, obj = NULL, tt, bin = TRUE, point_color = &quot;blue&quot;, raster_colours = c(&quot;white&quot;, &quot;blue&quot;), zero_one_list_censored = NULL){ ## Basic checks if(!is.null(obj)) stopifnot(class(obj) %in% c(&quot;flowmix&quot;, &quot;flowtrend&quot;)) stopifnot(ncol(ylist[[1]]) == 2) if(!is.null(obj)) stopifnot(obj$dimdat == 2) ## if(!bin) stop(&quot;2d plotting for particle level data isn&#39;t supported.&quot;) ## Take data from one time point y = ylist %&gt;% .[[tt]] if(is.null(colnames(y))){ colnames(y) = paste0(&quot;dim&quot;, c(1,2)) } y = y %&gt;% as_tibble() ## Handle counts if(is.null(countslist)){ counts = rep(1, nrow(ylist[[1]])) } if(!is.null(countslist)){ counts = countslist[[tt]] } ## Get variable names varnames = y %&gt;% colnames() varname1 = varnames[1] varname2 = varnames[2] ## Get data from one timepoint y = y %&gt;% add_column(counts = counts) ## If a list of 0&#39;s and 1&#39;s (marking censored points) is provided, plot it if(!is.null(zero_one_list_censored)){ zero_one = zero_one_list_censored[[tt]] y = y %&gt;% add_column(censored = factor(zero_one, levels = c(FALSE, TRUE)))##c(0,1))) } if(!bin){ p = y %&gt;% ggplot() + geom_point(aes(x = !!sym(varname1), y=!!sym(varname2)), size = rel(counts), alpha = .2, col = point_color) + theme_minimal() + theme(legend.position = &quot;none&quot;) + ## scale_size() scale_size_area() if(!is.null(zero_one_list_censored)){ p = y %&gt;% ggplot() + geom_point(aes(x = !!sym(varname1), y = !!sym(varname2), col = zero_one), size = rel(counts), alpha = 0.2) + theme_minimal() + theme(legend.position = &quot;none&quot;) + ## scale_size() + scale_size_area() + scale_color_manual(values=c(&quot;blue&quot;, &quot;greenyellow&quot;)) } } if(bin){ p = y %&gt;% ggplot() + geom_raster(aes(x = !!sym(varname1), y=!!sym(varname2), fill = counts)) + scale_fill_gradientn(guide=&quot;none&quot;, colours = raster_colours) } p = p + ggtitle(paste0(&quot;Time=&quot;, tt)) ## Adding visualizations of the model |obj| if(is.null(obj)){ return(p) } else { mnlist = lapply(1:obj$numclust, function(iclust){ one_mnmat = obj$mn[,,iclust] colnames(one_mnmat) = paste0(&quot;dim&quot;, 1:2) one_mnmat %&gt;% as_tibble() %&gt;% add_column(cluster = iclust) }) mnmat = do.call(rbind, mnlist) mn_colours = rep(&quot;red&quot;, obj$numclust) for(iclust in 1:obj$numclust){ ## Add ellipse el = ellipse::ellipse(x = obj$sigma[iclust,,], centre = obj$mn[tt,,iclust]) %&gt;% as_tibble() p = p + geom_path(aes(x = x, y = y), data = el, colour = mn_colours[iclust], lty = 2, linewidth = pmin(obj$prob[tt,iclust] * 8, 0.8)) ## Add mean p = p + geom_point(aes(x = dim1, y = dim2), data = mnmat %&gt;% subset(cluster == iclust) %&gt;% .[tt,], colour = mn_colours[iclust], ## size = rel(3)) size = obj$prob[tt,iclust] * 10) } } return(p) } ## This is from the many-cruises 02-helpers.R file; come back to this. if(FALSE){ p = datobj_2d %&gt;% ggplot() + theme_minimal() + geom_raster(aes(x = !!sym(varname1), y=!!sym(varname2), fill = counts)) + scale_fill_gradientn(colours = colours, guide=&quot;colorbar&quot;)+ xlim(c(0,8)) + ylim(c(0, 8)) + theme(legend.position = &quot;none&quot;) } Let’s try it out. datobj = gendat_2d(2, c(1000,1000)) plot_2d(datobj$ylist, tt=1, bin = FALSE) 4.3 3d data Given 3d data ylist and an estimated flowtrend model object obj, we want to plot both in three two-dimensional plot. plot_3d() lets you do this. 4.3.1 Plotting 3d data First, here’s a plotting helper. #&#39; Combine list of ggplots #&#39; @export my_mfrow &lt;- function(glist, ncol = NULL, nrow = NULL){ if(is.null(ncol)) ncol = 1 if(is.null(nrow)) ncol = 3 do.call(ggpubr::ggarrange, c(glist, ncol=ncol, nrow=nrow)) } #&#39; Makes three 2-dimensional plots of the 3d data #&#39; #&#39; @param ylist Data. A list of (|nt| by |dimdat|) matrices #&#39; @param countslist Count data. #&#39; @param obj flowmix or flowtrend object #&#39; @param tt Time point #&#39; @param return_list_of_plots If TRUE, return the list of three plots instead #&#39; of the combined plot #&#39; @param zero_one_list_censored A list of zeros and 1s marking the censored #&#39; points. #&#39; #&#39; @export #&#39; @return #&#39; plot_3d &lt;- function(ylist, obj = NULL, tt, countslist = NULL, mn_colours = NULL, labels = NULL, bin = TRUE, plot_title = NULL, return_list_of_plots = FALSE, zero_one_list_censored = NULL){ ## Basic checks if(!is.null(obj)) stopifnot(class(obj) %in% c(&quot;flowmix&quot;, &quot;flowtrend&quot;)) stopifnot(ncol(ylist[[1]]) == 3) if(!is.null(labels)) assertthat::assert_that(length(labels) == obj$numclust) ## if(!bin) stop(&quot;This function is only for binned 3d data!&quot;) ## Extract data ## y = ylist[[tt]][,dims] labs = colnames(ylist[[1]]) if(is.null(labs)) labs = paste0(&quot;dim&quot;, 1:3) if(is.null(countslist)){ counts = rep(1, nrow(ylist[[tt]])) } if(!is.null(countslist)){ counts = countslist[[tt]] } ## Aggregate counts into the two dimensions y2d_list = list() counts2d_list = list() for(ii in 1:3){ dims = list(c(1:2), c(2:3), c(3,1))[[ii]] if(bin){ yy = flowmix::collapse_3d_to_2d(ylist[[tt]], counts, dims) y2d = yy[,1:2] colnames(y2d) = labs[dims] one_counts = yy[,3] } else { y2d = ylist[[tt]][,dims] colnames(y2d) = labs[dims] one_counts = counts } y2d_list[[ii]] = y2d counts2d_list[[ii]] = one_counts } total_range = sapply(counts2d_list, range) %&gt;% range() if(bin) zero_one = NULL if(!bin &amp; !is.null(zero_one_list_censored)) zero_one = list(zero_one_list_censored[[tt]]) if(!bin &amp; is.null(zero_one_list_censored)) zero_one = NULL ## Create three 2d plots plotlist = list() for(ii in 1:3){ dims = list(c(1:2), c(2:3), c(3,1))[[ii]] ## Make data plot ##one_countslist = (if(!is.null(countslist)) list(counts2d_list[[ii]]) else NULL) p = plot_2d(list(y2d_list[[ii]]), list(counts2d_list[[ii]]), obj = NULL, tt = 1, bin = bin, zero_one_list_censored = zero_one) if(bin){ p$scales$scales &lt;- list() p = p + scale_fill_gradientn( colors = c(&quot;white&quot;, &quot;blue&quot;, &quot;yellow&quot;), limits = total_range, guide = &quot;none&quot;) } ## Adding visualizations of the model |obj| at time tt if(!is.null(obj)){ if(is.null(mn_colours)){ mn_colours = rep(&quot;red&quot;, obj$numclust) } ## Make data matrix mnlist = lapply(1:obj$numclust, function(iclust){ one_mnmat = obj$mn[tt,dims,iclust] %&gt;% t() colnames(one_mnmat) = paste0(&quot;dim&quot;, 1:2) one_mnmat %&gt;% as_tibble() %&gt;% add_column(cluster = iclust) }) mnmat = do.call(rbind, mnlist) for(iclust in 1:obj$numclust){ ## Add ellipse el = ellipse::ellipse(x = obj$sigma[iclust,dims,dims], centre = obj$mn[tt,dims,iclust]) %&gt;% as_tibble() p = p + geom_path(aes(x = x, y = y), data = el, colour = mn_colours[iclust], lty = 2, linewidth = pmin(obj$prob[tt,iclust] * 8, 0.8)) ## Add mean p = p + geom_point(aes(x = dim1, y = dim2), data = mnmat %&gt;% subset(cluster == iclust), colour = mn_colours[iclust], size = obj$prob[tt,iclust] * 10) ## Add cluster number as a label cex = rel(3) fac = 10 if(is.null(labels)){ labels = 1:(obj$numclust) } dt = data.frame(dim1 = mnmat[,1], dim2 = mnmat[,2], prob = obj$prob[tt,] * fac) p = p + ggrepel::geom_text_repel(aes(x = dim1, y = dim2, label = labels, point.size = sqrt(prob)), col = mn_colours, cex = cex, bg.color = &quot;white&quot;, bg.r = 0.1, fontface = &quot;bold&quot;, force_pull = 5, # do not pull toward data points ## data = mnmat, data = dt, seed = 1) } } ## Format a bit more and save if(ii == 1){ if(is.null(plot_title)) plot_title = paste0(&quot;Time=&quot;, tt) p = p + ggtitle(plot_title) } else { p = p + ggtitle(&quot;&quot;) } p = p + theme(legend.position = &quot;none&quot;) plotlist[[ii]] = p } p_combined = my_mfrow(plotlist) ## Return the plots if(return_list_of_plots) return(plotlist) return(p_combined) } 4.3.2 Generating 3d data #&#39; Generates some synthetic 3-dimensional data with three clusters. #&#39; #&#39; @param TT Number of time points. #&#39; @param nt Number of particles at each time. #&#39; #&#39; @return List containing (1) ylist, (2) mnlist, (3) clusterlist. #&#39; @export gendat_3d &lt;- function(TT, ntlist){ ## Basic checks stopifnot(length(ntlist) == TT) ## Make cluster probabilities, by time cluster_prob1 = sapply(1:TT, function(tt) sin(tt/24 * 2 * pi)/3 + 1 + (tt/TT)*5) cluster_prob2 = sapply(1:TT, function(tt) cos(tt/24 * 2 * pi)/3 + 8 - (tt/TT)*5) cluster_prob3 = rep(3, TT) probs = cbind(cluster_prob1, cluster_prob2, cluster_prob3) probs = probs/rowSums(probs) colnames(probs) = 1:3 probs_long = as_tibble(probs) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) probs_long %&gt;% ggplot() + geom_line(aes(x=time,y=prob, group=cluster, col=cluster)) + ylim(c(0,1)) ## Make cluster means, by time numclust = 3 dimdat = 3 ## means &lt;- array(NA, dim = c(TT, 3, 2)) means &lt;- array(NA, dim = c(TT, numclust, dimdat)) for(ii in 1:3){ for(tt in 1:TT){ ## First cluster means[tt, 1, 1] = means[tt, 1, 2] = means[tt, 1, 3] = tt/TT + 0.5 ## Second cluster means[tt, 2, 1] = sin(tt/24 * 2 * pi)##seq(-1, 1, length.out = TT)[tt]*3.1415) means[tt, 2, 2] = 0 means[tt, 2, 3] = 0 ## Third cluster means[tt, 3, 1] = means[tt, 3, 2] = means[tt, 3, 3] = -3+cos(tt/24 * 2 * pi)##seq(-1, 1, length.out = TT)[tt]*6.282) } } dimnames(means)[[2]] = c(1:3) means_long = as_tibble(means) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) model = full_join(means_long, probs_long, by = c(&quot;time&quot;, &quot;cluster&quot;)) ylist = list() mulist = list() clusterlist = list() for(tt in 1:TT){ Y &lt;- vector(mode = &quot;list&quot;, length = numclust) mu &lt;- vector(mode = &quot;list&quot;, length = numclust) clusters_count &lt;- rmultinom(n = 1, size = ntlist[tt], prob = probs[tt,]) for(ii in 1:3){ if(ii == 1){ mn = means[tt,ii,,drop=TRUE] Sigma1 = matrix(rep(0.3, 9), ncol = 3) diag(Sigma1) = 0.4 ## Sigma1 = matrix(c(0.4, 0.3, 0.3, 0.4), ncol = 2) Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0,0), Sigma= Sigma1)) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } if(ii == 2){ mn = means[tt,ii,, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0,0), Sigma = diag(c(0.5, 0.1, 0.2)))) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } if(ii == 3){ mn = means[tt,ii,, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0,0), Sigma = diag(c(0.35, 0.35, 0.35)))) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } } Y &lt;- Y %&gt;% purrr::compact() %&gt;% do.call(rbind, .) mu &lt;- mu %&gt;% purrr::compact() %&gt;% do.call(rbind, .) cluster &lt;- rep(1:3, times = clusters_count) colnames(Y) = paste0(&quot;dim&quot;, 1:3) ylist[[tt]] = Y mulist[[tt]] = mu clusterlist[[tt]] = cluster } return(list(ylist = ylist, mulist = mulist, clusterlist = clusterlist, probs = probs, means = means)) } Now, let’s try using plot_3d on some synthetic data. set.seed(2321) datobj = gendat_3d(100, rep(30,100)) plot_3d(ylist = datobj$ylist, tt = 1, bin = FALSE) "],["building-the-modelalgorithm.html", "5 Building the model/algorithm 5.1 Trend filtering 5.2 Objective (data log-likelihood) 5.3 Initial parameters for EM algorithm 5.4 E step 5.5 M step 5.6 The main “flowtrend” function", " 5 Building the model/algorithm 5.1 Trend filtering Trend filtering is a non-parametric regression technique for a sequence of output points \\(y = (y_1,..., y_T)\\) observed at locations \\(x = (x_1, ..., x_T)\\). It is usually assumed that \\(x_1, ..., x_T\\) are evenly spaced points, though this can be relaxed. The trend filtering estimate of order \\(l\\) of the time series \\(\\mu_t = \\mathbb{E}(y_t), t \\in x\\) is obtained by calculating \\[\\hat{\\mu} = \\mathop{\\mathrm{argmin}}_{\\mu \\in \\mathbb{R}^T} \\frac{1}{2}\\| \\mu - y\\|_2^2 + \\lambda \\| D^{(l+1)} \\mu\\|_1,\\] where \\(\\lambda\\) is a tuning parameter and \\(D^{(l+1)} \\in \\mathbb{R}^{T-l}\\) is the \\((l+1)^\\text{th}\\) order discrete differencing matrix. We first need to be able to construct the trend filtering “differencing matrix” used for smoothing the mixture parameters over time. The general idea of the trend filtering is explained masterfully in [Ryan’s paper, Section 6 and equation (41)]. The differencing matrix is formed by recursion, starting with \\(D^{(1)}\\). \\[\\begin{equation*} D^{(1)} = \\begin{bmatrix} -1 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 1 &amp; \\cdots &amp; 0 &amp; 0\\\\ \\vdots &amp; &amp; &amp; &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; -1 &amp; 1 \\end{bmatrix}. \\end{equation*}\\] For \\(l&gt;1\\), the differencing matrox \\(D^{(l+1)}\\) is defined recursively as \\(D^{(l+1)} = D^{(1)} D^{(l)}\\), starting with \\(D^{(1)}\\). #&#39; Generating Difference Matrix of Specified Order #&#39; #&#39; @param n length of vector to be differenced #&#39; @param l order of differencing #&#39; @param x optional. Spacing of input points. #&#39; #&#39; @return A n by n-l-1 matrix #&#39; @export #&#39; #&#39; @examples gen_diff_mat &lt;- function(n, l, x = NULL){ ## Basic check if(!is.null(x)) stopifnot(length(x) == n) if(is.unsorted(x)) stop(&quot;x must be in increasing order!&quot;) get_D1 &lt;- function(t) {do.call(rbind, lapply(1:(t-1), FUN = function(x){ v &lt;- rep(0, t) v[x] &lt;- -1 v[x+1] &lt;- 1 v }))} if(is.null(x)){ if(l == 0){ return(diag(rep(1,n))) } if(l == 1){ return(get_D1(n)) } if(l &gt; 1){ D &lt;- get_D1(n) for(k in 1:(l-1)){ D &lt;- get_D1(n-k) %*% D } return(D) } } else{ if(l == 0){ return(diag(rep(1,n))) } if(l == 1){ return(get_D1(n)) } if(l &gt; 1){ D &lt;- get_D1(n) for(k in 1:(l-1)){ D1 = get_D1(n-k) facmat = diag(k / diff(x, lag = k)) D &lt;- D1 %*% facmat %*% D } return(D) } } } For equally spaced inputs with \\(l=1\\) and \\(l=2\\): l = 1 Dl = gen_diff_mat(n = 10, l = l+1, x = NULL) print(Dl) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 -2 1 0 0 0 0 0 0 0 ## [2,] 0 1 -2 1 0 0 0 0 0 0 ## [3,] 0 0 1 -2 1 0 0 0 0 0 ## [4,] 0 0 0 1 -2 1 0 0 0 0 ## [5,] 0 0 0 0 1 -2 1 0 0 0 ## [6,] 0 0 0 0 0 1 -2 1 0 0 ## [7,] 0 0 0 0 0 0 1 -2 1 0 ## [8,] 0 0 0 0 0 0 0 1 -2 1 l = 2 Dl = gen_diff_mat(n = 10, l = l+1, x = NULL) print(Dl) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] -1 3 -3 1 0 0 0 0 0 0 ## [2,] 0 -1 3 -3 1 0 0 0 0 0 ## [3,] 0 0 -1 3 -3 1 0 0 0 0 ## [4,] 0 0 0 -1 3 -3 1 0 0 0 ## [5,] 0 0 0 0 -1 3 -3 1 0 0 ## [6,] 0 0 0 0 0 -1 3 -3 1 0 ## [7,] 0 0 0 0 0 0 -1 3 -3 1 When the inputs have a gap in it: ## See what a l=2 difference matrix looks like: x = (1:10)[-(3)] l = 2 TT = length(x) Dl = gen_diff_mat(n = TT, l = l+1, x = x) print(Dl) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] -0.6666667 1.3333333 -1.333333 0.6666667 0 0 0 0 0 ## [2,] 0.0000000 -0.3333333 2.000000 -2.6666667 1 0 0 0 0 ## [3,] 0.0000000 0.0000000 -1.000000 3.0000000 -3 1 0 0 0 ## [4,] 0.0000000 0.0000000 0.000000 -1.0000000 3 -3 1 0 0 ## [5,] 0.0000000 0.0000000 0.000000 0.0000000 -1 3 -3 1 0 ## [6,] 0.0000000 0.0000000 0.000000 0.0000000 0 -1 3 -3 1 ## Formally test it d1 = gen_diff_mat(n = 10, l = l+1, x = 1:10) d2 = gen_diff_mat(n = 10, l = l+1, x = (1:10)*2) print(d1) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] -1 3 -3 1 0 0 0 0 0 0 ## [2,] 0 -1 3 -3 1 0 0 0 0 0 ## [3,] 0 0 -1 3 -3 1 0 0 0 0 ## [4,] 0 0 0 -1 3 -3 1 0 0 0 ## [5,] 0 0 0 0 -1 3 -3 1 0 0 ## [6,] 0 0 0 0 0 -1 3 -3 1 0 ## [7,] 0 0 0 0 0 0 -1 3 -3 1 print(d2) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] -0.25 0.75 -0.75 0.25 0.00 0.00 0.00 0.00 0.00 0.00 ## [2,] 0.00 -0.25 0.75 -0.75 0.25 0.00 0.00 0.00 0.00 0.00 ## [3,] 0.00 0.00 -0.25 0.75 -0.75 0.25 0.00 0.00 0.00 0.00 ## [4,] 0.00 0.00 0.00 -0.25 0.75 -0.75 0.25 0.00 0.00 0.00 ## [5,] 0.00 0.00 0.00 0.00 -0.25 0.75 -0.75 0.25 0.00 0.00 ## [6,] 0.00 0.00 0.00 0.00 0.00 -0.25 0.75 -0.75 0.25 0.00 ## [7,] 0.00 0.00 0.00 0.00 0.00 0.00 -0.25 0.75 -0.75 0.25 d1_d2_ratio = as.numeric(d1/d2) %&gt;% na.omit() %&gt;% as.numeric() testthat::expect_true(all(d1_d2_ratio==d1_d2_ratio[1])) ## correct A formal test for unevenly spaced inputs will come soon, once we’ve defined gen_tf_mat, next. We now build a function to build a lasso regressor matrix \\(H\\) that can be used to solve an equivalent problem as the trend filtering of the l’th degree. (This is stated in Lemma 4, equation (25) from Tibshirani (2014).) #&#39; A lasso regressor matrix H that can be used to solve an equivalent problem as the trend filtering of the \\code{k}&#39;th degree. #&#39; #&#39; @param n Total number of time points. #&#39; @param k Degree of trend filtering for cluster probabilities. $k=0$ is fused lasso, $k=1$ is linear trend filtering, and so on. #&#39; @param x Time points #&#39; #&#39; @return $n$ by $n$ matrix. #&#39; #&#39; @export gen_tf_mat &lt;- function(n, k, x = NULL){ if(is.null(x) ){ x = (1:n)/n } if(!is.null(x)){ stopifnot(length(x) == n) } ## For every i,j&#39;th entry, use this helper function (from eq 25 of Tibshirani ## (2014)). gen_ij &lt;- function(x, i, j, k){ xi &lt;- x[i] if(j %in% 1:(k+1)){ return(xi^(j-1)) } if(j %in% (k+2):n){ ## Special handling for k==0, See lemma 4 eq 25 if(k == 0){ prd = 1 ind = j } if(k &gt;= 1){ ind = j - (1:k) prd = prod(xi - x[ind]) } return(prd * ifelse(xi &gt;= x[max(ind)], 1, 0)) } } ## Generate the H matrix, entry-wise. H &lt;- matrix(nrow = n, ncol = n) for(i in 1:n){ for(j in 1:n){ H[i,j] &lt;- gen_ij(x, i,j, k) } } return(H) } Here’s a simple test of gen_tf_mat(), against an alternative function built for equally spaced data. #&#39; Creates trend filtering regression matrix using Lemma 2 of Ryan Tibshirani&#39;s #&#39; trendfilter paper (2014); works on equally spaced data only. #&#39; #&#39; @param n Number of data points. #&#39; @param k Order of trend filter. 0 is fused lasso, and so on. #&#39; #&#39; @examples #&#39; ord = 1 #&#39; H_tf &lt;- gen_tf_mat_equalspace(n = 100, k = ord) #&#39; H_tf[,1] * 100 #&#39; H_tf[,2] * 100 #&#39; H_tf[,3] * 100 #&#39; H_tf[,4] * 100 #&#39; H_tf[,99] * 100 #&#39; H_tf[,100] * 100 #&#39; #&#39; @return n by n matrix. gen_tf_mat_equalspace &lt;- function(n, k){ nn = n kk = k ##&#39; Connects kk to kk-1. sigm &lt;- function(ii, kk){ if(kk == 0) return(rep(1, ii)) cumsum(sigm(ii, kk-1)) } mat = matrix(NA, ncol = nn, nrow = nn) for(ii in 1:nn){ for(jj in 1:nn){ if(jj &lt;= kk+1) mat[ii,jj] = ii^(jj-1) / nn^(jj-1) if(ii &lt;= jj-1 &amp; jj &gt;= kk+2) mat[ii, jj] = 0 if(ii &gt; jj-1 &amp; jj &gt;= kk+2){ mat[ii, jj] = (sigm(ii-jj+1, kk) %&gt;% tail(1)) * factorial(kk) / nn^kk } } } return(mat) } testthat::test_that(&quot;Trend filtering regression matrix is created correctly on equally spaced data.&quot;, { ## Check that equally spaced data creates same trendfilter regression matrix ## Degree 1 H1 &lt;- gen_tf_mat(10, 1) H1_other &lt;- gen_tf_mat(10, 1, x=(1:10)/10) testthat::expect_equal(H1, H1_other) ## Degree 1 H2 &lt;- gen_tf_mat(10, 2) H2_other &lt;- gen_tf_mat(10, 2, x=(1:10)/10) testthat::expect_equal(H2, H2_other) ## Check the dimension testthat::expect_equal(dim(H1), c(10,10)) ## Check against an alternative function. for(ord in c(0,1,2,3,4)){ H &lt;- gen_tf_mat(10, ord) H_eq = gen_tf_mat_equalspace(10, ord) testthat::expect_true(max(abs(H_eq- H)) &lt; 1E-10) } }) ## Test passed 😀 Finally, let’s test the difference matrix \\(D\\) formed using unevenly spaced \\(x\\)’s. testthat::test_that(&quot;Uneven spaced D matrix is formed correctly&quot;, { x = (1:6)[-(2)] ## k = 0 is piecewise constant, k=1 is piecewise linear, etc. for(k in 0:3){ ## Temp k=1 ## End of temp l = k+1 ## Form Dl using our function Dl &lt;- flowtrend::gen_diff_mat(n=5, l = l, x=x) ## Compare it to rows of H matrix, per section 6 of Tibshirani et al. 2014 gen_tf_mat(n = length(x), k = k, x = x) %&gt;% solve() %&gt;% `*`(factorial(k)) %&gt;% ## This part is missing in Tibshirani et al. 2014 tail(length(x)-(k+1)) -&gt; Hx ratiomat = Hx/Dl ## Process the ratios of each entry, and check that they&#39;re equal to 1 ratios = ratiomat[!is.nan(ratiomat)] ratios = ratios[is.finite(ratios)] testthat::expect_true(all.equal(ratios, rep(1, length(ratios)))) } }) ## Test passed 🎉 5.2 Objective (data log-likelihood) The function loglik_tt() calculates the log-likelihood of one cytogram, which is: \\[\\sum_{i=1}^{n_t} C_i^{(t)} \\log\\left( \\sum_{k=1}^K \\pi_{itk} \\phi(y_i^{(t)}; \\mu_{kt}, \\Sigma_k) \\right). \\] This is the portion of the objective function of the flowtrend model. #&#39; Log likelihood for a single time point&#39;s data. #&#39; #&#39; @param mu Cluster means. #&#39; @param prob Cluster probabilities. #&#39; @param sigma Cluster variances. #&#39; @param ylist Data. #&#39; @param tt Time point. loglik_tt &lt;- function(ylist, tt, mu, sigma, prob, dimdat = NULL, countslist = NULL, numclust){ ## One particle&#39;s log likelihood (weighted density) weighted.densities = sapply(1:numclust, function(iclust){ if(dimdat == 1){ return(prob[tt,iclust] * dnorm(ylist[[tt]], mu[tt,,iclust], sqrt(sigma[iclust,,]))) } if(dimdat &gt; 1){ return(prob[tt,iclust] * dmvnorm_arma_fast(ylist[[tt]], mu[tt,,iclust], as.matrix(sigma[iclust,,]), FALSE)) } }) nt = nrow(ylist[[tt]]) counts = (if(!is.null(countslist)) countslist[[tt]] else rep(1, nt)) sum_wt_dens = rowSums(weighted.densities) sum_wt_dens = sum_wt_dens %&gt;% pmax(1E-100) return(sum(log(sum_wt_dens) * counts)) } Next, here is the function that calculates the entire objective from all cytograms, given model parameter mu, prob, and sigma: \\[\\sum_{t=1}^T\\sum_{i=1}^{n_t} C_i^{(t)} \\log\\left( \\sum_{k=1}^K \\pi_{itk} \\phi(y_i^{(t)}; \\mu_{kt}, \\Sigma_k) \\right). \\] #&#39; Evaluating the penalized negative log-likelihood on all data \\code{ylist} given parameters \\code{mu}, \\code{prob}, and \\code{sigma}. #&#39; #&#39; @param mu #&#39; @param prob #&#39; @param prob_link #&#39; @param sigma #&#39; @param ylist #&#39; @param Dl #&#39; @param l #&#39; @param lambda #&#39; @param l_prob #&#39; @param Dl_prob #&#39; @param lambda_prob #&#39; @param alpha #&#39; @param beta #&#39; @param countslist #&#39; @param unpenalized if TRUE, return the unpenalized out-of-sample fit. #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples objective &lt;- function(mu, prob, prob_link = NULL, sigma, ylist, Dlp1 = NULL, l = NULL, lambda = 0, l_prob = NULL, Dlp1_prob = NULL, lambda_prob = 0, alpha = NULL, beta = NULL, countslist = NULL, unpenalized = FALSE){ ## Set some important variables TT = dim(mu)[1] numclust = dim(mu)[3] if(is.null(countslist)){ ntlist = sapply(ylist, nrow) } else { ntlist = sapply(countslist, sum) } N = sum(ntlist) dimdat = ncol(ylist[[1]]) ## Calculate the log likelihood loglik = sapply(1:TT, function(tt){ return(loglik_tt(ylist, tt, mu, sigma, prob, countslist, numclust = numclust, dimdat = dimdat)) }) if(unpenalized){ obj = -1/N * sum(unlist(loglik)) return(obj) } else { ## Return penalized likelihood mu.splt &lt;- asplit(mu, MARGIN = 2) ## This was 3, which I think produces the same result. diff_mu &lt;- sum(unlist(lapply(mu.splt, FUN = function(m) sum(abs(Dlp1 %*% m))))) diff_prob &lt;- sum(abs(Dlp1_prob %*% prob_link)) obj = -1/N * sum(unlist(loglik)) + lambda * diff_mu + lambda_prob * diff_prob return(obj) } } Here’s a helper to check numerical convergence of the EM algorithm. #&#39; Checks numerical improvement in objective value. Returns TRUE if |old-new|/|old| is smaller than tol. #&#39; #&#39; @param old Objective value from previous iteration. #&#39; @param new Objective value from current iteration. #&#39; @param tol Numerical tolerance. check_converge_rel &lt;- function(old, new, tol=1E-6){ return(abs((old-new)/old) &lt; tol ) } Here’s also a helper function to do the softmax-ing of \\(\\alpha_t \\in \\mathbb{R}^K\\). #&#39; Softmax function. #&#39; #&#39; @param prob_link alpha, which is a (T x K) matrix. #&#39; #&#39; @return exp(alpha)/rowSum(exp(alpha)). A (T x K) matrix. softmax &lt;- function(prob_link){ exp_prob_link = exp(prob_link) prob = exp_prob_link / rowSums(exp_prob_link) } testthat::test_that(&quot;Test for softmax&quot;,{ link = runif(100, min = -10, max = 10) %&gt;% matrix(nrow = 10, ncol = 10) testthat::expect_true(all(abs(rowSums(softmax(link)) - 1) &lt; 1E-13)) }) ## Test passed 🎉 5.3 Initial parameters for EM algorithm The EM algorithm requires some initial values for \\(\\mu\\), \\(\\pi\\) and \\(\\Sigma\\). Initializing \\(\\pi\\) is done in one line, prob = matrix(1/numclust, nrow = TT, ncol = numclust), which sets everything to \\(1/K\\). For \\(\\mu\\) and \\(\\Sigma\\), we write some functions. Essentially, initial means \\(\\mu\\) are jittered versions of a \\(K\\) means that are drawn from a downsampled version of \\(ylist\\) (downsampling is done because \\(ylist\\) can have a large number of particles). \\(\\Sigma\\) is \\(d\\times d\\) identity matrices, with fac=1 diagonal by default. #&#39; Initialize the cluster centers. #&#39; #&#39; @param ylist A T-length list of (nt by 3) datasets. There should be T of #&#39; such datasets. 3 is actually \\code{mulen}. #&#39; @param numclust Number of clusters (M). #&#39; @param TT total number of (training) time points. #&#39; #&#39; @return An array of dimension (T x dimdat x M). #&#39; @export init_mn &lt;- function(ylist, numclust, TT, dimdat, countslist = NULL, seed=NULL){ if(!is.null(seed)){ assertthat::assert_that(all((seed %&gt;% sapply(., class)) == &quot;integer&quot;)) assertthat::assert_that(length(seed) == 7) RNGkind(&quot;L&#39;Ecuyer-CMRG&quot;) .Random.seed &lt;&lt;- seed } if(is.null(countslist)){ ntlist = sapply(ylist, nrow) countslist = lapply(ntlist, FUN = function(nt) rep(1, nt)) } ## Initialize the means by (1) collapsing to one cytogram (2) random ## sampling from this distribution, after truncation, TT = length(ylist) ylist_downsampled &lt;- lapply(1:TT, function(tt){ y = ylist[[tt]] counts = countslist[[tt]] ## Sample so that, in total, we get mean(nt) * 30 sized sample. In the case ## of binned data, nt is the number of bins. if(nrow(y) &gt; 500) nsize = pmin(nrow(y) / TT * 30, nrow(y)) else nsize = nrow(y) some_rows = sample(1:nrow(y), size = nsize, prob = counts/sum(counts)) y[some_rows,, drop=FALSE] }) ## Jitter the means a bit yy = do.call(rbind, ylist_downsampled) new_means = yy[sample(1:nrow(yy), numclust),, drop=FALSE] jitter_sd = apply(yy, 2, sd) / 100 jitter_means = MASS::mvrnorm(n = nrow(new_means), mu = rep(0, dimdat), Sigma = diag(jitter_sd, ncol = dimdat)) new_means = new_means + jitter_means ## Repeat TT times == flat/constant initial means across time. mulist = lapply(1:TT, function(tt){ new_means }) ## New (T x dimdat x numclust) array is created. muarray = array(NA, dim=c(TT, dimdat, numclust)) for(tt in 1:TT){ muarray[tt,,] = as.matrix(mulist[[tt]]) } return(muarray) } #&#39; Initialize the covariances. #&#39; #&#39; @param data The (nt by 3) datasets. There should be T of them. #&#39; @param numclust Number of clusters. #&#39; @param fac Value to use for the diagonal of the (dimdat x dimdat) covariance #&#39; matrix. #&#39; #&#39; @return An (K x dimdat x dimdat) array containing the (dimdat by dimdat) #&#39; covariances. #&#39; @export init_sigma &lt;- function(data, numclust, fac = 1){ ndat = nrow(data[[1]]) pdat = ncol(data[[1]]) sigmas = lapply(1:numclust, function(iclust){ onesigma = diag(fac * rep(1, pdat)) if(pdat==1) onesigma = as.matrix(fac) colnames(onesigma) = paste0(&quot;datcol&quot;, 1:pdat) rownames(onesigma) = paste0(&quot;datcol&quot;, 1:pdat) return(onesigma) }) sigmas = abind::abind(sigmas, along=0) return(sigmas) } Here’s a helper function for printing the progress. #&#39; A helper function to print the progress of a loop or simulation. #&#39; #&#39; @param isim Replicate number. #&#39; @param nsim Total number of replicates. #&#39; @param type Type of job you&#39;re running. Defaults to &quot;simulation&quot;. #&#39; @param lapsetime Lapsed time, in seconds (by default). #&#39; @param lapsetimeunit &quot;second&quot;. #&#39; @param start.time start time. #&#39; @param fill Whether or not to fill the line. #&#39; #&#39; @return No return print_progress &lt;- function(isim, nsim, type = &quot;simulation&quot;, lapsetime = NULL, lapsetimeunit = &quot;seconds&quot;, start.time = NULL, fill = FALSE){ ## If lapse time is present, then use it if(fill) cat(fill = TRUE) if(is.null(lapsetime) &amp; is.null(start.time)){ cat(&quot;\\r&quot;, type, &quot; &quot;, isim, &quot;out of&quot;, nsim) } else { if(!is.null(start.time)){ lapsetime = round(difftime(Sys.time(), start.time, units = &quot;secs&quot;), 0) remainingtime = round(lapsetime * (nsim-isim)/isim,0) endtime = Sys.time() + remainingtime } cat(&quot;\\r&quot;, type, &quot; &quot;, isim, &quot;out of&quot;, nsim, &quot;with lapsed time&quot;, lapsetime, lapsetimeunit, &quot;and remaining time&quot;, remainingtime, lapsetimeunit, &quot;and will finish at&quot;, strftime(endtime)) } if(fill) cat(fill = TRUE) } 5.4 E step #&#39; E step, which updates the &quot;responsibilities&quot;, which are posterior membership probabilities of each particle. #&#39; #&#39; @param mn #&#39; @param sigma covariance #&#39; @param prob #&#39; @param ylist #&#39; @param numclust #&#39; @param denslist_by_clust #&#39; @param first_iter #&#39; @param countslist #&#39; @param padding A small amount of padding to add to the weighted #&#39; densities. Note, a very small value (like 1E-20) should be used, otherwise #&#39; the probabilitistic (soft) gating can be affected quite noticably. #&#39; #&#39; @return #&#39; @export #&#39; Estep &lt;- function(mn, sigma, prob, ylist = NULL, numclust, denslist_by_clust = NULL, first_iter = FALSE, countslist = NULL, padding = 1E-20){ ## Basic setup TT = length(ylist) ntlist = sapply(ylist, nrow) resp = list() dimdat = dim(mn)[2] assertthat::assert_that(dim(mn)[1] == length(ylist)) ## Helper to calculate Gaussian density for each \\code{N(y_{t,k},mu_{t,k} and ## Sigma_k)}. calculate_dens &lt;- function(iclust, tt, y, mn, sigma, denslist_by_clust, first_iter) { mu &lt;- mn[tt, , iclust] if (dimdat == 1) { dens = dnorm(y, mu, sd = sqrt(sigma[iclust, , ])) } else { dens = dmvnorm_arma_fast(y, mu, sigma[iclust,,], FALSE) } return(dens) } ## Calculate posterior probability of membership of $y_{it}$. ncol.prob = ncol(prob) empty_row = rbind(1:numclust)[-1,] for (tt in 1:TT) { ylist_tt = ylist[[tt]] if(nrow(ylist_tt) == 0){ resp[[tt]] &lt;- empty_row } else { densmat &lt;- sapply(1:numclust, calculate_dens, tt, ylist_tt, mn, sigma, denslist_by_clust, first_iter) wt.densmat &lt;- matrix(prob[tt, ], nrow = ntlist[tt], ncol = ncol.prob, byrow = TRUE) * densmat wt.densmat = wt.densmat + padding##1e-20 wt.densmat &lt;- wt.densmat/rowSums(wt.densmat) resp[[tt]] &lt;- wt.densmat } } ## Weight the responsibilities by $C_{it}$. if (!is.null(countslist)) { resp &lt;- Map(function(myresp, mycount) { myresp * mycount }, resp, countslist) } return(resp) } The E step should return a list whose elements are matrices that have the same number of rows as ylist, (that is, a \\(T\\) -length list of matrices of size \\(n_t \\times K\\) for \\(t=1,\\cdots, T\\)). (This test fails for some reason; will come back to this.) testthat::test_that(&quot;E step returns appropriately sized responsibilities.&quot;,{ ## Generate some fake data TT = 10 dimdat = 1 ylist = lapply(1:TT, function(tt){ runif(30*dimdat) %&gt;% matrix(ncol = dimdat, nrow = 30)}) numclust = 3 ## Initialize a few parameters, not carefully sigma = init_sigma(ylist, numclust) ## (T x numclust x (dimdat x dimdat)) mn = init_mn(ylist, numclust, TT, dimdat)##, countslist = countslist) prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. ## Calculate responsibility resp = Estep(mn = mn, sigma = sigma, prob = prob, ylist = ylist, numclust = numclust) ## Check these things testthat::expect_equal(length(resp), length(ylist)) testthat::expect_equal(length(resp), length(ylist)) testthat::expect_equal(sapply(resp, nrow), sapply(ylist, nrow)) testthat::expect_true(all(sapply(resp, ncol) == numclust)) }) ## Test passed 🥇 5.5 M step The M step of the EM algorithm has three steps – one each for \\(\\mu\\), \\(\\pi\\), and \\(\\Sigma\\). 5.5.1 M step for \\(\\pi\\) #&#39; The M step for the cluster probabilities #&#39; #&#39; @param resp Responsibilities. #&#39; @param H_tf Trend filtering matrix. #&#39; @param countslist Particle multiplicities. #&#39; @param lambda_prob Regularization. #&#39; @param l_prob Trend filtering degree. #&#39; #&#39; @return (T x k) matrix containing the alphas, for \\code{prob = exp(alpha)/ #&#39; rowSums(exp(alpha))}. #&#39; @export #&#39; Mstep_prob &lt;- function(resp, H_tf, countslist = NULL, lambda_prob = NULL, l_prob = NULL, x = NULL){ ## Basic setup TT &lt;- length(resp) ## Basic checks stopifnot(is.null(l_prob) == is.null(lambda_prob)) ## If glmnet isn&#39;t actually needed, don&#39;t use it. if(is.null(l_prob) &amp; is.null(lambda_prob)){ ## Calculate the average responsibilities, per time point. if(is.null(countslist)){ resp.avg &lt;- lapply(resp, colMeans) %&gt;% do.call(rbind, .) } else { resp.avg &lt;- lapply(1:TT, FUN = function(ii){ colSums(resp[[ii]])/sum(countslist[[ii]]) }) %&gt;% do.call(rbind, .) } return(resp.avg) ## If glmnet is needed, use it. } else { lambda_range &lt;- function(lam, nlam = 50, lam.max = max(1, 5*lam)){ return(exp(seq(log(lam.max), log(lam), length.out = nlam))) } penalty.facs &lt;- c(rep(0, l_prob+1), rep(1, nrow(H_tf) - l_prob - 1)) resp.predict &lt;- do.call(rbind, lapply(resp, colSums)) fac = resp.predict %&gt;% apply(2, function(mycol){max(mycol)/min(mycol)}) %&gt;% max() if(FALSE){ ## Run GLMNET but if it throws an error, reduce the dynamic range |fac| of ## each column |resp.predict| until it&#39;s okay. error_means_null = NULL fac = 1024 while(is.null(error_means_null)){ print(&quot;fac is&quot;) print(fac) error_means_null &lt;- tryCatch({ resp.predict = resp.predict0 %&gt;% apply(2, function(mycol){mycol = pmax(mycol, max(mycol)/fac)}) glmnet_obj &lt;- glmnet::glmnet(x = H_tf, y = resp.predict, family = &quot;multinomial&quot;, penalty.factor = penalty.facs, maxit = 1e7, lambda = mean(penalty.facs)*lambda_range(lambda_prob), standardize = F, intercept = FALSE) pred_link &lt;- predict(glmnet_obj, newx = H_tf, type = &quot;link&quot;, s = mean(penalty.facs) * lambda_prob)[,,1] }, error = function(e){ return(NULL) }) if(is.null(error_means_null)){ fac = fac/2 resp.predict = resp.predict0 %&gt;% apply(2, function(mycol){mycol = pmax(mycol, max(mycol)/fac)}) } } } else { glmnet_obj &lt;- glmnet::glmnet(x = H_tf, y = resp.predict, family = &quot;multinomial&quot;, penalty.factor = penalty.facs, maxit = 1e7, lambda = mean(penalty.facs)*lambda_range(lambda_prob), standardize = F, intercept = FALSE) } pred_link &lt;- predict(glmnet_obj, newx = H_tf, type = &quot;link&quot;, s = mean(penalty.facs) * lambda_prob)[,,1] return(pred_link) } } This should return a \\(T\\) by \\(K\\) matrix, which we’ll test here: testthat::test_that(&quot;Mstep of pi returns a (T x K) matrix.&quot;, { ## Generate some fake responsibilities and trend filtering matrix TT = 100 numclust = 3 nt = 10 resp = lapply(1:TT, function(tt){ oneresp = runif(nt*numclust) %&gt;% matrix(ncol=numclust) oneresp = oneresp/rowSums(oneresp) }) H_tf &lt;- gen_tf_mat(n = TT, k = 0) ## Check the size pred_link = Mstep_prob(resp, H_tf, l_prob = 0, lambda_prob = 1E-3) testthat::expect_equal(dim(pred_link), c(TT, numclust)) pred_link = Mstep_prob(resp, H_tf) testthat::expect_equal(dim(pred_link), c(TT, numclust)) ## Check the correctness pred_link = Mstep_prob(resp, H_tf) }) Each row of this matrix should contain the fitted values \\(\\alpha_k \\in \\mathbb{R}^3\\) where \\(\\alpha_{kt} = h_t^T w_{k}\\), for.. \\(h_t\\) that are rows of the trend filtering matrix \\(H \\in \\mathbb{R}^{T \\times T}\\). \\(w_k \\in \\mathbb{R}^{n}\\) that are the regression coefficients estimated by glmnet(). Here is a test for the correctness of the M step for \\(\\pi\\). testthat::test_that(&quot;Test the M step of \\pi against CVXR&quot;, {}) 5.5.2 M step for \\(\\Sigma\\) #&#39; M step for cluster covariance (sigma). #&#39; #&#39; @param resp Responsibility. #&#39; @param ylist Data. #&#39; @param mn Means #&#39; @param numclust Number of clusters. #&#39; #&#39; @return (K x d x d) array containing K (d x d) covariance matrices. #&#39; @export #&#39; #&#39; @examples Mstep_sigma &lt;- function(resp, ylist, mn, numclust){ ## Find some sizes TT = length(ylist) ntlist = sapply(ylist, nrow) dimdat = ncol(ylist[[1]]) cs = c(0, cumsum(ntlist)) ## Set up empty residual matrix (to be reused) cs = c(0, cumsum(ntlist)) vars &lt;- vector(mode = &quot;list&quot;, numclust) ylong = do.call(rbind, ylist) ntlist = sapply(ylist, nrow) irows = rep(1:nrow(mn), times = ntlist) for(iclust in 1:numclust){ resp.thisclust = lapply(resp, function(myresp) myresp[,iclust, drop = TRUE]) resp.long = do.call(c, resp.thisclust) mnlong = mn[irows,,iclust] if(is.vector(mnlong)) mnlong = mnlong %&gt;% cbind() resid &lt;- ylong - mnlong resid_weighted &lt;- resp.long * resid sig_temp &lt;- t(resid_weighted) %*% resid/sum(resp.long) vars[[iclust]] &lt;- sig_temp } ## Make into an array sigma_array = array(NA, dim=c(numclust, dimdat, dimdat)) for(iclust in 1:numclust){ sigma_array[iclust,,] = vars[[iclust]] } ## Basic check stopifnot(all(dim(sigma_array) == c(numclust, dimdat, dimdat))) return(sigma_array) } 5.5.3 M step for \\(\\mu\\) This is a big one. It uses the ADMM algorithm as written in section 2 of the paper, reproduced briefly here. We need a convergence checker for the outer layer of LA-ADMM, which checks whether the objective values have plateaued. Next, we define the main function Mstep_mu(). This uses a “locally-adaptive” ADMM paper. M-step_mu() calls la_admm_oneclust() on each cluster \\(k=1,\\cdots, K\\); this function will be introduced shortly. #&#39; Computes the M step for mu. #&#39; #&#39; @param resp Responsibilities of each particle. #&#39; @param ylist #&#39; @param lambda #&#39; @param l #&#39; @param sigma #&#39; @param sigma_eig_by_clust #&#39; @param Dl #&#39; @param Dlp1 #&#39; @param TT #&#39; @param N #&#39; @param dimdat #&#39; @param first_iter #&#39; @param mus #&#39; @param Zs #&#39; @param Ws #&#39; @param uws #&#39; @param uzs #&#39; @param maxdev #&#39; @param x #&#39; @param niter #&#39; @param err_rel #&#39; @param err_abs #&#39; @param zerothresh #&#39; @param local_adapt #&#39; @param local_adapt_niter #&#39; @param rho_init #&#39; @param iter #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples Mstep_mu &lt;- function(resp, ylist, lambda = 0.5, l = 3, sigma, sigma_eig_by_clust = NULL, Dlsqrd, Dl, tDl, Dlp1, TT, N, dimdat, first_iter = TRUE, e_mat, ## Warm startable variables mus = NULL, Zs = NULL, Ws = NULL, uws = NULL, uzs = NULL, ## End of warm startable variables maxdev = NULL, x = NULL, niter = (if(local_adapt) 1e2 else 1e3), err_rel = 1E-3, err_abs = 0, zerothresh = 1E-6, local_adapt = FALSE, local_adapt_niter = 10, rho_init = 0.01, iter = NULL){ #################### ## Preliminaries ### #################### TT = length(ylist) numclust = ncol(resp[[1]]) dimdat = ncol(ylist[[1]]) ntlist = sapply(ylist, nrow) resp.sum = lapply(resp, colSums) %&gt;% do.call(rbind, .) N = sum(unlist(resp.sum)) ## Other preliminaries schur_syl_A_by_clust = schur_syl_B_by_clust = term3list = list() ybarlist = list() ycentered_list = Xcentered_list = yXcentered_list = list() Qlist = list() sigmainv_list = list() for(iclust in 1:numclust){ ## Retrieve sigma inverse from pre-computed SVD, if necessary if(is.null(sigma_eig_by_clust)){ sigmainv = solve(sigma[iclust,,]) } else { sigmainv = sigma_eig_by_clust[[iclust]]$sigma_inv } resp.iclust &lt;- lapply(resp, FUN = function(r) matrix(r[,iclust])) AB &lt;- get_AB_mats(y = y, resp = resp.iclust, Sigma_inv = sigmainv, e_mat = e_mat, N = N, Dlp1 = Dlp1, Dl = Dl, Dlsqrd = Dlsqrd, rho = rho_init, z = NULL, w = NULL, uz = NULL, uw = NULL) ## Store the Schur decomposition schur_syl_A_by_clust[[iclust]] = myschur(AB$A) schur_syl_B_by_clust[[iclust]] = myschur(AB$B) ycentered &lt;- NULL ycentered_list[[iclust]] = ycentered sigmainv_list[[iclust]] = sigmainv } ########################################## ## Run ADMM separately on each cluster ## ######################################### admm_niters = admm_inner_iters = vector(length = numclust, mode = &quot;list&quot;) if(first_iter){ mus = vector(length = numclust, mode = &quot;list&quot;) } Zs &lt;- lapply(1:numclust, function(x) matrix(0, nrow = TT, ncol = dimdat)) Ws &lt;- lapply(1:numclust, function(x) matrix(0, nrow = TT - l, ncol = dimdat)) uzs &lt;- lapply(1:numclust, function(x) matrix(0, nrow = TT, ncol = dimdat)) uws &lt;- lapply(1:numclust, function(x) matrix(0, nrow = TT - l, ncol = dimdat)) ## For every cluster, run LA-ADMM start.time = Sys.time() for(iclust in 1:numclust){ resp.iclust &lt;- lapply(resp, FUN = function(r) matrix(r[,iclust])) ## Possibly locally adaptive ADMM, for now just running with rho == lambda res = la_admm_oneclust(K = (if(local_adapt) local_adapt_niter else 1), local_adapt = local_adapt, iclust = iclust, niter = niter, TT = TT, N = N, dimdat = dimdat, maxdev = maxdev, schurA = schur_syl_A_by_clust[[iclust]], schurB = schur_syl_B_by_clust[[iclust]], sigmainv = sigmainv_list[[iclust]], rho = rho_init, rhoinit = rho_init, ## rho = (if(iclust == 1) rho_init else res$rho/2), ## rhoinit = (if(iclust == 1) rho_init else res$rho/2), sigma = sigma, lambda = lambda, resp = resp.iclust, resp_sum = resp.sum[,iclust], l = l, Dlp1 = Dlp1, Dl = Dl, tDl = tDl, y = ylist, err_rel = err_rel, err_abs = err_abs, zerothresh = zerothresh, sigma_eig_by_clust = sigma_eig_by_clust, em_iter = iter, ## Warm starts from previous *EM* iteration first_iter = first_iter, uw = uws[[iclust]], uz = uzs[[iclust]], z = Zs[[iclust]], w = Ws[[iclust]]) ## Store the results mus[[iclust]] = res$mu admm_niters[[iclust]] = res$kk admm_inner_iters[[iclust]] = res$inner.iter ## Store other things for for warmstart Zs[[iclust]] = res$Z uzs[[iclust]] = res$uz uws[[iclust]] = res$uw Ws[[iclust]] = res$W } ## Aggregate the yhats into one array mu_array = array(NA, dim = c(TT, dimdat, numclust)) for(iclust in 1:numclust){ mu_array[,,iclust] = mus[[iclust]] } ## Each are lists of length |numclust|. return(list(mns = mu_array, admm_niters = admm_niters, admm_inner_iters = admm_inner_iters, ## For warmstarts Zs = Zs, Ws = Ws, uws = uws, uzs = uzs, N = N, ## Return the column rho = res$rho, ## For using in the Sigma M step ycentered_list = ycentered_list, Xcentered_list = Xcentered_list, yXcentered_list = yXcentered_list, Qlist = Qlist )) } The locally adaptive admm for a single cluster involves an inner and outer loop. The inner loop is an ADMM for a fixed \\(\\rho\\). The outer loop is written here in la_admm_oneclust, and runs the inner ADMM with a fixed step-size \\(\\rho\\) while sequentially doubling \\(\\rho\\). #&#39; Locally adaptive ADMM; wrapper to \\code{admm_oneclust()}. #&#39; #&#39; @param K #&#39; @param ... #&#39; #&#39; @return #&#39; #&#39; @examples la_admm_oneclust &lt;- function(K, ...){ ## Initialize arguments for ADMM. args &lt;- list(...) p = args$p l = args$l TT = args$TT dimdat = args$dimdat rhoinit = args$rhoinit ## This initialization can come from the previous *EM* iteration. if(args$first_iter){ mu = matrix(0, nrow = TT, ncol = dimdat) Z &lt;- matrix(0, nrow = TT, ncol = dimdat) W &lt;- matrix(0, nrow = TT-l, ncol = dimdat) uz &lt;- matrix(0, nrow = TT, ncol = dimdat) uw &lt;- matrix(0, nrow = TT - l, ncol = dimdat) args[[&#39;mu&#39;]] &lt;- mu args[[&#39;z&#39;]] &lt;- Z args[[&#39;w&#39;]] &lt;- W args[[&#39;uz&#39;]] &lt;- uz args[[&#39;uw&#39;]] &lt;- uw } cols = c() some_admm_objectives = c() ## Run ADMM repeatedly with (1) double rho, and (2) previous b for(kk in 1:K){ if(kk &gt; 1){ ## These ensure warm starts are true args[[&#39;mu&#39;]] &lt;- mu args[[&#39;z&#39;]] &lt;- Z args[[&#39;w&#39;]] &lt;- W args[[&#39;uz&#39;]] &lt;- uz args[[&#39;uw&#39;]] &lt;- uw args[[&#39;rho&#39;]] &lt;- rho } ## Run ADMM args[[&#39;outer_iter&#39;]] &lt;- kk ## Call main function argn &lt;- lapply(names(args), as.name) names(argn) &lt;- names(args) call &lt;- as.call(c(list(as.name(&quot;admm_oneclust&quot;)), argn)) res = eval(call, args) if(any(abs(res$mu)&gt;1E2)){ stop(&quot;mu is blowing up! Probably because the initial ADMM step size (rho) is too large (and possibly the ball constraint on the means is large.&quot;) } some_admm_objectives = c(some_admm_objectives, res$single_admm_objective) ## Handling the scenario where the objectives are all zero padding = 1E-12 some_admm_objectives = some_admm_objectives + padding ## See if outer iterations should terminate if(res$converge){ res$converge &lt;- T break } ## Update some parameters; double the rho value, and update the B matrix rho = 2 * args$rho mu = res$mu Z = res$Z W = res$W uz = res$uz uw = res$uw } if(!res$converge) { ## warning(&quot;ADMM didn&#39;t converge for one cluster.&quot;) ## Disabling this because it&#39;s not really something to fix. The only thing change a user would make is to make |rho_init| smaller } ## Record how long the admm took; in terms of # iterations. res$kk = kk ## Record the final rho res$rho = args$rho return(res) } Next, the inner loop. The workhorse admm_oneclust() actually performs the ADMM update for one cluster for a fixed step-size \\(\\rho\\) across optimization iterations iter=1,...,n. This admm_oneclust() uses the following helpers: W_update_fused(): this uses the prox function, which uses a RCpp function. prox_dp. Z_update() U_update_Z() U_update_W() A prox_dp function, written in C, will be used. #&#39; Solve a fused lasso problem for the W update. Internally, a fused lasso dynamic #&#39; programming solver \\code{prox()} (which calls \\code{prox_dp()} written in C) #&#39; is used. #&#39; W_update_fused &lt;- function(l, TT, mu, uw, rho, lambda, Dl){ # modified lambda for fused lasso routine mod_lam &lt;- lambda/rho # generating pseudo response xi if( l &lt; 0 ){ stop(&quot;l should never be /below/ zero!&quot;) } else if( l == 0 ){ xi &lt;- mu + 1/rho * uw ## This is faster } else { xi &lt;- Dl %*% mu + 1/rho * uw if(any(is.nan(xi))) browser() ## l = 2 is quadratic trend filtering ## l = 1 is linear trend filtering ## l = 0 is fused lasso ## D^{(1)} is first differences, so it correponds to l=0 ## Dl = gen_diff_mat(n = TT, l = l, x = x) &lt;--- (T-l) x T matrix } ## Running the fused LASSO ## which solves min_zhat 1/2 |z-zhat|_2^2 + lambda |D^{(1)}zhat| ## fit &lt;- prox(z = xi, lam = mod_lam) ## fit &lt;- prox_dp(z = xi, lam = mod_lam) ## instead of FlowTF::prox() ## fit &lt;- flowtrendprox::prox_dp(z = xi, lam = mod_lam) fit &lt;- FlowTF::prox(z = xi, lam = mod_lam) ## TODO: eventually change to fit &lt;- flowtrendprox::prox(z = xi, lam = mod_lam) return(fit) } ## This function is in FlowTF now. It&#39;s the last function there! ## #&#39; Fused LASSO for scalar inputs. ## #&#39; ## #&#39; @param z scalar input to be smoothed via the fused LASSO ## #&#39; @param lam Fused LASSO smoothing parameter ## #&#39; ## #&#39; @return Estimates of the fused LASSO solution ## #&#39; @export prox ## #&#39; ## #&#39; @references All credit for writing this function goes to Ryan Tibshirani. See ## #&#39; the original code for calling this function at ## #&#39; ## #&#39; @useDynLib FlowTF prox_dp ## prox &lt;- function(z, lam) { ## o &lt;- .C(&quot;prox_dp&quot;, ## as.integer(length(z)), ## as.double(z), ## as.double(lam), ## as.double(numeric(length(z))), ## # dup=FALSE, ## PACKAGE=&quot;FlowTF&quot;) ## return(o[[4]]) ## } Z_update &lt;- function(m, Uz, C, rho){ mat = m + Uz/rho Z = projCmat(mat, C) return(Z) } #&#39; Project rows of matrix \\code{mat} into a ball of size \\code{C}. #&#39; #&#39; @param mat Matrix whose rows will be projected into a C-sized ball. #&#39; @param C radius #&#39; #&#39; @return Projected matrix. projCmat &lt;- function(mat, C){ if(!is.null(C)){ vlens = sqrt(rowSums(mat * mat)) inds = which(vlens &gt; C) if(length(inds) &gt; 0){ mat[inds,] = mat[inds,] * C / vlens[inds] } } return(mat) } We will test the projCmat function. testthat::test_that(&quot;Test the ball projection&quot;, { set.seed(100) mat = matrix(rnorm(100), ncol=2) projected_mat = flowtrend:::projCmat(mat, 1) testthat::expect_true(all(projected_mat %&gt;% apply(1, function(myrow)sum(myrow*myrow)) &lt; 1+1E-8)) }) ## Test passed 🎉 The other parameter updates are done with these functions. #&#39; @param U (T x dimdat) matrix. U_update_Z &lt;- function(U, rho, mu, Z, TT){ # return(U + rho * (scale(mu, scale = F) - Z)) stopifnot(nrow(U) == TT) centered_mu = sweep(mu, 2, colMeans(mu)) ## stopifnot(all(abs(colMeans(centered_mu))&lt;1E-8)) Unew = U + rho * (centered_mu - Z) ## Expect a (T-l) x dimdat matrix. stopifnot(all(dim(U) == dim(Unew))) stopifnot(nrow(U) == TT) return(Unew) } #&#39; @param U ((T-l) x dimdat) matrix. U_update_W &lt;- function(U, rho, mu, W, l, Dl, TT){ # l = 2 is quadratic trend filtering # l = 1 is linear trend filtering # l = 0 is fused lasso # D^{(1)} is first differences, so it correponds to l=0 # D^{(l+1)} is used for order-l trend filtering. stopifnot(nrow(W) == TT - l) Unew &lt;- U + rho * (Dl %*% mu - W) ## Expect a (T-l) x dimdat matrix. stopifnot(all(dim(U) == dim(Unew))) stopifnot(nrow(U) == TT-l) return(Unew) } Here is that main workhorse admm_oneclust(). #&#39; One cluster&#39;s ADMM for a fixed step size (\\code{rho}). #&#39; #&#39; #&#39; @export #&#39; @param iclust #&#39; @param niter #&#39; @param y #&#39; @param Dl #&#39; @param tDl #&#39; @param Dlp1 #&#39; @param l Default: NULL #&#39; @param TT #&#39; @param N #&#39; @param dimdat #&#39; @param maxdev #&#39; @param rho #&#39; @param rhoinit Default: rho #&#39; @param Xinv #&#39; @param schurA #&#39; @param schurB #&#39; @param sigmainv #&#39; @param lambda #&#39; @param resp #&#39; @param resp_sum #&#39; @param ylist #&#39; @param err_rel Default: 1e-3 #&#39; @param err_abs Default: 0 #&#39; @param zerothresh #&#39; @param mu #&#39; @param z #&#39; @param w #&#39; @param uw #&#39; @param uz #&#39; @param first_iter #&#39; @param em_iter #&#39; @param outer_iter #&#39; @param local_adapt #&#39; @param sigma #&#39; @param sigma_eig_by_clust #&#39; #&#39; @return A list containing the updated parameters for the cluster. #&#39; @seealso \\code{\\link{la_admm_oneclust}} which is a wrapper over this function. admm_oneclust &lt;- function(iclust = 1, niter, y, Dl, tDl, Dlp1, l = NULL, TT, N, dimdat, maxdev, rho, rhoinit = rho, Xinv, schurA, schurB, sigmainv, lambda, resp, resp_sum, ylist, err_rel = 1e-3, err_abs = 0, zerothresh, mu, z, w, uw, uz, first_iter,## Not used em_iter, outer_iter, local_adapt, sigma, sigma_eig_by_clust){ ## Initialize the variables ### ## resid_mat = matrix(NA, nrow = ceiling(niter/5), ncol = 4) ## colnames(resid_mat) = c(&quot;primresid&quot;, &quot;primerr&quot;, &quot;dualresid&quot;, &quot;dualerr&quot;) resid_mat = matrix(NA, nrow = ceiling(niter/5), ncol = 6) colnames(resid_mat) = c(&quot;prim1&quot;, &quot;prim2&quot;, &quot;primresid&quot;, &quot;primerr&quot;, &quot;dualresid&quot;, &quot;dualerr&quot;) rhofac = rho / rhoinit ## This doesn&#39;t change over iterations schurB = myschur(schurB$orig * rhofac) ## In flowmix, this is done on A. Here, it&#39;s done on B (in AX + XB + C = 0). TA = schurA$T TB = schurB$T UA = schurA$Q UB = schurB$Q tUA = schurA$tQ tUB = schurB$tQ ## This also doesn&#39;t change over iterations C1 &lt;- do.call(cbind, lapply(1:TT, FUN = function(tt){ multmat &lt;- apply(y[[tt]], FUN = function(yy) yy * resp[[tt]], MARGIN = 2) sigmainv %*% colSums(multmat) })) for(iter in 1:niter){ syl_C &lt;- get_C_mat(C1 = C1, resp_sum = resp_sum, TT = TT, dimdat = dimdat, Sigma_inv = sigmainv, N = N, Dl = Dl, rho = rho, z = z, w = w, uz = uz, uw = uw, l=l) FF = (-1) * tUA %*% syl_C %*% UB mu = UA %*% matrix_function_solve_triangular_sylvester_barebonesC2(TA, TB, FF) %*% tUB mu = t(mu) stopifnot(nrow(mu) == TT) stopifnot(ncol(mu) == dimdat) centered_mu = sweep(mu, 2, colMeans(mu)) z &lt;- Z_update(centered_mu, Uz = uz, C = maxdev, rho = rho) if(any(abs(mu)&gt;1E2)){ stop(&quot;mu is blowing up!&quot;) ## break } wlist = lapply(1:dimdat, function(j){ W_update_fused(l = l, TT = TT, mu = mu[, j, drop = TRUE], rho = rho, lambda = lambda, uw = uw[,j,drop=TRUE], Dl = Dl)}) w &lt;- do.call(cbind, wlist) stopifnot(nrow(w) == TT-l) stopifnot(ncol(w) == dimdat) uz = U_update_Z(uz, rho, mu, z, TT) ## uw = U_update_W(uw, rho, mu, w, l, TT) uw = U_update_W(uw, rho, mu, w, l, Dl, TT) ## Check convergence if( iter &gt; 1 &amp; iter %% 5 == 0){## &amp; !local_adapt){ ## Calculate convergence criterion obj = check_converge(mu, rho, w, z, w_prev, z_prev, uw, uz, Dl, tDl, err_rel = err_rel, err_abs = err_abs) jj = (iter/ 5) resid_mat[jj,] = c( norm(obj$primal_resid1, &quot;F&quot;), ## Temp norm(obj$primal_resid2, &quot;F&quot;), ## Temp norm(obj$primal_resid, &quot;F&quot;), obj$primal_err, norm(obj$dual_resid,&quot;F&quot;), obj$dual_err) if(is.na(obj$converge)){ obj$converge = converge = FALSE warning(&quot;Convergence was NA&quot;) } if(obj$converge){ converge = TRUE break } else { converge = FALSE } } w_prev = w z_prev = z } if(FALSE){ ## Calculate optimization objective values for this cluster. obj.value &lt;- objective_per_cluster(y = y, mu = mu, resp = resp, Sigma_inv = sigmainv, TT = TT, d = dimdat, Dlp1 = Dlp1, Dl = Dl, l = l, maxdev = maxdev, lambda = lambda, rho = rho, N = N) ## This is very expensive to do within each ADMM iteration, so it&#39;s commented out for now. } obj.value = NA return(list(mu = mu, resid_mat = resid_mat, converge = converge, ## Other variables to return. Z = z, W = w, uz = uz, uw = uw, inner.iter = iter, single_admm_objective = obj.value)) } 5.5.4 Helpers for M step (\\(\\mu\\)) First, let’s start with a few helper functions. #&#39; a #&#39; #&#39; @param TT etilde_mat &lt;- function(TT){ mats &lt;- lapply(1:TT, FUN = function(t){ e_vec &lt;- rep(0, TT) e_vec[t] &lt;- 1 (e_vec - 1/TT) %*% t(e_vec - 1/TT) }) Reduce(&#39;+&#39;, mats) } #&#39; a #&#39; #&#39; @param y get_AB_mats &lt;- function(y, resp, Sigma_inv, e_mat, Dlsqrd, N, Dlp1, Dl, rho, z, w, uz, uw){ # A matrix A &lt;- 1/N * Sigma_inv # B matrix sum_resp &lt;- sapply(resp, sum) #B &lt;- rho*(t(Dl)%*%Dl + e_mat)%*% diag(1/unlist(sum_resp)) B &lt;- rho*(Dlsqrd + e_mat) B &lt;- B/sum_resp[col(B)] #B &lt;- rho*(Dlsqrd + e_mat) %*% diag(1/unlist(sum_resp)) return(list(A = A, B = B)) } #&#39; a #&#39; @param C1 get_C_mat &lt;- function(C1, resp_sum, TT, dimdat, Sigma_inv, e_mat, N, Dlp1, Dl, rho, z, w, uz, uw, l){ C2 &lt;- t(uz - rho*z) # averaging C2 &lt;- C2 - rowMeans(C2) # third component C3 &lt;- do.call(rbind, lapply(1:dimdat, FUN = function(j){ ((uw[,j, drop=TRUE] - rho * w[,j,drop=TRUE]) %*% Dl) %&gt;% as.numeric() })) # combining C &lt;- (-1/N * C1 + C2 + C3) C &lt;- C/resp_sum[col(C)] return(C) } #&#39; @param mat Matrix to Schur-decompose. myschur &lt;- function(mat){ stopifnot(nrow(mat) == ncol(mat)) if(is.numeric(mat) &amp; length(mat)==1) mat = mat %&gt;% as.matrix() obj = Matrix::Schur(mat) obj$tQ = t(obj$Q) obj$orig = mat return(obj) } Here’s a function to check convergence of the ADMM with a fixed step size. #&#39; Check convergence of ADMM with a fixed step size (rho). check_converge &lt;- function(mu, rho, w, z, w_prev, z_prev, uw, uz, Dl, tDl, err_rel = 1E-4, err_abs = 0 ){ ## Constraints are: Ax + Bz =c, where x = mu, z =(w, z) prim1 = rbind(Dl %*% mu, mu - colMeans(mu)) prim2 = rbind(-w, -z) primal_resid = prim1 + prim2 ## Ax + Bz - c change_z = z - z_prev change_w = w - w_prev dual_resid = rho * (-(change_z - colMeans(change_z)) - tDl %*% change_w) tAU = (uz - colMeans(uz)) + tDl %*% uw ## Form primal and dual tolerances. primal_err = sqrt(length(primal_resid)) * err_abs + err_rel * max(norm(prim1, &quot;F&quot;), norm(prim2, &quot;F&quot;)) dual_err = sqrt(length(dual_resid)) * err_abs + err_rel * norm(tAU, &quot;F&quot;) ## Check convergence. primal_resid_size = norm(primal_resid, &quot;F&quot;) dual_resid_size = norm(dual_resid, &quot;F&quot;) primal_converge = ( primal_resid_size &lt;= primal_err ) dual_converge = ( dual_resid_size &lt;= dual_err ) ## Some checks (trying to address problems with |converge|). assertthat::assert_that(is.numeric(primal_resid_size)) assertthat::assert_that(is.numeric(primal_err)) assertthat::assert_that(is.numeric(dual_resid_size)) assertthat::assert_that(is.numeric(dual_err)) ## return(primal_converge &amp; dual_converge) converge = primal_converge &amp; dual_converge return(list( primal_resid1 = prim1, primal_resid2 = prim2, primal_resid = primal_resid, primal_err = primal_err, dual_resid = dual_resid, dual_err = dual_err, converge = converge)) } Here’s a function to compute the augmented Lagrangian of the M-step (this is not used for now). #&#39; computes the Augmented lagrangian. aug_lagr &lt;- function(y, TT, d, z, w, l, uz, uw, mu, resp, Sigma_inv, Dlp1, Dl, maxdev, lambda, rho, N){ mu_dd &lt;- rowMeans(mu) # Check the Z&#39;s for ball constraint, up to a tolerance of 1e-4 znorms &lt;- apply(z, FUN = function(zz) sqrt(sum(zz^2)), MARGIN = 1) if(any(is.na(znorms))) browser() if(any(znorms &gt; (maxdev + 1e-4))){ warning(&quot;||z||_2 &gt; maxdev, current iterate not feasible.&quot;) return(Inf) } aug1 &lt;- sum(do.call(cbind, lapply(1:TT, FUN = function(t){ multmat &lt;- apply(y[[t]], FUN = function(yy){ t(yy - mu[,t]) %*% Sigma_inv %*% (yy - mu[,t])}, MARGIN = 1) sum(1/(2*N) * resp[[t]]*multmat) }))) aug2 &lt;- lambda*sum(do.call(rbind, lapply(1:d, FUN = function(j) sum(abs(diff(w[j,], differences = 1)))))) aug3 &lt;- sum(do.call(cbind, lapply(1:TT, FUN = function(t){ uz[t,]%*%(mu[,t] - mu_dd - z[t,]) + rho/2 * sum((mu[,t] - mu_dd - z[t,])^2) }))) aug4 &lt;- sum(do.call(rbind, lapply(1:d, FUN = function(j){ uw[,j] %*% (Dl %*% mu[j,] - w[j,]) + rho/2 * sum((Dl %*% mu[j,] - w[j,])^2) }))) total &lt;- aug1 + aug2 + aug3 + aug4 return(total) } Here’s a function to calculate the objective value for the ADMM. \\[\\frac{1}{2N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\hat{\\gamma}_{it} (y_i^{(t)} - \\mu_{\\cdot t})^T \\hat{\\Sigma}^{-1} ( y_i^{(t)} - \\mu_{\\cdot t}) + \\lambda \\sum_{j=1}^d \\|D^{(l+1)}\\mu_{j\\cdot }\\|_1\\] (This is the penalized surrogate objective \\(Q_{\\tilde \\theta}(\\mu, \\Sigma, \\pi) + \\lambda \\sum_{j=1}^d \\|D^{(l+1)} \\mu_{j \\cdot}\\|_1\\), taking only the parts, and leaving out a constant \\(C=-\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2} \\log det(\\Sigma_k)\\), since the constant \\(C\\) doesn’t change over ADMM iterations.) #&#39; computes the ADMM objective for one cluster objective_per_cluster &lt;- function(y, TT, d, l, mu, resp, Sigma_inv, Dlp1, Dl, maxdev, lambda, rho, N){ aug1 &lt;- sum(do.call(cbind, lapply(1:TT, FUN = function(tt){ multmat &lt;- apply(y[[tt]], FUN = function(yy){ t(yy - mu[tt,]) %*% Sigma_inv %*% (yy - mu[tt,])}, MARGIN = 1) sum(1/(2*N) * resp[[tt]] * multmat) }))) aug2 &lt;- lambda*sum(do.call(rbind, lapply(1:d, FUN = function(j) sum(abs(diff(mu[,j], differences = l)))))) total &lt;- aug1 + aug2 return(total) } 5.5.5 All Rcpp functions The main function we write in Rcpp is the “barebones” Sylvester equation solver that takes upper-triangular coefficient matrices. // [[Rcpp::depends(RcppArmadillo)]] // [[Rcpp::depends(RcppEigen)]] #include &lt;RcppArmadillo.h&gt; #include &lt;RcppEigen.h&gt; #include &lt;numeric&gt; using namespace arma; using namespace Eigen; using Eigen::Map; // &#39;maps&#39; rather than copies using Eigen::MatrixXd; // variable size matrix, double precision //&#39; Solve &quot;barebones&quot; sylvester equation that takes upper triangular matrices as coefficients. //&#39; //&#39; @param TA Upper-triangular matrix //&#39; @param TB Upper-triangular matrix //&#39; @param C matrix //&#39; @export // [[Rcpp::export]] Eigen::MatrixXd matrix_function_solve_triangular_sylvester_barebonesC2(const Eigen::MatrixXd &amp; TA, const Eigen::MatrixXd &amp; TB, const Eigen::MatrixXd &amp; C){ // Eigen::eigen_assert(TA.rows() == TA.cols()); // Eigen::eigen_assert(TA.Eigen::isUpperTriangular()); // Eigen::eigen_assert(TB.rows() == TB.cols()); // Eigen::eigen_assert(TB.Eigen::isUpperTriangular()); // Eigen::eigen_assert(C.rows() == TA.rows()); // Eigen::eigen_assert(C.cols() == TB.rows()); // typedef typename MatrixType::Index Index; // typedef typename MatrixType::Scalar Scalar; int m = TA.rows(); int n = TB.rows(); Eigen::MatrixXd X(m, n); for (int i = m - 1; i &gt;= 0; --i) { for (int j = 0; j &lt; n; ++j) { // Compute T_A X = \\sum_{k=i+1}^m T_A_{ik} X_{kj} double TAX; if (i == m - 1) { TAX = 0; } else { MatrixXd TAXmatrix = TA.row(i).tail(m-1-i) * X.col(j).tail(m-1-i); TAX = TAXmatrix(0,0); } // Compute X T_B = \\sum_{k=1}^{j-1} X_{ik} T_B_{kj} double XTB; if (j == 0) { XTB = 0; } else { MatrixXd XTBmatrix = X.row(i).head(j) * TB.col(j).head(j); XTB = XTBmatrix(0,0); } X(i,j) = (C(i,j) - TAX - XTB) / (TA(i,i) + TB(j,j)); } } return X; } Also, we define a faster dmvnorm function written in C++. // [[Rcpp::depends(RcppArmadillo)]] #include &lt;RcppArmadillo.h&gt; static double const log2pi = std::log(2.0 * M_PI); /* C++ version of the dtrmv BLAS function */ void inplace_tri_mat_mult(arma::rowvec &amp;x, arma::mat const &amp;trimat){ arma::uword const n = trimat.n_cols; for(unsigned j = n; j-- &gt; 0;){ double tmp(0.); for(unsigned i = 0; i &lt;= j; ++i) tmp += trimat.at(i, j) * x[i]; x[j] = tmp; } } // [[Rcpp::export]] arma::vec dmvnorm_arma_fast(arma::mat const &amp;x, arma::rowvec const &amp;mean, arma::mat const &amp;sigma, bool const logd = false) { using arma::uword; uword const n = x.n_rows, xdim = x.n_cols; arma::vec out(n); arma::mat const rooti = arma::inv(trimatu(arma::chol(sigma))); double const rootisum = arma::sum(log(rooti.diag())), constants = -(double)xdim/2.0 * log2pi, other_terms = rootisum + constants; arma::rowvec z; for (uword i = 0; i &lt; n; i++) { z = (x.row(i) - mean); inplace_tri_mat_mult(z, rooti); out(i) = other_terms - 0.5 * arma::dot(z, z); } if (logd) return out; return exp(out); } // All credit goes to https://gallery.rcpp.org/articles/dmvnorm_arma/ We need this blob (from https://github.com/jacobbien/litr-project/blob/main/examples/make-an-r-package-with-armadillo/create-witharmadillo.Rmd ): usethis::use_rcpp_armadillo(name = &quot;code&quot;) ## ℹ Leaving &#39;src/code.cpp&#39; unchanged. ## ☐ Edit &#39;src/code.cpp&#39;. ## ✔ Adding RcppArmadillo to &#39;LinkingTo&#39; field in DESCRIPTION. ## ✔ Created &#39;src/Makevars&#39; and &#39;src/Makevars.win&#39; with requested compilation settings. usethis::use_rcpp_eigen(name = &quot;code&quot;) ## ℹ Leaving &#39;src/code.cpp&#39; unchanged. ## ☐ Edit &#39;src/code.cpp&#39;. ## ✔ Adding RcppEigen to &#39;LinkingTo&#39; field in DESCRIPTION. ## ✔ Adding &quot;@import RcppEigen&quot; to &#39;R/flowtrend-package.R&#39;. ## ☐ Run `devtools::document()` to update &#39;NAMESPACE&#39;. #&#39; Used as a fallback or to test against \\code{Mstep_mu()}. #&#39; #&#39; @param ylist #&#39; @param resp #&#39; @param lambda #&#39; @param l #&#39; @param Sigma_inv inverse of Sigma #&#39; @param x covariates Mstep_mu_cvxr &lt;- function(ylist, resp, lambda, l, Sigma_inv, x = NULL, thresh = 1E-8, maxdev = NULL, dimdat, N, ecos_thresh = 1E-8, scs_eps = 1E-5){ ## Define dimensions TT = length(ylist) ## Responsibility Weighted Data ytildes &lt;- lapply(1:TT, FUN = function(tt){ yy &lt;- ylist[[tt]] g &lt;- resp[[tt]] yy &lt;- apply(yy, MARGIN = 2, FUN = function(x) x * g) yrow = matrix(c(NA, NA), nrow=1, ncol=dimdat) yrow[1,] = colSums(yy) yrow }) %&gt;% do.call(rbind, .) ## Auxiliary term, needed to make the objective interpretable aux.y &lt;- Reduce(&quot;+&quot;, lapply(1:TT, FUN = function(tt){ yy &lt;- ylist[[tt]] g &lt;- sqrt(resp[[tt]]) yy &lt;- apply(yy, MARGIN = 2, FUN = function(x) x * g) sum(diag(yy %*% Sigma_inv %*% t(yy))) })) ## Mu, d x T matrix mumat &lt;- CVXR::Variable(cols=dimdat, rows=TT) ## Summed sqrt responsibilities - needed in the objective. resp.sum.sqrt &lt;- lapply(resp, FUN = function(x) sqrt(sum(x))) ## Differencing Matrix, (TT-(l+1)) x TT Dlp1 &lt;- gen_diff_mat(n = TT, l = l+1, x = x) # l = 2 is quadratic trend filtering # l = 1 is linear trend filtering # l = 0 is fused lasso ## Forming the objective obj = 1/(2*N) *( Reduce(&quot;+&quot;, lapply(1:TT, FUN = function(tt) CVXR::quad_form(t(resp.sum.sqrt[[tt]]*mumat[tt,]), Sigma_inv))) -2 * Reduce(&quot;+&quot;, lapply(1:TT, FUN = function(tt) t(ytildes[tt,]) %*% Sigma_inv %*% t(mumat[tt,]))) + aux.y) + lambda * sum(CVXR::sum_entries(abs(Dlp1 %*% mumat), axis = 1)) ## Putting together the ball constraint rowmns &lt;- matrix(rep(1, TT^2), nrow = TT)/TT mu_dotdot &lt;- rowmns %*% mumat constraints = list() if(!is.null(maxdev)){ constraints = list(CVXR::sum_entries(CVXR::square(mumat - mu_dotdot), axis = 2) &lt;= rep(maxdev^2, TT) ) } ## Try all two CVXR solvers. prob &lt;- CVXR::Problem(CVXR::Minimize(obj), constraints) result = NULL result &lt;- tryCatch({ CVXR::solve(prob, solver=&quot;ECOS&quot;, FEASTOL = ecos_thresh, RELTOL = ecos_thresh, ABSTOL = ecos_thresh) }, error=function(err){ err$message = paste(err$message, &quot;\\n&quot;, &quot;Lasso solver using ECOS has failed.&quot; ,sep=&quot;&quot;) cat(err$message, fill=TRUE) return(NULL) }) ## If anything is wrong, flag to use SCS solver scs = FALSE if(is.null(result)){ scs = TRUE } else { if(result$status != &quot;optimal&quot;) scs = TRUE } ## Use the SCS solver if(scs){ result = CVXR::solve(prob, solver=&quot;SCS&quot;, eps = scs_eps) if(any(is.na(result$getValue(mumat)))){ ## A clumsy way to check. stop(&quot;Lasso solver using both ECOS and SCS has failed.&quot;, sep=&quot;&quot;) } } ## Record Interesting Parameters num_iters &lt;- result$num_iters status &lt;- result$status mumat &lt;- result$getValue(mumat) val &lt;- result$value return(list(mu = mumat, value = val, status = status, num_iters = num_iters)) } This function solves the following problem: \\[ \\begin{align*} &amp;\\text{minimize}_{\\mu} {\\frac{1}{2N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\hat{\\gamma}_{it} (y_i^{(t)} - \\mu_{\\cdot t})^\\top \\hat{\\Sigma}^{-1} ( y_i^{(t)} - \\mu_{\\cdot t}) + \\lambda \\sum_{j=1}^d \\|D^{(l)}\\mu_{j\\cdot }\\|_1}\\\\ &amp;\\text{subject to}\\;\\; {\\| \\mu_{\\cdot t} - \\bar{\\mu}_{\\cdot \\cdot}\\|_2 \\le r \\;\\;\\forall t=1,\\cdots, T, } \\end{align*} \\] and is directly equivalent to Mstep_mu(). The resulting solution and the objective value should be the same. Let’s check that. First, set up some objects to run Mstep_mu() and Mstep_mu_cvxr(). numclust = 3 TT = 100 dimdat = 1 set.seed(0) dt = gendat_1d(TT = TT, ntlist = rep(TT, 100)) ylist = dt %&gt;% dt2ylist() sigma = init_sigma(ylist, numclust) ## (T x numclust x (dimdat x dimdat)) mn = init_mn(ylist, numclust, TT, dimdat)##, countslist = countslist) prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. resp = Estep(mn, sigma, prob, ylist = ylist, numclust) lambda = .01 l = 1 x = 1:TT Dlp1 = gen_diff_mat(n = TT, l = l+1, x = x) Dl = gen_diff_mat(n = TT, l = l, x = x) Dlsqrd &lt;- t(Dl) %*% Dl maxdev = NULL Then, compare the result of the two implementations. They should look identical. ## overall ADMM res1 = Mstep_mu(resp, ylist, lambda, l=l, sigma=sigma, Dlsqrd = Dlsqrd, Dl=Dl, Dlp1=Dlp1, TT=TT, N=N, dimdat=dimdat, e_mat=etilde_mat(TT = TT), maxdev = maxdev) mn1 = res1$mns ## CVXR just ONE cluster res2list = lapply(1:numclust, function(iclust){ Sigma_inv_oneclust = solve(sigma[iclust,,]) resp_oneclist = lapply(resp, function(resp_onetime){resp_onetime[,iclust, drop=FALSE]}) N = sum(unlist(resp)) res2 = Mstep_mu_cvxr(ylist, resp_oneclust, lambda, l, Sigma_inv_oneclust, thresh = 1E-8, maxdev = maxdev, dimdat, N) res2$mu }) mn2 = array(NA, dim=c(100, dimdat, 3)) for(iclust in 1:numclust){ mn2[,,iclust] = res2list[[iclust]] %&gt;% as.matrix() } ## Plot them plot(x = dt$time, y=dt$Y, col = rgb(0,0,0,0.04), main = &#39;admm (solid) vs cvxr (dashed)&#39;, ylab = &quot;&quot;, xlab = &quot;time&quot;) mn1[,1,] %&gt;% matlines(lwd = 1, lty = 1) mn2[,1,] %&gt;% matlines(lwd = 3, lty = 3) Next, do this for uneven inputs. ## Setup numclust = 3 TT = 100 dimdat = 1 set.seed(0) dt = gendat_1d(TT = TT, ntlist = rep(TT, 100)) ylist = dt %&gt;% dt2ylist() ## Subset them ylist = ylist[-seq(from=10,to=100,by=10)] x = (1:100)[-seq(from=10,to=100,by=10)] sigma = init_sigma(ylist, numclust) ## (T x numclust x (dimdat x dimdat)) mn = init_mn(ylist, numclust, TT, dimdat)##, countslist = countslist) prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. resp = Estep(mn, sigma, prob, ylist = ylist, numclust) lambda = .1 l = 2 ##x = 1:TT Dlp1 = gen_diff_mat(n = length(x), l = l+1, x = x) Dl = gen_diff_mat(n = length(x), l = l, x = x) Dlsqrd &lt;- t(Dl) %*% Dl maxdev = NULL ## Try the algorithm itelf. set.seed(100) obj = flowtrend_once(ylist = ylist, x = x, lambda = .1, lambda_prob = .1, l = 2, l_prob = 2, maxdev = 5, numclust = 3, rho_init = 0.01, verbose = TRUE) plot_1d(ylist=ylist, obj=obj) plot(obj$objectives, type =&#39;l&#39;) mn1 = obj$mn obj$mn[,1,] %&gt;% diff() %&gt;% diff() %&gt;% diff() %&gt;% matplot(type=&#39;l&#39;) mn[,1,] %&gt;% diff() %&gt;% diff() %&gt;% diff() %&gt;% matplot(type=&#39;l&#39;) mn[,1,] %&gt;% matplot(type=&#39;l&#39;) ## Okay, so cluster 3 has a very irregular mean. plot(x = dt$time, y=dt$Y, col = rgb(0,0,0,0.04), main = &#39;admm (solid) vs cvxr (dashed)&#39;, ylab = &quot;&quot;, xlab = &quot;time&quot;) mn_old[,1,] %&gt;% matlines(lwd = 1, x=x, lty = 1) mn_less_old[,1,] %&gt;% matlines(lwd = 1, x=x, lty = 2) mn[,1,] %&gt;% matlines(lwd = 1, x=x, lty = 3) 5.6 The main “flowtrend” function Now we’ve assembled all ingredients we need, we’ll build the main function flowtrend_once() to estimate a flowtrend model. Here goes: #&#39; Estimate flowtrend model once. #&#39; #&#39; @param ylist Data. #&#39; @param countslist Counts corresponding to multiplicities. #&#39; @param x Times, if points are not evenly spaced. Defaults to NULL, in which #&#39; case the value becomes \\code{1:T}, for the $T==length(ylist)$. #&#39; @param numclust Number of clusters. #&#39; @param niter Maximum number of EM iterations. #&#39; @param l Degree of differencing for the mean trend filtering. l=0 will give #&#39; you piecewise constant means; l=1 is piecewise linear, and so forth. #&#39; @param l_prob Degree of differencing for the probability trend filtering. #&#39; @param mn Initial value for cluster means. Defaults to NULL, in which case #&#39; initial values are randomly chosen from the data. #&#39; @param lambda Smoothing parameter for means #&#39; @param lambda_prob Smoothing parameter for probabilities #&#39; @param verbose Loud or not? EM iteration progress is printed. #&#39; @param tol_em Relative numerical improvement of the objective value at which #&#39; to stop the EM algorithm #&#39; @param maxdev Maximum deviation of cluster means across time.. #&#39; @param countslist_overwrite #&#39; @param admm_err_rel #&#39; @param admm_err_abs #&#39; @param admm_local_adapt #&#39; @param admm_local_adapt_niter #&#39; #&#39; @return List object with flowtrend model estimates. #&#39; @export #&#39; #&#39; @examples flowtrend_once &lt;- function(ylist, countslist = NULL, x = NULL, numclust, niter = 1000, l, l_prob = NULL, mn = NULL, lambda = 0, lambda_prob = NULL, verbose = FALSE, tol_em = 1E-4, maxdev = NULL, countslist_overwrite = NULL, ## beta Mstep (ADMM) settings admm = TRUE, admm_err_rel = 1E-3, admm_err_abs = 1E-4, ## Mean M step (Locally Adaptive ADMM) settings admm_local_adapt = TRUE, admm_local_adapt_niter = if(admm_local_adapt) 10 else 1, rho_init = 0.1, ## Other options check_convergence = TRUE, ## Random seed seed = NULL){ ## Basic checks if(!is.null(maxdev)){ assertthat::assert_that(maxdev!=0) } else { maxdev = 1E10 } assertthat::assert_that(numclust &gt; 1) assertthat::assert_that(niter &gt; 1) if(is.null(countslist)){ ntlist = sapply(ylist, nrow) countslist = lapply(ntlist, FUN = function(nt) rep(1, nt)) } if(!is.null(seed)){ assertthat::assert_that(all((seed %&gt;% sapply(., class)) == &quot;integer&quot;)) assertthat::assert_that(length(seed) == 7) } ## Setup for EM algorithm TT = length(ylist) dimdat = ncol(ylist[[1]]) if(is.null(x)) x &lt;- 1:TT if(is.unsorted(x)) stop(&quot;x must be ordered!&quot;) # l = 2 is quadratic trend filtering # l = 1 is linear trend filtering # l = 0 is fused lasso # D^{(1)} is first differences, so it correponds to l=0 Dlp1 = gen_diff_mat(n = TT, l = l+1, x = x) if(l &gt; 1){ facmat = diag(l / diff(x, lag = l)) } else { facmat = diag(rep(1, TT-l)) } Dl = facmat %*% gen_diff_mat(n = TT, l = l, x = x) tDl = t(Dl) Dlsqrd &lt;- t(Dl) %*% Dl e_mat &lt;- etilde_mat(TT = TT) # needed to generate B Dlp1_prob = gen_diff_mat(n = TT, l = l_prob+1, x = x) H_tf &lt;- gen_tf_mat(n = TT, k = l_prob, x = x) if(is.null(mn)){ mn = init_mn(ylist, numclust, TT, dimdat, countslist = countslist, seed = seed) } ntlist = sapply(ylist, nrow) N = sum(ntlist) ## Initialize some objects prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. denslist_by_clust &lt;- NULL objectives = c(+1E20, rep(NA, niter-1)) sigma_fac &lt;- diff(range(do.call(rbind, ylist)))/8 sigma = init_sigma(ylist, numclust, sigma_fac) ## (T x numclust x (dimdat x dimdat)) sigma_eig_by_clust = NULL zero.betas = zero.alphas = list() ## The least elegant solution I can think of.. used only for blocked cv if(!is.null(countslist_overwrite)) countslist = countslist_overwrite #if(!is.null(countslist)) check_trim(ylist, countslist) vals &lt;- vector(length = niter) latest_rho = NA start.time = Sys.time() for(iter in 2:niter){ if(verbose){ print_progress(iter-1, niter-1, &quot;EM iterations.&quot;, start.time = start.time, fill = FALSE) } resp &lt;- Estep(mn, sigma, prob, ylist = ylist, numclust = numclust, denslist_by_clust = denslist_by_clust, first_iter = (iter == 2), countslist = countslist, padding = 1E-8) ## M step (three parts) ## 1. Means res_mu = Mstep_mu(resp, ylist, lambda = lambda, first_iter = (iter == 2), l = l, Dlp1 = Dlp1, Dl = Dl, tDl = tDl, Dlsqrd = Dlsqrd, sigma_eig_by_clust = sigma_eig_by_clust, sigma = sigma, maxdev = maxdev, e_mat = e_mat, Zs = NULL, Ws = NULL, uws = NULL, uzs = NULL, x = x, err_rel = admm_err_rel, err_abs = admm_err_abs, local_adapt = admm_local_adapt, local_adapt_niter = admm_local_adapt_niter, rho_init = rho_init, iter = iter) ## rho_init = (if(iter == 2) rho_init else latest_rho)) ## latest_rho = res_mu$rho mn = res_mu$mns ## 2. Sigma sigma = Mstep_sigma(resp, ylist, mn, numclust) ## 3. Probabilities prob_link = Mstep_prob(resp, countslist = countslist, H_tf = H_tf, lambda_prob = lambda_prob, l_prob = l_prob, x = x) prob = softmax(prob_link) objectives[iter] = objective(ylist = ylist, mu = mn, sigma = sigma, prob = prob, prob_link = prob_link, lambda = lambda, Dlp1 = Dlp1, l = l, countslist = countslist, Dlp1_prob = Dlp1_prob, l_prob = l_prob, lambda_prob = lambda_prob) ## Check convergence if(iter &gt; 10){ if(check_convergence &amp; check_converge_rel(objectives[iter-1], objectives[iter], tol = tol_em) &amp; check_converge_rel(objectives[iter-2], objectives[iter-1], tol = tol_em)&amp; check_converge_rel(objectives[iter-3], objectives[iter-2], tol = tol_em)){ ## check_converge_rel(objectives[iter-4], objectives[iter-3], tol = tol_em)){ break } } } return(structure(list(mn = mn, prob = prob, prob_link = prob_link, sigma = sigma, objectives = objectives[2:iter], final.iter = iter, ## Above is output, below are data/algorithm settings. dimdat = dimdat, TT = TT, N = N, l = l, l_prob = l_prob, x = x, numclust = numclust, lambda = lambda, lambda_prob = lambda_prob, maxdev = maxdev, niter = niter ), class = &quot;flowtrend&quot;)) } Next, flowtrend() is the main user-facing function. #&#39; Main function. Repeats the EM algorithm (\\code{flowtrend_once()}) with |nrep| restarts (5 by default). #&#39; #&#39; @param ... : arguments for \\code{flowtrend_once()} #&#39; @param nrestart : number of random restarts #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples flowtrend &lt;- function(..., nrestart = 5){ args = list(...) if(&quot;verbose&quot; %in% names(args)){ if(args$verbose){ cat(&quot;EM will restart&quot;, nrestart, &quot;times&quot;, fill=TRUE) } } out_models &lt;- lapply(1:nrestart, FUN = function(irestart){ if(&quot;verbose&quot; %in% names(args)){ if(args$verbose){ cat(&quot;EM restart:&quot;, irestart, fill=TRUE) } } model_temp &lt;- flowtrend_once(...) model_obj &lt;- tail(model_temp$objectives, n = 1) if(&quot;verbose&quot; %in% names(args)){ if(args$verbose){ cat(fill=TRUE) } } return(list(model = model_temp, final_objective = model_obj)) }) final_objectives &lt;- sapply(out_models, FUN = function(x) x$final_objective) best_model &lt;- which.min(final_objectives) final_model = out_models[[best_model]][[&quot;model&quot;]] ## Add the objectives final_model$all_objectives = lapply(1:nrestart, function(irestart){ one_model = out_models[[irestart]] data.frame(objective=one_model$model$objectives) %&gt;% mutate(iter=row_number(), irestart=irestart) %&gt;% select(irestart, iter, objective) }) %&gt;% bind_rows() return(final_model) } "],["reordering-clusters.html", "6 Reordering clusters", " 6 Reordering clusters It’s useful to be able to reorder, or permute, one model’s cluster labels (cluster 1,2,.. of newres which are arbitrary) to that of another model origres. The function reorder_kl() does this by (1) taking the posterior probabilities of the particles in ylist_particle (row-binded to be a \\(\\sum_t n_t \\times K\\) matrix), and then (2) using a Hungarian algorithm [@kuhn-hungarian] to best match the two elongated matrices \\(A\\) and \\(B\\) by measuring the symmetric KL divergence between all permutations of the two matrices’ \\(K\\) columns. #&#39; Reorder the cluster numbers for a new flowtrend object \\code{newres}; the best #&#39; permutation (reordering) is to match the original flowtrend object #&#39; \\code{origres}. Also works for flowmix objects. #&#39; #&#39; @param newres New flowtrend object to reorder. #&#39; @param origres Original flowtrend object. #&#39; @param ylist_particle The particle-level data. #&#39; @param fac Defaults to 100, to take 1/100&#39;th of the particles from each time point. #&#39; @param verbose Loud or not? #&#39; #&#39; @return Reordered flowtrend object. #&#39; #&#39; @export reorder_kl &lt;- function(newres, origres, ylist_particle, fac = 100, verbose = FALSE){ ## Randomly sample 1/100 of the original particles (mainly for memory reasons) TT = length(ylist_particle) N = sapply(ylist_particle, nrow) %&gt;% sum() ntlist = sapply(ylist_particle, nrow) indlist = lapply(1:TT, function(tt){ nt = ntlist[[tt]] ind = sample(1:nt, round(nt / fac), replace=FALSE) }) ## Sample responsibilities ylist_particle_small = Map(function(ind, y){ y[ind,,drop = FALSE] }, indlist, ylist_particle) ## Calculate new responsibilities resp_orig_small &lt;- Estep(origres$mn, origres$sigma, origres$prob, ylist = ylist_particle_small, numclust = origres$numclust, first_iter = TRUE) resp_new_small &lt;- Estep(newres$mn, newres$sigma, newres$prob, ylist = ylist_particle_small, numclust = newres$numclust, first_iter = TRUE) assertthat::assert_that(all(sapply(resp_orig_small, dim) == sapply(resp_new_small, dim))) ## Get best ordering (using symm. KL divergence and Hungarian algorithm for ## matching) best_ord &lt;- get_best_match_from_kl(resp_new_small, resp_orig_small) if(verbose) cat(&quot;New order is&quot;, best_ord, fill=TRUE) newres_reordered_kl = newres %&gt;% reorder_clust(ord = best_ord) ## Return the reordered object return(newres_reordered_kl) } This function uses get_best_match_from_kl(), which takes two lists containing responsibilities (posterior probabilities of particles) – one from each model – and returns the cluster ordering to apply to the model that produced resp_new. We define this function and a couple of helper functions next. #&#39; Compute KL divergence from responsibilities between two models&#39; #&#39; responsibilities \\code{resp_new} and \\code{resp_old}. #&#39; #&#39; @param resp_new New responsibilities #&#39; @param resp_orig Original responsiblities. #&#39; #&#39; @return Calculate reordering \\code{o} of the clusters in model represented #&#39; by \\code{resp_new}. To be clear, \\code{o[i]} of new model is the best #&#39; match with the i&#39;th cluster of the original model. #&#39; #&#39; @export #&#39; @importFrom clue solve_LSAP get_best_match_from_kl &lt;- function(resp_new, resp_orig){ ## Basic checks . = NULL ## Fixing check() assertthat::assert_that(all(sapply(resp_new, dim) == sapply(resp_orig , dim))) ## Row-bind all the responsibilities to make a long matrix distmat = form_symmetric_kl_distmat(resp_orig %&gt;% do.call(rbind,.), resp_new %&gt;% do.call(rbind,.)) ## Use Hungarian algorithm to solve. fit &lt;- clue::solve_LSAP(distmat) o &lt;- as.numeric(fit) ## Return the ordering return(o) } #&#39; From two probability matrices, form a (K x K) distance matrix of the #&#39; (n)-vectors. The distance between the vectors is the symmetric KL #&#39; divergence. #&#39; #&#39; @param mat1 Matrix 1 of size (n x K). #&#39; @param mat2 Matrix 2 of size (n x K). #&#39; #&#39; @return K x K matrix containing symmetric KL divergence of each column of #&#39; \\code{mat1} and \\code{mat2}. form_symmetric_kl_distmat &lt;- function(mat1, mat2){ ## Manually add some small, in case some columns are all zero mat1 = (mat1 + 1E-10) %&gt;% pmin(1) mat2 = (mat2 + 1E-10) %&gt;% pmin(1) ## Calculate and return distance matrix. KK1 = ncol(mat1) KK2 = ncol(mat2) distmat = matrix(NA, ncol=KK2, nrow=KK1) for(kk1 in 1:KK1){ for(kk2 in 1:KK2){ mydist = symmetric_kl(mat1[,kk1, drop=TRUE], mat2[,kk2, drop=TRUE]) distmat[kk1, kk2] = mydist } } stopifnot(all(!is.na(distmat))) return(distmat) } #&#39; Symmetric KL divergence, of two probability vectors. #&#39; #&#39; @param vec1 First probability vector. #&#39; @param vec2 Second prbability vector. #&#39; #&#39; @return Symmetric KL divergence (scalar). symmetric_kl &lt;- function(vec1, vec2){ stopifnot(all(vec1 &lt;= 1) &amp; all(vec1 &gt;= 0)) stopifnot(all(vec2 &lt;= 1) &amp; all(vec2 &gt;= 0)) kl &lt;- function(vec1, vec2){ sum(vec1 * log(vec1 / vec2)) } return((kl(vec1, vec2) + kl(vec2, vec1))/2) } Finally, the function that actually performs the manual reordering the clusters of an estimated model obj is reorder_clust(). #&#39; Reorder the results of one object so that cluster 1 through #&#39; \\code{numclust} is in a particular order. The default is decreasing order of #&#39; the averages (over time) of the cluster means. #&#39; #&#39; @param res Model object. #&#39; @param ord Defaults to NULL. Use if you have an ordering in mind. #&#39; #&#39; @return Same object, but with clusters reordered. #&#39; #&#39; @export reorder_clust &lt;- function(res, ord = NULL){ ## Find an order by sums (averages) stopifnot(class(res) == &quot;flowtrend&quot;) if(is.null(ord)) ord = res$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing = TRUE) if(!is.null(ord)) all(sort(ord) == 1:res$numclust) ## Reorder mean res$mn = res$mn[,,ord, drop=FALSE] ## Reorder sigma res$sigma = res$sigma[ord,,,drop=FALSE] ## Reorder prob res$prob = res$prob[,ord, drop=FALSE] ## Reorder the responsibilities ## if(&#39;resp&#39; %in% res){ resp_temp = res$resp for(tt in 1:res$TT){ resp_temp[[tt]] = res$resp[[tt]][,ord] } ## } res$resp = resp_temp return(res) } Here’s an example of how to use this. devtools::load_all(&quot;~/repos/FlowTF&quot;) set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45) dt_model &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45, return_model = TRUE) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() ## Fit model twice. set.seed(2) objlist &lt;- lapply(1:2, function(isim){ flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, nrestart = 1, verbose = TRUE)}) ## Perform the reordering, make three plots newres = objlist[[1]] origres = objlist[[2]] newres_reordered = reorder_kl(newres, origres, ylist, fac = 100, verbose = FALSE) plot_1d(ylist, newres, x = x) + ggtitle(&quot;before reordering, model 1&quot;) plot_1d(ylist, origres, x = x) + ggtitle(&quot;model 2&quot;) plot_1d(ylist, newres_reordered, x = x) + ggtitle(&quot;reordered model 1&quot;) "],["tuning-lambda.html", "7 Tuning \\(\\lambda\\) 7.1 Predicting and evaluating on new time points 7.2 Maximum \\((\\lambda_\\mu, \\lambda_\\pi)\\) values to test 7.3 Define CV data folds 7.4 CV = many single jobs 7.5 Running cross-validation 7.6 Summarizing the output 7.7 CV on your own computer 7.8 CV produces too many files? No problem 7.9 CV for unevenly spaced data 7.10 Documenting the package and building", " 7 Tuning \\(\\lambda\\) Now that the flowtrend() function has been built (we will test it in the next section, i.e., 6test.Rmd), we need to build up quite a few functions before we’re able to cross-validate. These include: Predicting out-of-sample, using predict_flowtrend(). Evaluating data fit (by likelihood) in an out-of-sample measurement, using objective(..., unpenalized = TRUE). Numerically estimating the maximum regularization values to test, using get_max_lambda(). Making data splits, using make_cv_folds(). 7.1 Predicting and evaluating on new time points First, let’s write a couple of functions interpolate_mn() and interpolate_prob() which linearly interpolate the means and probabilities at new time points. #&#39; Do a linear interpolation of the cluster means. #&#39; #&#39; @param x Training times. #&#39; @param tt Prediction time. #&#39; @param iclust Cluster number. #&#39; @param mn length(x) by dimdat by numclust matrix. #&#39; #&#39; @return A dimdat-length vector. interpolate_mn &lt;- function(x, tt, iclust, mn){ ## Basic checks stopifnot(length(x) == dim(mn)[1]) stopifnot(iclust &lt;= dim(mn)[3]) if(tt %in% x) return(mn[which(x==tt),,iclust,drop=TRUE]) ## Set up for linear interpolation floor_t &lt;- max(x[which(x &lt;= tt)]) ceiling_t &lt;- min(x[which(x &gt;= tt)]) floor_t_ind &lt;- which(x == floor_t) ceiling_t_ind &lt;- which(x == ceiling_t) ## Do the linear interpolation mn_t &lt;- mn[ceiling_t_ind,,iclust,drop=TRUE]*(tt - floor_t)/(ceiling_t - floor_t) + mn[floor_t_ind,,iclust,drop=TRUE]*(ceiling_t - tt)/(ceiling_t - floor_t) ## Basic checks stopifnot(length(mn_t) == dim(mn)[2]) return(mn_t) } #&#39; Do a linear interpolation of the cluster means. #&#39; #&#39; @param x Training times. #&#39; @param tt Prediction time. #&#39; @param iclust Cluster number. #&#39; @param prob length(x) by numclust array or matrix. #&#39; #&#39; @return One probability. interpolate_prob &lt;- function(x, tt, iclust, prob){ ## Basic checks numdat = dim(prob)[1] numclust = dim(prob)[2] stopifnot(length(x) == numdat) stopifnot(iclust &lt;= numclust) if(tt %in% x) return(prob[which(x == tt),iclust,drop=TRUE]) ## Set up for linear interpolation floor_t &lt;- max(x[which(x &lt;= tt)]) ceiling_t &lt;- min(x[which(x &gt;= tt)]) floor_t_ind &lt;- which(x == floor_t) ceiling_t_ind &lt;- which(x == ceiling_t) ## Do the linear interpolation prob_t &lt;- prob[ceiling_t_ind,iclust,drop=TRUE]*(tt - floor_t)/(ceiling_t - floor_t) + prob[floor_t_ind,iclust,drop=TRUE]*(ceiling_t - tt)/(ceiling_t - floor_t) # linear interpolation between floor_t and ceiling_t ## Basic checks stopifnot(length(prob_t) == 1) stopifnot(0 &lt;= prob_t &amp; prob_t &lt;= 1) return(prob_t) } Next, let’s build a prediction function predict_flowtrend() which takes the model object obj, and the new time points newtimes, and produces. #&#39; Prediction: Given new timepoints in the original time interval,generate a set #&#39; of means and probs (and return the same Sigma). #&#39; #&#39; @param obj Object returned from covariate EM flowtrend(). #&#39; @param newtimes New times at which to make predictions. #&#39; #&#39; @return List containing mean, prob, and sigma, and x. #&#39; #&#39; @export #&#39; predict_flowtrend &lt;- function(obj, newtimes = NULL){ ## Check the dimensions newx &lt;- newtimes if(is.null(newtimes)){ newx = obj$x } ## Check if the new times are within the time range of the original data stopifnot(all(sapply(newx, FUN = function(t) t &gt;= min(obj$x) &amp; t &lt;= max(obj$x)))) ## Setup some things x &lt;- obj$x TT_new = length(newx) numclust = obj$numclust dimdat = obj$dimdat ## Predict the means (manually). newmn_array = array(NA, dim = c(TT_new, dimdat, numclust)) for(iclust in 1:numclust){ newmn_oneclust &lt;- lapply(newx, function(tt){ interpolate_mn(x, tt, iclust, obj$mn) }) %&gt;% do.call(rbind, . ) newmn_array[,,iclust] = newmn_oneclust } ## Predict the probs. newprob = array(NA, dim = c(TT_new, numclust)) for(iclust in 1:numclust){ newprob_oneclust &lt;- lapply(newx, function(tt){ interpolate_prob(x, tt, iclust, obj$prob) }) %&gt;% do.call(c, .) newprob[,iclust] = newprob_oneclust } ## Basic checks stopifnot(all(dim(newprob) == c(TT_new,numclust))) stopifnot(all(newprob &gt;= 0)) stopifnot(all(newprob &lt;= 1)) ## Return the predictions return(list(mn = newmn_array, prob = newprob, sigma = obj$sigma, x = newx)) } Here’s a quick test (no new data) to make sure this function returns a list containing: the mean, probability, covariance, and new times. testthat::test_that(&quot;The prediction function returns the right things&quot;, { ## Generate data set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100)) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, ## nrestart = 1, niter = 3) predobj = predict_flowtrend(obj) testthat::expect_named(predobj, c(&quot;mn&quot;, &quot;prob&quot;, &quot;sigma&quot;, &quot;x&quot;)) }) Now, we try to make predictions at new held-out time points held_out=25:35, from a model that is estimated without those time points. ## Generate data set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100)) dt_model &lt;- gendat_1d(100, rep(100, 100), return_model = TRUE) held_out = 25:35 dt_subset = dt %&gt;% subset(time %ni% held_out) ylist = dt_subset %&gt;% dt2ylist() x = dt_subset %&gt;% pull(time) %&gt;% unique() set.seed(686) obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, l = 2, l_prob = 2, lambda = 0.02, lambda_prob = .1, ## nrestart = 5, verbose = TRUE) ## EM will restart 5 times ## EM restart: 1 ## EM iterations. 1 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:34:45 EM iterations. 2 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:34:45 EM iterations. 3 out of 999 with lapsed time 1 seconds and remaining time 332 seconds and will finish at 2025-07-23 12:40:17 EM iterations. 4 out of 999 with lapsed time 1 seconds and remaining time 249 seconds and will finish at 2025-07-23 12:38:54 EM iterations. 5 out of 999 with lapsed time 1 seconds and remaining time 199 seconds and will finish at 2025-07-23 12:38:05 EM iterations. 6 out of 999 with lapsed time 1 seconds and remaining time 166 seconds and will finish at 2025-07-23 12:37:32 EM iterations. 7 out of 999 with lapsed time 1 seconds and remaining time 142 seconds and will finish at 2025-07-23 12:37:08 EM iterations. 8 out of 999 with lapsed time 2 seconds and remaining time 248 seconds and will finish at 2025-07-23 12:38:54 EM iterations. 9 out of 999 with lapsed time 2 seconds and remaining time 220 seconds and will finish at 2025-07-23 12:38:26 EM iterations. 10 out of 999 with lapsed time 2 seconds and remaining time 198 seconds and will finish at 2025-07-23 12:38:04 EM iterations. 11 out of 999 with lapsed time 2 seconds and remaining time 180 seconds and will finish at 2025-07-23 12:37:46 EM iterations. 12 out of 999 with lapsed time 2 seconds and remaining time 164 seconds and will finish at 2025-07-23 12:37:31 EM iterations. 13 out of 999 with lapsed time 2 seconds and remaining time 152 seconds and will finish at 2025-07-23 12:37:19 EM iterations. 14 out of 999 with lapsed time 2 seconds and remaining time 141 seconds and will finish at 2025-07-23 12:37:08 EM iterations. 15 out of 999 with lapsed time 2 seconds and remaining time 131 seconds and will finish at 2025-07-23 12:36:58 EM iterations. 16 out of 999 with lapsed time 3 seconds and remaining time 184 seconds and will finish at 2025-07-23 12:37:51 EM iterations. 17 out of 999 with lapsed time 3 seconds and remaining time 173 seconds and will finish at 2025-07-23 12:37:40 EM iterations. 18 out of 999 with lapsed time 3 seconds and remaining time 164 seconds and will finish at 2025-07-23 12:37:32 EM iterations. 19 out of 999 with lapsed time 3 seconds and remaining time 155 seconds and will finish at 2025-07-23 12:37:23 EM iterations. 20 out of 999 with lapsed time 3 seconds and remaining time 147 seconds and will finish at 2025-07-23 12:37:15 EM iterations. 21 out of 999 with lapsed time 3 seconds and remaining time 140 seconds and will finish at 2025-07-23 12:37:08 EM iterations. 22 out of 999 with lapsed time 4 seconds and remaining time 178 seconds and will finish at 2025-07-23 12:37:46 EM iterations. 23 out of 999 with lapsed time 4 seconds and remaining time 170 seconds and will finish at 2025-07-23 12:37:38 EM iterations. 24 out of 999 with lapsed time 4 seconds and remaining time 162 seconds and will finish at 2025-07-23 12:37:31 EM iterations. 25 out of 999 with lapsed time 4 seconds and remaining time 156 seconds and will finish at 2025-07-23 12:37:25 EM iterations. 26 out of 999 with lapsed time 4 seconds and remaining time 150 seconds and will finish at 2025-07-23 12:37:19 EM iterations. 27 out of 999 with lapsed time 5 seconds and remaining time 180 seconds and will finish at 2025-07-23 12:37:49 EM iterations. 28 out of 999 with lapsed time 5 seconds and remaining time 173 seconds and will finish at 2025-07-23 12:37:42 EM iterations. 29 out of 999 with lapsed time 5 seconds and remaining time 167 seconds and will finish at 2025-07-23 12:37:37 EM iterations. 30 out of 999 with lapsed time 5 seconds and remaining time 162 seconds and will finish at 2025-07-23 12:37:32 EM iterations. 31 out of 999 with lapsed time 5 seconds and remaining time 156 seconds and will finish at 2025-07-23 12:37:26 EM iterations. 32 out of 999 with lapsed time 5 seconds and remaining time 151 seconds and will finish at 2025-07-23 12:37:21 EM iterations. 33 out of 999 with lapsed time 6 seconds and remaining time 176 seconds and will finish at 2025-07-23 12:37:46 EM iterations. 34 out of 999 with lapsed time 6 seconds and remaining time 170 seconds and will finish at 2025-07-23 12:37:41 ## EM restart: 2 ## EM iterations. 1 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:34:51 EM iterations. 2 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:34:51 EM iterations. 3 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:34:51 EM iterations. 4 out of 999 with lapsed time 1 seconds and remaining time 249 seconds and will finish at 2025-07-23 12:39:00 EM iterations. 5 out of 999 with lapsed time 1 seconds and remaining time 199 seconds and will finish at 2025-07-23 12:38:10 EM iterations. 6 out of 999 with lapsed time 1 seconds and remaining time 166 seconds and will finish at 2025-07-23 12:37:38 EM iterations. 7 out of 999 with lapsed time 1 seconds and remaining time 142 seconds and will finish at 2025-07-23 12:37:14 EM iterations. 8 out of 999 with lapsed time 1 seconds and remaining time 124 seconds and will finish at 2025-07-23 12:36:56 EM iterations. 9 out of 999 with lapsed time 1 seconds and remaining time 110 seconds and will finish at 2025-07-23 12:36:42 EM iterations. 10 out of 999 with lapsed time 1 seconds and remaining time 99 seconds and will finish at 2025-07-23 12:36:31 EM iterations. 11 out of 999 with lapsed time 1 seconds and remaining time 90 seconds and will finish at 2025-07-23 12:36:22 EM iterations. 12 out of 999 with lapsed time 2 seconds and remaining time 164 seconds and will finish at 2025-07-23 12:37:36 EM iterations. 13 out of 999 with lapsed time 2 seconds and remaining time 152 seconds and will finish at 2025-07-23 12:37:24 EM iterations. 14 out of 999 with lapsed time 2 seconds and remaining time 141 seconds and will finish at 2025-07-23 12:37:14 EM iterations. 15 out of 999 with lapsed time 2 seconds and remaining time 131 seconds and will finish at 2025-07-23 12:37:04 EM iterations. 16 out of 999 with lapsed time 2 seconds and remaining time 123 seconds and will finish at 2025-07-23 12:36:56 EM iterations. 17 out of 999 with lapsed time 2 seconds and remaining time 116 seconds and will finish at 2025-07-23 12:36:49 EM iterations. 18 out of 999 with lapsed time 2 seconds and remaining time 109 seconds and will finish at 2025-07-23 12:36:42 EM iterations. 19 out of 999 with lapsed time 2 seconds and remaining time 103 seconds and will finish at 2025-07-23 12:36:36 EM iterations. 20 out of 999 with lapsed time 2 seconds and remaining time 98 seconds and will finish at 2025-07-23 12:36:31 EM iterations. 21 out of 999 with lapsed time 2 seconds and remaining time 93 seconds and will finish at 2025-07-23 12:36:26 EM iterations. 22 out of 999 with lapsed time 2 seconds and remaining time 89 seconds and will finish at 2025-07-23 12:36:22 EM iterations. 23 out of 999 with lapsed time 3 seconds and remaining time 127 seconds and will finish at 2025-07-23 12:37:00 EM iterations. 24 out of 999 with lapsed time 3 seconds and remaining time 122 seconds and will finish at 2025-07-23 12:36:55 EM iterations. 25 out of 999 with lapsed time 3 seconds and remaining time 117 seconds and will finish at 2025-07-23 12:36:51 EM iterations. 26 out of 999 with lapsed time 3 seconds and remaining time 112 seconds and will finish at 2025-07-23 12:36:46 ## EM restart: 3 ## EM iterations. 1 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:34:54 EM iterations. 2 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:34:54 EM iterations. 3 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:34:54 EM iterations. 4 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:34:54 EM iterations. 5 out of 999 with lapsed time 1 seconds and remaining time 199 seconds and will finish at 2025-07-23 12:38:13 EM iterations. 6 out of 999 with lapsed time 1 seconds and remaining time 166 seconds and will finish at 2025-07-23 12:37:40 EM iterations. 7 out of 999 with lapsed time 1 seconds and remaining time 142 seconds and will finish at 2025-07-23 12:37:16 EM iterations. 8 out of 999 with lapsed time 1 seconds and remaining time 124 seconds and will finish at 2025-07-23 12:36:59 EM iterations. 9 out of 999 with lapsed time 1 seconds and remaining time 110 seconds and will finish at 2025-07-23 12:36:45 EM iterations. 10 out of 999 with lapsed time 1 seconds and remaining time 99 seconds and will finish at 2025-07-23 12:36:34 EM iterations. 11 out of 999 with lapsed time 1 seconds and remaining time 90 seconds and will finish at 2025-07-23 12:36:25 EM iterations. 12 out of 999 with lapsed time 1 seconds and remaining time 82 seconds and will finish at 2025-07-23 12:36:17 EM iterations. 13 out of 999 with lapsed time 2 seconds and remaining time 152 seconds and will finish at 2025-07-23 12:37:27 EM iterations. 14 out of 999 with lapsed time 2 seconds and remaining time 141 seconds and will finish at 2025-07-23 12:37:16 EM iterations. 15 out of 999 with lapsed time 2 seconds and remaining time 131 seconds and will finish at 2025-07-23 12:37:06 EM iterations. 16 out of 999 with lapsed time 2 seconds and remaining time 123 seconds and will finish at 2025-07-23 12:36:59 EM iterations. 17 out of 999 with lapsed time 2 seconds and remaining time 116 seconds and will finish at 2025-07-23 12:36:52 EM iterations. 18 out of 999 with lapsed time 2 seconds and remaining time 109 seconds and will finish at 2025-07-23 12:36:45 EM iterations. 19 out of 999 with lapsed time 2 seconds and remaining time 103 seconds and will finish at 2025-07-23 12:36:39 EM iterations. 20 out of 999 with lapsed time 3 seconds and remaining time 147 seconds and will finish at 2025-07-23 12:37:23 EM iterations. 21 out of 999 with lapsed time 3 seconds and remaining time 140 seconds and will finish at 2025-07-23 12:37:17 EM iterations. 22 out of 999 with lapsed time 3 seconds and remaining time 133 seconds and will finish at 2025-07-23 12:37:10 EM iterations. 23 out of 999 with lapsed time 3 seconds and remaining time 127 seconds and will finish at 2025-07-23 12:37:04 EM iterations. 24 out of 999 with lapsed time 3 seconds and remaining time 122 seconds and will finish at 2025-07-23 12:36:59 EM iterations. 25 out of 999 with lapsed time 3 seconds and remaining time 117 seconds and will finish at 2025-07-23 12:36:54 EM iterations. 26 out of 999 with lapsed time 4 seconds and remaining time 150 seconds and will finish at 2025-07-23 12:37:27 EM iterations. 27 out of 999 with lapsed time 4 seconds and remaining time 144 seconds and will finish at 2025-07-23 12:37:22 EM iterations. 28 out of 999 with lapsed time 4 seconds and remaining time 139 seconds and will finish at 2025-07-23 12:37:17 EM iterations. 29 out of 999 with lapsed time 4 seconds and remaining time 134 seconds and will finish at 2025-07-23 12:37:12 EM iterations. 30 out of 999 with lapsed time 5 seconds and remaining time 162 seconds and will finish at 2025-07-23 12:37:40 EM iterations. 31 out of 999 with lapsed time 5 seconds and remaining time 156 seconds and will finish at 2025-07-23 12:37:34 EM iterations. 32 out of 999 with lapsed time 5 seconds and remaining time 151 seconds and will finish at 2025-07-23 12:37:30 EM iterations. 33 out of 999 with lapsed time 5 seconds and remaining time 146 seconds and will finish at 2025-07-23 12:37:25 EM iterations. 34 out of 999 with lapsed time 5 seconds and remaining time 142 seconds and will finish at 2025-07-23 12:37:21 EM iterations. 35 out of 999 with lapsed time 6 seconds and remaining time 165 seconds and will finish at 2025-07-23 12:37:44 EM iterations. 36 out of 999 with lapsed time 6 seconds and remaining time 160 seconds and will finish at 2025-07-23 12:37:40 EM iterations. 37 out of 999 with lapsed time 6 seconds and remaining time 156 seconds and will finish at 2025-07-23 12:37:36 EM iterations. 38 out of 999 with lapsed time 6 seconds and remaining time 152 seconds and will finish at 2025-07-23 12:37:32 EM iterations. 39 out of 999 with lapsed time 7 seconds and remaining time 172 seconds and will finish at 2025-07-23 12:37:52 EM iterations. 40 out of 999 with lapsed time 7 seconds and remaining time 168 seconds and will finish at 2025-07-23 12:37:49 EM iterations. 41 out of 999 with lapsed time 7 seconds and remaining time 164 seconds and will finish at 2025-07-23 12:37:45 EM iterations. 42 out of 999 with lapsed time 7 seconds and remaining time 160 seconds and will finish at 2025-07-23 12:37:41 EM iterations. 43 out of 999 with lapsed time 8 seconds and remaining time 178 seconds and will finish at 2025-07-23 12:37:59 EM iterations. 44 out of 999 with lapsed time 8 seconds and remaining time 174 seconds and will finish at 2025-07-23 12:37:56 EM iterations. 45 out of 999 with lapsed time 8 seconds and remaining time 170 seconds and will finish at 2025-07-23 12:37:52 EM iterations. 46 out of 999 with lapsed time 8 seconds and remaining time 166 seconds and will finish at 2025-07-23 12:37:48 EM iterations. 47 out of 999 with lapsed time 9 seconds and remaining time 182 seconds and will finish at 2025-07-23 12:38:04 EM iterations. 48 out of 999 with lapsed time 9 seconds and remaining time 178 seconds and will finish at 2025-07-23 12:38:01 ## EM restart: 4 ## EM iterations. 1 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:35:03 EM iterations. 2 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:35:03 EM iterations. 3 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:35:03 EM iterations. 4 out of 999 with lapsed time 1 seconds and remaining time 249 seconds and will finish at 2025-07-23 12:39:13 EM iterations. 5 out of 999 with lapsed time 1 seconds and remaining time 199 seconds and will finish at 2025-07-23 12:38:23 EM iterations. 6 out of 999 with lapsed time 1 seconds and remaining time 166 seconds and will finish at 2025-07-23 12:37:50 EM iterations. 7 out of 999 with lapsed time 1 seconds and remaining time 142 seconds and will finish at 2025-07-23 12:37:26 EM iterations. 8 out of 999 with lapsed time 1 seconds and remaining time 124 seconds and will finish at 2025-07-23 12:37:08 EM iterations. 9 out of 999 with lapsed time 1 seconds and remaining time 110 seconds and will finish at 2025-07-23 12:36:54 EM iterations. 10 out of 999 with lapsed time 1 seconds and remaining time 99 seconds and will finish at 2025-07-23 12:36:43 EM iterations. 11 out of 999 with lapsed time 2 seconds and remaining time 180 seconds and will finish at 2025-07-23 12:38:04 EM iterations. 12 out of 999 with lapsed time 2 seconds and remaining time 164 seconds and will finish at 2025-07-23 12:37:49 EM iterations. 13 out of 999 with lapsed time 2 seconds and remaining time 152 seconds and will finish at 2025-07-23 12:37:37 EM iterations. 14 out of 999 with lapsed time 2 seconds and remaining time 141 seconds and will finish at 2025-07-23 12:37:26 EM iterations. 15 out of 999 with lapsed time 2 seconds and remaining time 131 seconds and will finish at 2025-07-23 12:37:16 EM iterations. 16 out of 999 with lapsed time 2 seconds and remaining time 123 seconds and will finish at 2025-07-23 12:37:08 EM iterations. 17 out of 999 with lapsed time 2 seconds and remaining time 116 seconds and will finish at 2025-07-23 12:37:01 EM iterations. 18 out of 999 with lapsed time 2 seconds and remaining time 109 seconds and will finish at 2025-07-23 12:36:54 EM iterations. 19 out of 999 with lapsed time 2 seconds and remaining time 103 seconds and will finish at 2025-07-23 12:36:48 EM iterations. 20 out of 999 with lapsed time 2 seconds and remaining time 98 seconds and will finish at 2025-07-23 12:36:43 EM iterations. 21 out of 999 with lapsed time 2 seconds and remaining time 93 seconds and will finish at 2025-07-23 12:36:38 EM iterations. 22 out of 999 with lapsed time 2 seconds and remaining time 89 seconds and will finish at 2025-07-23 12:36:34 ## EM restart: 5 ## EM iterations. 1 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:35:05 EM iterations. 2 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:35:06 EM iterations. 3 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:35:06 EM iterations. 4 out of 999 with lapsed time 1 seconds and remaining time 249 seconds and will finish at 2025-07-23 12:39:15 EM iterations. 5 out of 999 with lapsed time 1 seconds and remaining time 199 seconds and will finish at 2025-07-23 12:38:25 EM iterations. 6 out of 999 with lapsed time 1 seconds and remaining time 166 seconds and will finish at 2025-07-23 12:37:52 EM iterations. 7 out of 999 with lapsed time 1 seconds and remaining time 142 seconds and will finish at 2025-07-23 12:37:28 EM iterations. 8 out of 999 with lapsed time 1 seconds and remaining time 124 seconds and will finish at 2025-07-23 12:37:10 EM iterations. 9 out of 999 with lapsed time 1 seconds and remaining time 110 seconds and will finish at 2025-07-23 12:36:57 EM iterations. 10 out of 999 with lapsed time 1 seconds and remaining time 99 seconds and will finish at 2025-07-23 12:36:46 EM iterations. 11 out of 999 with lapsed time 2 seconds and remaining time 180 seconds and will finish at 2025-07-23 12:38:07 EM iterations. 12 out of 999 with lapsed time 2 seconds and remaining time 164 seconds and will finish at 2025-07-23 12:37:51 EM iterations. 13 out of 999 with lapsed time 2 seconds and remaining time 152 seconds and will finish at 2025-07-23 12:37:39 EM iterations. 14 out of 999 with lapsed time 2 seconds and remaining time 141 seconds and will finish at 2025-07-23 12:37:28 EM iterations. 15 out of 999 with lapsed time 2 seconds and remaining time 131 seconds and will finish at 2025-07-23 12:37:18 EM iterations. 16 out of 999 with lapsed time 2 seconds and remaining time 123 seconds and will finish at 2025-07-23 12:37:11 EM iterations. 17 out of 999 with lapsed time 3 seconds and remaining time 173 seconds and will finish at 2025-07-23 12:38:01 EM iterations. 18 out of 999 with lapsed time 5 seconds and remaining time 272 seconds and will finish at 2025-07-23 12:39:42 EM iterations. 19 out of 999 with lapsed time 5 seconds and remaining time 258 seconds and will finish at 2025-07-23 12:39:28 EM iterations. 20 out of 999 with lapsed time 5 seconds and remaining time 245 seconds and will finish at 2025-07-23 12:39:16 EM iterations. 21 out of 999 with lapsed time 5 seconds and remaining time 233 seconds and will finish at 2025-07-23 12:39:04 EM iterations. 22 out of 999 with lapsed time 6 seconds and remaining time 266 seconds and will finish at 2025-07-23 12:39:37 EM iterations. 23 out of 999 with lapsed time 6 seconds and remaining time 255 seconds and will finish at 2025-07-23 12:39:26 EM iterations. 24 out of 999 with lapsed time 6 seconds and remaining time 244 seconds and will finish at 2025-07-23 12:39:15 EM iterations. 25 out of 999 with lapsed time 6 seconds and remaining time 234 seconds and will finish at 2025-07-23 12:39:06 EM iterations. 26 out of 999 with lapsed time 6 seconds and remaining time 225 seconds and will finish at 2025-07-23 12:38:57 EM iterations. 27 out of 999 with lapsed time 7 seconds and remaining time 252 seconds and will finish at 2025-07-23 12:39:24 obj$all_objectives %&gt;% mutate(irestart = as.factor(irestart)) %&gt;% ggplot() + geom_line(aes(x=iter, y=objective, group = irestart, col = irestart)) ## Also reorder the cluster labels of the truth, to match the fitted model. ord = obj$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing=TRUE) lookup &lt;- setNames(c(1:obj$numclust), ord) dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %&gt;% as.factor() ## Reorder the cluster labels of the fitted model. obj = reorder_clust(obj) testthat::test_that(&quot;prediction function returns the right things&quot;, { predobj = predict_flowtrend(obj, newtimes = held_out) ## Check a few things testthat::expect_equal(predobj$x, held_out) testthat::expect_equal(rowSums(predobj$prob), rep(1, length(held_out))) testthat::expect_equal(dim(predobj$mn), c(length(held_out), 1, 3)) }) ## Test passed 😸 Plot the predicted means \\(\\mu\\) and probabilities \\(\\pi\\), with purple points at the interpolated means. We can see that it works as expected. predobj = predict_flowtrend(obj, newtimes = held_out) g = plot_1d(ylist = ylist, obj=obj, x = x) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model,## %&gt;% subset(time %ni% held_out), linetype = &quot;dashed&quot;, size=2, alpha = .7) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## ℹ Using compatibility `.name_repair`. ## ℹ The deprecated feature was likely used in the litr package. ## Please report the issue to the authors. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ## Plot the predicted means preds = lapply(1:3, function(iclust){ tibble(mn = predobj$mn %&gt;% .[,,iclust, drop = TRUE], prob = predobj$prob %&gt;% .[,iclust, drop = TRUE], time = held_out, cluster = iclust) }) %&gt;% bind_rows() g + geom_line(aes(x=time, y=mn, group = cluster), data = preds, col = &#39;yellow&#39;, size = 2)##, alpha = .8) The estimated probabilities are shown here, with purple points showing the interpolation. It works as expected. plot_prob(obj, x=x) + geom_line(aes(x = time, y = prob, group = cluster, color = cluster), data = dt_model, linetype = &quot;dashed&quot;) + facet_wrap(~cluster) + geom_line(aes(x = time, y = prob), data = preds, col = &#39;yellow&#39;, size = 3) Let’s now try to space inputs unevenly, by x. set.seed(100) TT = 100 dt &lt;- gendat_1d(TT, rep(100, TT)) dt_model &lt;- gendat_1d(TT, rep(100, TT), return_model = TRUE) ylist_orig = dt %&gt;% dt2ylist() plot_1d(ylist_orig) x = sample(1:TT, floor(TT/2)) %&gt;% sort() ylist = ylist_orig[x] set.seed(55) obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, l = 2, l_prob = 2, lambda = .5, lambda_prob = .5, ## rho_init = .1, nrestart = 1, verbose = TRUE) ## EM will restart 1 times ## EM restart: 1 ## EM iterations. 1 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:35:13 EM iterations. 2 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:35:13 EM iterations. 3 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2025-07-23 12:35:14 EM iterations. 4 out of 999 with lapsed time 1 seconds and remaining time 249 seconds and will finish at 2025-07-23 12:39:23 EM iterations. 5 out of 999 with lapsed time 1 seconds and remaining time 199 seconds and will finish at 2025-07-23 12:38:33 EM iterations. 6 out of 999 with lapsed time 1 seconds and remaining time 166 seconds and will finish at 2025-07-23 12:38:00 EM iterations. 7 out of 999 with lapsed time 1 seconds and remaining time 142 seconds and will finish at 2025-07-23 12:37:36 EM iterations. 8 out of 999 with lapsed time 1 seconds and remaining time 124 seconds and will finish at 2025-07-23 12:37:19 EM iterations. 9 out of 999 with lapsed time 1 seconds and remaining time 110 seconds and will finish at 2025-07-23 12:37:05 EM iterations. 10 out of 999 with lapsed time 2 seconds and remaining time 198 seconds and will finish at 2025-07-23 12:38:33 EM iterations. 11 out of 999 with lapsed time 2 seconds and remaining time 180 seconds and will finish at 2025-07-23 12:38:15 EM iterations. 12 out of 999 with lapsed time 2 seconds and remaining time 164 seconds and will finish at 2025-07-23 12:37:59 EM iterations. 13 out of 999 with lapsed time 2 seconds and remaining time 152 seconds and will finish at 2025-07-23 12:37:47 EM iterations. 14 out of 999 with lapsed time 2 seconds and remaining time 141 seconds and will finish at 2025-07-23 12:37:36 EM iterations. 15 out of 999 with lapsed time 2 seconds and remaining time 131 seconds and will finish at 2025-07-23 12:37:26 EM iterations. 16 out of 999 with lapsed time 2 seconds and remaining time 123 seconds and will finish at 2025-07-23 12:37:18 EM iterations. 17 out of 999 with lapsed time 2 seconds and remaining time 116 seconds and will finish at 2025-07-23 12:37:12 EM iterations. 18 out of 999 with lapsed time 2 seconds and remaining time 109 seconds and will finish at 2025-07-23 12:37:05 EM iterations. 19 out of 999 with lapsed time 3 seconds and remaining time 155 seconds and will finish at 2025-07-23 12:37:51 EM iterations. 20 out of 999 with lapsed time 3 seconds and remaining time 147 seconds and will finish at 2025-07-23 12:37:43 EM iterations. 21 out of 999 with lapsed time 3 seconds and remaining time 140 seconds and will finish at 2025-07-23 12:37:36 EM iterations. 22 out of 999 with lapsed time 3 seconds and remaining time 133 seconds and will finish at 2025-07-23 12:37:29 EM iterations. 23 out of 999 with lapsed time 3 seconds and remaining time 127 seconds and will finish at 2025-07-23 12:37:23 EM iterations. 24 out of 999 with lapsed time 3 seconds and remaining time 122 seconds and will finish at 2025-07-23 12:37:18 EM iterations. 25 out of 999 with lapsed time 3 seconds and remaining time 117 seconds and will finish at 2025-07-23 12:37:13 obj$all_objectives %&gt;% mutate(irestart = as.factor(irestart)) %&gt;% ggplot() + geom_line(aes(x=iter, y=objective, group = irestart, col = irestart)) ## Make mean predictions newtimes = seq(from=min(x),to=max(x),length=10000) predobj = predict_flowtrend(obj, newtimes = newtimes) ## Plot the predicted means preds = lapply(1:3, function(iclust){ tibble(mn = predobj$mn %&gt;% .[,,iclust, drop = TRUE], prob = predobj$prob %&gt;% .[,iclust, drop = TRUE], time = newtimes, cluster = iclust) }) %&gt;% bind_rows() The estimated means \\(\\mu\\) in the training data are shown as solid triangle points. The out-of-sample \\(\\mu\\) predictions made on a fine grid of time points (shown by the yellow lines) look fine. g = plot_1d(ylist=ylist, obj = obj, x = x) g + ggtitle(&quot;Fitted model&quot;) g + geom_line(aes(x=time, y=mn, group = cluster), data = preds, col = &#39;yellow&#39;, size = rel(1), alpha = .7) + ggtitle(&quot;Predictions on fine grid of times&quot;) The out-of-sample \\(\\pi\\) predictions are the lines that connect the points. They look great as well. plot_prob(obj, x=x) + ## geom_line(aes(x = time, y = prob, group = cluster, color = cluster), ## data = dt_model, linetype = &quot;dashed&quot;) + facet_wrap(~cluster) + geom_line(aes(x = time, y = prob), data = preds, col = &#39;yellow&#39;, size = rel(.5), alpha = .7) Next, we’ll try evaluating an estimated model’s prediction in an out-of-sample measurement. This will be measured by the model prediction’s out-of-sample objective (negative log-likelihood). ## Generate data set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100)) dt_model &lt;- gendat_1d(100, rep(100, 100), return_model = TRUE) held_out = 25:35 dt_subset = dt %&gt;% subset(time %ni% held_out) ylist = dt_subset %&gt;% dt2ylist() x = dt_subset %&gt;% pull(time) %&gt;% unique() obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, ## nrestart = 5) ## Make prediction predobj = predict_flowtrend(obj, newtimes = held_out) ## Use the predicted (interpolated) model parameters obj_pred = objective(mu = predobj$mn, prob = predobj$prob, sigma = predobj$sigma, ylist = ylist[held_out], unpenalized = TRUE) truemn = array(NA, dim = dim(predobj$mn)) truemn[,1,] = dt_model %&gt;% select(time, cluster, mean) %&gt;% pivot_wider(names_from = cluster, values_from = mean) %&gt;% subset(time %in% held_out) %&gt;% select(-time) %&gt;% as.matrix() ## Use the true mean obj_better = objective(mu = truemn, prob = predobj$prob, sigma = predobj$sigma, ylist = ylist[held_out], unpenalized = TRUE) ## Here is the estimated model plot_1d(ylist=ylist, obj=obj, x= (1:100)[-held_out]) The out-of-sample prediction is similar for the predicted model and the estimated model. Below, we’re showing just the predicted means at the held-out points, overlaid with data. (This is measured by the objective (= negative log likelihood), so lower is better! Red is worse than black, naturally.) {r fit, fig.width = 7, fig.height = 5}) g = plot_1d(ylist = dt %&gt;% subset(time %in% held_out) %&gt;% dt2ylist(), x = held_out) + xlim(c(0,100)) g + geom_line(aes(x=time, y = value, group = name), data = data.frame(truemn[,1,]) %&gt;% add_column(time = held_out) %&gt;% pivot_longer(-time)) + geom_line(aes(x=time, y = value, group = name), data = data.frame(predobj$mn[,1,]) %&gt;% add_column(time = held_out) %&gt;% pivot_longer(-time), col = 'red') + ggtitle(paste0(round(obj_pred,3), \" (red, predicted) vs. \", round(obj_better, 3), \"(black, truth)\")) 7.2 Maximum \\((\\lambda_\\mu, \\lambda_\\pi)\\) values to test What should the maximum value of regularization parameters to use? It’s useful to be able to calculate the smallest value of regularization parameters that result in fully “simple” \\(\\mu\\) and \\(\\pi\\) over time, in all clusters. Call these \\(\\lambda_\\mu^{\\text{max}}\\) and \\(\\lambda_{\\pi}^{\\text{max}}\\). We use these to form a 2d grid of candidate \\(\\lambda\\) values – logarithmically-spaced pairs of values between starting at \\((\\lambda_{\\mu}^{\\text{max}}, \\lambda_{\\pi}^{\\text{max}})\\) decreasing to some small pair of values. The function get_max_lambda() numerically estimates this maximum pair \\((\\lambda_{\\mu}^{\\text{max}}, \\lambda_{\\pi}^{\\text{max}})\\). It proceeds by first running flowtrend() on a very large pair \\((\\lambda_\\mu, \\lambda_\\pi)\\), then sequentially halving both values while checking if the resulting estimated \\(\\mu\\) and \\(\\pi\\) are all as simple over time. The simplest, most regularized model of an \\(l\\)’th order trend filter estimate will be a single \\(l\\)’th order polynomial; this means that there will be no discontinuities in the \\(l-1\\)’th order differences. (For example, a linear trend filter is \\(l=1\\); the first differences should be piecewise constant, and second differences should be zero except for at the knots. A quadratic trend filter is \\(l=2\\); the first differences should be piecewise linear, the second differences should be piecewise constant, and the third differences should be zero except for at the knots.) As soon as they cease to be simple, we stop and take the immediately previous pair of values of \\((\\lambda_\\mu, \\lambda_\\pi)\\). get_max_lambda() is a wrapper around the workhorse calc_max_lambda(). It obtains the value and saves it to a maxres_file (which defaults to maxres.Rdata) in the destin directory. #&#39; A wrapper for \\code{calc_max_lambda}. Saves the two maximum lambda values in #&#39; a file. #&#39; #&#39; @param destin Where to save the output (A two-lengthed list called #&#39; &quot;maxres&quot;). #&#39; @param maxres_file Filename for output. Defaults to maxres.Rdata. #&#39; @param ... Additional arguments to \\code{flowtrend()}. #&#39; @inheritParams calc_max_lambda #&#39; #&#39; @return No return #&#39; #&#39; @export get_max_lambda &lt;- function(destin, maxres_file = &quot;maxres.Rdata&quot;, ylist, countslist, numclust, maxdev, max_lambda_mean, max_lambda_prob, ...){ if(file.exists(file.path(destin, maxres_file))){ load(file.path(destin, maxres_file)) cat(&quot;Maximum regularization values are loaded.&quot;, fill=TRUE) return(maxres) } else { print(Sys.time()) cat(&quot;Maximum regularization values being calculated.&quot;, fill = TRUE) cat(&quot;with initial lambda values (prob and mu):&quot;, fill = TRUE) print(c(max_lambda_prob, max_lambda_mean)); maxres = calc_max_lambda(ylist = ylist, countslist = countslist, numclust = numclust, maxdev = maxdev, ## This function&#39;s settings max_lambda_prob = max_lambda_prob, max_lambda_mean = max_lambda_mean, ...) print(maxres) save(maxres, file = file.path(destin, maxres_file)) cat(&quot;file was written to &quot;, file.path(destin, maxres_file), fill=TRUE) cat(&quot;maximum regularization value calculation done.&quot;, fill = TRUE) print(Sys.time()) return(maxres) } } The aforementioned workhorse calc_max_lambda() is here. #&#39; Estimate maximum lambda values numerically. First starts with a large #&#39; initial value \\code{max_lambda_mean} and \\code{max_lambda_prob}, and runs #&#39; the EM algorithm on decreasing set of values (sequentially halved). This #&#39; stops once you see non-simple probabilities or means, and returns the *smallest* #&#39; regularization (lambda) value pair that gives full sparsity. #&#39; #&#39; Note that the \\code{zero_stabilize=TRUE} option is used in #&#39; \\code{flowtrend()}, which basically means the EM algorithm runs only until #&#39; the zero pattern stabilizes. #&#39; #&#39; @param ylist List of responses. #&#39; @param numclust Number of clusters. #&#39; @param max_lambda_mean Defaults to 4000. #&#39; @param max_lambda_prob Defaults to 1000. #&#39; @param iimax Maximum value of x for 2^{-x} factors to try. #&#39; @param ... Other arguments to \\code{flowtrend_once()}. #&#39; #&#39; @return list containing the two maximum values to use. #&#39; #&#39; @export calc_max_lambda &lt;- function(ylist, countslist = NULL, numclust, max_lambda_mean = 4000, max_lambda_prob = 1000, verbose = FALSE, iimax = 16, ...){ ## Basic setup: in each dimension, the data should only vary by a relatively ## small amount (say 1/100) dimdat = ncol(ylist[[1]]) toler_by_dim = sapply(1:dimdat, function(idim){ datrange = ylist %&gt;% sapply(FUN = function(y) y %&gt;% .[,idim] %&gt;% range()) %&gt;% range() toler = (datrange[2] - datrange[1])/1E3 }) toler_prob = 1E-3 args = list(...) l = args$l l_prob = args$l_prob ## Get range of regularization parameters. facs = sapply(1:iimax, function(ii) 2^(-ii+1)) ## DECREASING order print(&quot;running the models once&quot;) for(ii in 1:iimax){ cat(&quot;###############################################################&quot;, fill=TRUE) cat(&quot;#### lambda_prob = &quot;, max_lambda_prob * facs[ii], &quot; and lambda = &quot;, max_lambda_mean * facs[ii], &quot;being tested. &quot;, fill=TRUE) cat(&quot;###############################################################&quot;, fill=TRUE) res = flowtrend_once(ylist = ylist, countslist = countslist, numclust = numclust, lambda_prob = max_lambda_prob * facs[ii], lambda = max_lambda_mean * facs[ii], verbose = verbose, ...) ## In each dimension, the data should only vary by a relatively small amount (say 1/100) mean_is_simple = sapply(1:dimdat, FUN = function(idim){ all(abs(diff(res$mn[,idim,], differences = l+1)) &lt; toler_by_dim[idim] * 2^l) }) prob_is_simple = all(abs(diff(res$prob, differences = l_prob+1)) &lt; toler_prob * 2^l_prob) all_are_simple = (all(mean_is_simple) &amp; prob_is_simple) if(!all_are_simple){ ## If there are *any* nonzero values at the first iter, prompt a restart ## with higher initial lambda values. if(ii == 1){ stop(paste0(&quot;Max lambdas: &quot;, max_lambda_mean, &quot; and &quot;, max_lambda_prob, &quot; were too small as maximum reg. values. Go up and try again!!&quot;)) ## If there are *any* nonzero values, return the immediately preceding ## lambda values -- these were the smallest values we had found that gives ## full sparsity. } else { ## Check one more time whether the model was actually zero, by fully running it; res = flowtrend_once(ylist = ylist, countslist = countslist, numclust = numclust, lambda_prob = max_lambda_prob * facs[ii], lambda = max_lambda_mean * facs[ii], ...) ## Check if both curves are maximally simple mean_is_simple = sapply(1:dimdat, FUN = function(idim){ all(abs(diff(res$mn[,idim,], differences = l+1)) &lt; toler_by_dim[idim]) }) prob_is_simple = all(abs(diff(res$prob, differences = l_prob+1)) &lt; toler_prob) all_are_simple = (all(mean_is_simple) &amp; prob_is_simple) ## If there are *any* nonzero values, stop. ## (Otherwise, just proceed to try a smaller set of lambdas.) if(!all_are_simple){ return(list(mean = max_lambda_mean * facs[ii-1], prob = max_lambda_prob * facs[ii-1])) } } } cat(fill=TRUE) } } 7.3 Define CV data folds make_cv_folds() makes the cross-validation “folds”, which are the \\(K\\) (nfold) list of data indices. These are not times! They simply split 1:length(ylist). #&#39; Define the time folds cross-validation. #&#39; #&#39; @param ylist Data. #&#39; @param TT Length of data; if provided, ylist is ignored. #&#39; @param nfold Number of folds. #&#39; @param blocksize Defaults to 1. If larger than 1, creates a set of time folds #&#39; that use contiguous time blocks (by calling #&#39; \\code{make_cv_folds_in_blocks()}). #&#39; @return List of fold indices. #&#39; @export #&#39; make_cv_folds &lt;- function(ylist = NULL, nfold, TT = NULL, blocksize = 1){ if(blocksize &gt; 1){ return(make_cv_folds_in_blocks(ylist = ylist, nfold, TT = TT, blocksize = blocksize)) } ## Make hour-long index list if(is.null(TT)) TT = length(ylist) folds &lt;- rep(1:nfold, ceiling( (TT-2)/nfold))[1:(TT-2)] inds &lt;- lapply(1:nfold, FUN = function(k) (2:(TT-1))[folds == k]) names(inds) = paste0(&quot;Fold&quot;, 1:nfold) return(inds) } We can visualize how the data is to be split. In the following plot, vertical lines mark data indices in each fold, using different colors . For nfold = 5, the first fold is every 5th point starting at 2, \\(\\{2,7,\\dots\\}\\), and the second fold is \\(\\{3,8,\\dots\\}\\), and so forth. Note: the first index \\(1\\) and the last \\(TT\\) are left out at this stage, and instead made available to all folds at training time (in cv_flowtrend()). This is because, otherwise, it would be impossible to make predictions at either ends of the data. nfold = 5 TT = 100 inds = make_cv_folds(nfold = nfold, TT = TT) print(inds) ## $Fold1 ## [1] 2 7 12 17 22 27 32 37 42 47 52 57 62 67 72 77 82 87 92 97 ## ## $Fold2 ## [1] 3 8 13 18 23 28 33 38 43 48 53 58 63 68 73 78 83 88 93 98 ## ## $Fold3 ## [1] 4 9 14 19 24 29 34 39 44 49 54 59 64 69 74 79 84 89 94 99 ## ## $Fold4 ## [1] 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 ## ## $Fold5 ## [1] 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86 91 96 plot(NA, xlim = c(0,TT), ylim=1:2, ylab = &quot;&quot;, xlab = &quot;Data index of ylist&quot;, yaxt = &quot;n&quot;, xaxt=&quot;n&quot;) axis(1, at = c(1, seq(10, 100,10))) for(ifold in 1:nfold){ abline(v = inds[[ifold]], col = ifold, lwd = 2) } This is another function that makes the fold indices but using blocks of time points. #&#39; Define the &quot;blocked&quot; time folds for cross-validation. #&#39; This means that contiguous of times will be used to define CV folds. #&#39; #&#39; The first fold will be ( 1 2 3 16 17 18 31 32 33 46 47 48 61 62 63 76 77 78 #&#39; 91 92 93), the second fold will be (4 5 6 19 20 21 34 35 36 49 50 51 64 65 66 #&#39; 79 80 81 94 95 96), and so forth. #&#39; #&#39; @param nfold Number of folds. #&#39; @param blocksize Size of block (e.g. 3 will produce the example above). #&#39; @return List of fold indices. #&#39; @export #&#39; make_cv_folds_in_blocks &lt;- function(ylist=NULL, nfold, TT=NULL, blocksize){ ## Make hour-long index list if(is.null(TT)) TT = length(ylist) endpoints = round(seq(from = 1, to = TT + blocksize, by = blocksize)) inds = Map(function(begin, end){ if(begin &gt;= TT-1) return(NULL) return(seq(begin+1, pmin(end,TT-1))) }, endpoints[-length(endpoints)], endpoints[-1]) null.elt = sapply(inds, is.null) if(any(null.elt)){ inds = inds[-which(null.elt)] } ## Further make these into (e.g. 5) blocks of test indices. test.ii.list = lapply(1:nfold, function(ifold){ which.test.inds = seq(from = ifold, to = length(inds), by = nfold) test.ii = unlist(inds[which.test.inds]) return(test.ii) }) names(test.ii.list) = paste0(&quot;Fold&quot;, 1:nfold) ## Useful plotting code showing the plots. if(FALSE){ plot(NA, xlim = c(0,TT), ylim=1:2) lapply(1:nfold, function(ifold){ a = test.ii.list[[ifold]]; abline(v=a, col=ifold) }) } return(test.ii.list) } 7.4 CV = many single jobs Next, we build the immediate elements needed for cross-validation. There are two ways in which flowtrend_once() will be applied to data for cross-validation; one is when estimating models from held-in data folds, and the other is when re-estimating models on the full data. Estimating models on the held-in data is done by one_job(). Re-estimating models on the entire dataset is done by one_job_refit(). Here is one_job(). #&#39; Helper function to run ONE job for CV, in iprob, imu, ifold, irestart. #&#39; #&#39; @param iprob Index for prob. #&#39; @param imu Index for beta. #&#39; @param ifold Index for CV folds. #&#39; @param irestart Index for 1 through nrestart. #&#39; @param folds CV folds (from \\code{make_cv_folds()}). #&#39; @param destin Destination directory. #&#39; @param lambda_means List of regularization parameters for mean model. #&#39; @param lambda_probs List of regularization parameters for prob model. #&#39; @param ylist Data. #&#39; @param countslist Counts or biomass. #&#39; @param ... Rest of arguments for \\code{flowtrend_once()}. #&#39; #&#39; @return Nothing is returned. Instead, a file named &quot;1-1-1-1-cvscore.Rdata&quot; #&#39; is saved in \\code{destin}. (The indices here are iprob-imu-ifold-irestart). #&#39; #&#39; @export one_job &lt;- function(iprob, imu, ifold, irestart, folds, destin, lambda_means, lambda_probs, seedtab = NULL, x = NULL, ## The rest that is needed explicitly for flowtrend() ylist, countslist, l, l_prob, ...){ ## Get the train/test data TT &lt;- length(ylist) if(is.null(x)) x = 1:TT test.inds = unlist(folds[ifold]) %&gt;% sort() test.dat = ylist[test.inds] test.count = countslist[test.inds] train.inds = c(1, unlist(folds[-ifold]), TT) %&gt;% sort() train.dat = ylist[train.inds] train.count = countslist[train.inds] ## NEW: fit the model on the *time points* in training indices, and ## NEW: test the model on *time points* in the test indices. ## Check whether this job has been done already. filename = make_cvscore_filename(iprob, imu, ifold, irestart) best_filename = make_best_cvscore_filename(iprob, imu, ifold) ## if(file.exists(file.path(destin, filename)) ){ if(file.exists(file.path(destin, filename)) | file.exists(file.path(destin, best_filename)) ){ cat(fill=TRUE) cat(filename, &quot;already done.&quot;, fill=TRUE) return(NULL) } ## Get the seed ready if(!is.null(seedtab)){ seed = seedtab %&gt;% dplyr::filter(iprob == !!iprob &amp; imu == !!imu &amp; ifold == !!ifold &amp; irestart == !!irestart) %&gt;% dplyr::select(seed1, seed2, seed3, seed4, seed5, seed6, seed7) %&gt;% unlist() %&gt;% as.integer() } else { seed = NULL } lambda_prob = lambda_probs[iprob] lambda_mean = lambda_means[imu] ## Run the algorithm (all this trouble because of |nrestart|) args = list(...) args$ylist = train.dat args$countslist = train.count args$x = x[train.inds] ## NEW args$lambda = lambda_mean args$lambda_prob = lambda_prob args$l = l args$l_prob = l_prob args$seed = seed if(&quot;nrestart&quot; %in% names(args)){ args = args[-which(names(args) %in% &quot;nrestart&quot;)] ## remove |nrestart| prior to feeding to flowtrend_once(). } tryCatch({ ## Estimate model argn &lt;- lapply(names(args), as.name) names(argn) &lt;- names(args) call &lt;- as.call(c(list(as.name(&quot;flowtrend_once&quot;)), argn)) res.train = eval(call, args) ## Assign mn and prob pred = predict_flowtrend(res.train, newtimes = x[test.inds]) ## the x is NEW. stopifnot(all(pred$prob &gt;= 0)) ## Evaluate on test data, by calculating objective (penalized likelihood with penalty parameters set to 0) cvscore = objective(mu = pred$mn, prob = pred$prob, sigma = pred$sigma, ylist = test.dat, countslist = test.count, unpenalized = TRUE) ## Store (temporarily) the run times time_per_iter = res.train$time_per_iter final_iter = res.train$final.iter total_time = res.train$total_time ## Store the results. mn = res.train$mn prob = res.train$prob objectives = res.train$objectives ## Save the CV results save(cvscore, ## Time time_per_iter, final_iter, total_time, ## Results lambda_mean, lambda_prob, lambda_means, lambda_probs, mn, prob, objectives, ## Save the file file = file.path(destin, filename)) return(NULL) }, error = function(err) { err$message = paste(err$message, &quot;\\n(No file will be saved for lambdas (&quot;, signif(lambda_probs[iprob],3), &quot;, &quot;, signif(lambda_means[imu],3), &quot;) whose indices are: &quot;, iprob, &quot;-&quot;, imu, &quot;-&quot;, ifold, &quot;-&quot;, irestart, &quot; .)&quot;,sep=&quot;&quot;) cat(err$message, fill=TRUE) warning(err)}) } Here is one_job_refit(). #&#39; Refit model for one pair of regularization parameter values. Saves to #&#39; \\code{nrestart} files named like &quot;1-4-3-fit.Rdata&quot;, for #&#39; &quot;(iprob)-(imu)-(irestart)-fit.Rdata&quot;. #&#39; #&#39; (Note, \\code{nrestart} is not an input to this function.) #&#39; #&#39; @inheritParams one_job #&#39; #&#39; @export one_job_refit &lt;- function(iprob, imu, destin, lambda_means, lambda_probs, l, l_prob, seedtab = NULL, ## The rest that is needed explicitly for flowtrend_once() ylist, countslist, x, ...){ args = list(...) nrestart = args$nrestart assertthat::assert_that(!is.null(nrestart)) for(irestart in 1:nrestart){ ## Writing file filename = make_refit_filename(iprob = iprob, imu = imu, irestart = irestart) best_filename = make_best_refit_filename(iprob, imu) ## if(file.exists(file.path(destin, filename)) ){ if(file.exists(file.path(destin, filename)) | file.exists(file.path(destin, best_filename))){ cat(filename, &quot;already done.&quot;, fill=TRUE) next } else { ## Get the seed ready if(!is.null(seedtab)){ ifold = 0 seed = seedtab %&gt;% dplyr::filter(iprob == !!iprob, imu == !!imu, ifold == !!ifold, irestart == !!irestart) %&gt;% dplyr::select(seed1, seed2, seed3, seed4, seed5, seed6, seed7) %&gt;% unlist() %&gt;% as.integer() } else { seed = NULL } ## Get the fitted results on the entire data args = list(...) args$ylist = ylist args$countslist = countslist args$x = x args$lambda_prob = lambda_probs[iprob] args$lambda = lambda_means[imu] args$l = l args$l_prob = l_prob args$seed = seed if(&quot;nrestart&quot; %in% names(args)){ ## remove |nrestart| prior to feeding it into flowtrend_once args = args[-which(names(args) %in% &quot;nrestart&quot;)] } ## Call the function. argn &lt;- lapply(names(args), as.name) names(argn) &lt;- names(args) call &lt;- as.call(c(list(as.name(&quot;flowtrend_once&quot;)), argn)) res = eval(call, args) ## Save the results cat(&quot;Saving file here:&quot;, file.path(destin, filename), fill=TRUE) save(res, file=file.path(destin, filename)) } } } Since cross-validation entails running many jobs, we need to index individual “jobs” carefully. Here are some more helpers for indexing: make_iimat(): Make a table whose rows index each “job” (iprob, imu, ifold, irestart), to be used by one_job(). make_iimat_small(): Make a table whose rows index each (iprob, imu, irestart) for re-estimating models, to be used by one_job_refit(). #&#39; Indices for the cross validation jobs. #&#39; #&#39; The resulting iimat looks like this: #&#39; #&#39; ind iprob imu ifold irestart #&#39; 55 6 1 2 1 #&#39; 56 7 1 2 1 #&#39; 57 1 2 2 1 #&#39; 58 2 2 2 1 #&#39; 59 3 2 2 1 #&#39; 60 4 2 2 1 #&#39; @param cv_gridsize CV grid size. #&#39; @param nfold Number of CV folds. #&#39; @param nrestart Number of random restarts of EM algorithm. #&#39; #&#39; @return Integer matrix. #&#39; #&#39; @export make_iimat &lt;- function(cv_gridsize, nfold, nrestart){ iimat = expand.grid(iprob = 1:cv_gridsize, imu = 1:cv_gridsize, ifold = 1:nfold, irestart = 1:nrestart) iimat = cbind(ind = as.numeric(rownames(iimat)), iimat) return(iimat) } #&#39; 2d indices for the cross validation jobs. #&#39; #&#39; The resulting iimat looks like this: #&#39; (#, iprob, imu, irestart) #&#39; 1, 1, 1, 1 #&#39; 2, 1, 2, 1 #&#39; 3, 1, 3, 1 #&#39; #&#39; @inheritParams make_iimat #&#39; #&#39; @return Integer matrix. #&#39; #&#39; @export make_iimat_small &lt;- function(cv_gridsize, nrestart){ iimat = expand.grid(iprob = 1:cv_gridsize, imu = 1:cv_gridsize, irestart = 1:nrestart) iimat = cbind(ind = as.numeric(rownames(iimat)), iimat) return(iimat) } Let’s see the integer matrices that these functions make. make_iimat(cv_gridsize = 5, nfold = 5, nrestart = 10) %&gt;% head() ## ind iprob imu ifold irestart ## 1 1 1 1 1 1 ## 2 2 2 1 1 1 ## 3 3 3 1 1 1 ## 4 4 4 1 1 1 ## 5 5 5 1 1 1 ## 6 6 1 2 1 1 make_iimat_small(cv_gridsize = 5, nrestart = 10) %&gt;% head() ## ind iprob imu irestart ## 1 1 1 1 1 ## 2 2 2 1 1 ## 3 3 3 1 1 ## 4 4 4 1 1 ## 5 5 5 1 1 ## 6 6 1 2 1 Let’s say you have ten cores to run jobs, perhaps on several different computers (“nodes”). In order to divide the jobs up into ten chunks, we’ll make function that splits a matrix iimat into a list of smaller matrices. #&#39; Helper to divide up the jobs in \\code{iimat} into a total of #&#39; \\code{arraynum_max} jobs. The purpose is to divide the jobs, in order to run #&#39; this on a server. #&#39; #&#39; @param arraynum_max Maximum SLURM array number. #&#39; @param iimat matrix whose rows contain CV job indices. #&#39; #&#39; @export make_iilist &lt;- function(arraynum_max, iimat){ iimax = nrow(iimat) if(arraynum_max &gt; iimax){ iilist = lapply(1:iimax, function(a)a) } else { ends = round(seq(from=0, to=iimax, length=arraynum_max+1)) iilist = Map(function(a,b){ (a+1):b}, ends[-length(ends)], ends[-1]) stopifnot(length(unlist(iilist)) == nrow(iimat)) } stopifnot(length(unlist(iilist)) == nrow(iimat)) stopifnot(all(sort(unique(unlist(iilist))) == sort(unlist(iilist)))) return(iilist) } Next, the functions make_cvscore_filename() and make_refit_filename() are used to form the names of the numerous output files. #&#39; Create file name (a string) for cross-validation results. #&#39; @param iprob #&#39; @param imu #&#39; @param ifold #&#39; @param irestart #&#39; #&#39; @export make_cvscore_filename &lt;- function(iprob, imu, ifold, irestart){ filename = paste0(iprob, &quot;-&quot;, imu, &quot;-&quot;, ifold, &quot;-&quot;, irestart, &quot;-cvscore.Rdata&quot;) return(filename) } #&#39; Create file name (a string) for cross-validation results. #&#39; @param iprob #&#39; @param imu #&#39; @param ifold #&#39; @param irestart #&#39; #&#39; @export make_best_cvscore_filename &lt;- function(iprob, imu, ifold){ filename = paste0(iprob, &quot;-&quot;, imu, &quot;-&quot;, ifold, &quot;-best-cvscore.Rdata&quot;) return(filename) } #&#39; Create file name (a string) for re-estimated models for the lambda values #&#39; indexed by \\code{iprob} and \\code{imu}. #&#39; @param iprob #&#39; @param imu #&#39; @param irestart #&#39; #&#39; @export make_refit_filename &lt;- function(iprob, imu, irestart){ filename = paste0(iprob, &quot;-&quot;, imu, &quot;-&quot;, irestart, &quot;-fit.Rdata&quot;) return(filename) } #&#39; Create file name (a string) for re-estimated models for the lambda values #&#39; indexed by \\code{iprob} and \\code{imu}. #&#39; @param iprob #&#39; @param imu #&#39; #&#39; @export make_best_refit_filename &lt;- function(iprob, imu){ filename = paste0(iprob, &quot;-&quot;, imu, &quot;-best-fit.Rdata&quot;) return(filename) } Here’s a useful helper logspace(max, min) to make logarithmically spaced set of numbers, given min and max. We can use this to make a grid of lambda pairs to be used for cross-validation. #&#39; Helper function to logarithmically space out R. \\code{length} values linear #&#39; on the log scale from \\code{max} down to \\code{min}. #&#39; #&#39; @param max Maximum value. #&#39; @param min Minimum value. #&#39; @param length Length of the output string. #&#39; @param min.ratio Factor to multiply to \\code{max}. #&#39; #&#39; @return Log spaced #&#39; #&#39; @export logspace &lt;- function(max, min=NULL, length, min.ratio = 1E-4){ if(is.null(min)) min = max * min.ratio vec = 10^seq(log10(min), log10(max), length = length) stopifnot(abs(vec[length(vec)] - max) &lt; 1E10) return(vec) } 7.5 Running cross-validation Putting the helpers all together, you get the main user-facing function cv_flowtrend(). #&#39; Cross-validation for flowtrend(). Saves results to separate files in #&#39; \\code{destin}. #&#39; #&#39; @param destin Directory where output files are saved. #&#39; @param nfold Number of cross-validation folds. Defaults to 5. #&#39; @param nrestart Number of repetitions. #&#39; @param save_meta If TRUE, save meta data. #&#39; @param lambda_means Regularization parameters for means. #&#39; @param lambda_probs Regularization parameters for probs. #&#39; @param folds Manually provide CV folds (list of time points of data to use #&#39; as CV folds). Defaults to NULL. #&#39; @param mc.cores Use this many CPU cores. #&#39; @param blocksize Contiguous time blocks from which to form CV time folds. #&#39; @param refit If TRUE, estimate the model on the full data, for each pair of #&#39; regularization parameters. #&#39; @param ... Additional arguments to flowtrend(). #&#39; @inheritParams flowtrend_once #&#39; #&#39; @return No return. #&#39; #&#39; @export cv_flowtrend &lt;- function(## Data ylist, countslist, x = NULL, ## THIS IS NEW ## Define the locations to save the CV. destin = &quot;.&quot;, ## Regularization parameter values lambda_means, lambda_probs, l, l_prob, iimat = NULL, ## Other settings maxdev, numclust, nfold, blocksize, nrestart, verbose = FALSE, refit = FALSE, save_meta = FALSE, mc.cores = 1, folds = NULL, seedtab = NULL, niter = 1000, ...){ ## Basic checks stopifnot(length(lambda_probs) == length(lambda_means)) cv_gridsize = length(lambda_means) ## There&#39;s an option to input one&#39;s own iimat matrix. if(is.null(iimat)){ ## Make an index of all jobs if(!refit) iimat = make_iimat(cv_gridsize, nfold, nrestart) if(refit) iimat = make_iimat_small(cv_gridsize, nrestart) } ## Define the CV folds ## folds = make_cv_folds(ylist = ylist, nfold = nfold, blocksize = 1) if(is.null(folds)){ folds = make_cv_folds(ylist = ylist, nfold = nfold, blocksize = blocksize) } else { stopifnot(length(folds) == nfold) } ## Save meta information, once. if(save_meta){ ##if(!refit){ if(file.exists(file = file.path(destin, &#39;meta.Rdata&#39;))){ ## Put aside the current guys cat(fill = TRUE) cat(&quot;Meta data already exists!&quot;) folds_current = folds nfold_current = nfold nrestart_current = nrestart cv_gridsize_current = cv_gridsize lambda_means_current = lambda_means lambda_probs_current = lambda_probs ylist_current = ylist x_current = x countslist_current = countslist ## Load the saved metadata and check if they are all the same as the current guys load(file = file.path(destin, &#39;meta.Rdata&#39;), verbose = FALSE) stopifnot(identical(folds, folds_current)) stopifnot(nfold == nfold_current) stopifnot(nrestart == nrestart_current) ## Added recently stopifnot(cv_gridsize == cv_gridsize_current) stopifnot(all(lambda_means == lambda_means_current)) stopifnot(all(lambda_probs == lambda_probs_current)) stopifnot(identical(ylist, ylist_current)) stopifnot(identical(x, x_current)) stopifnot(identical(countslist, countslist_current)) cat(fill=TRUE) cat(&quot;Successfully checked that the saved metadata is identical to the current one.&quot;, fill = TRUE) } else { save(folds, nfold, nrestart, ## Added recently cv_gridsize, lambda_means, lambda_probs, ylist, countslist, x, ## Save the file file = file.path(destin, &#39;meta.Rdata&#39;)) print(paste0(&quot;wrote meta data to &quot;, file.path(destin, &#39;meta.Rdata&#39;))) } ## } } ## Run the EM algorithm many times, for each value of (iprob, imu, ifold, irestart) start.time = Sys.time() parallel::mclapply(1:nrow(iimat), function(ii){ print_progress(ii, nrow(iimat), &quot;Jobs (EM replicates) assigned on this computer&quot;, start.time = start.time) if(!refit){ iprob = iimat[ii,&quot;iprob&quot;] imu = iimat[ii,&quot;imu&quot;] ifold = iimat[ii,&quot;ifold&quot;] irestart = iimat[ii,&quot;irestart&quot;] ## if(verbose) cat(&#39;(iprob, imu, ifold, irestart)=&#39;, c(iprob, imu, ifold, irestart), fill=TRUE) } else { iprob = iimat[ii, &quot;iprob&quot;] imu = iimat[ii, &quot;imu&quot;] ifold = 0 } if(!refit){ one_job(iprob = iprob, imu = imu, l = l, l_prob = l_prob, ifold = ifold, irestart = irestart, folds = folds, destin = destin, lambda_means = lambda_means, lambda_probs = lambda_probs, ## Arguments for flowtrend() ylist = ylist, countslist = countslist, x = x, ## Additional arguments for flowtrend(). numclust = numclust, maxdev = maxdev, verbose = FALSE, seedtab = seedtab, niter = niter) } else { one_job_refit(iprob = iprob, imu = imu, l = l, l_prob = l_prob, destin = destin, lambda_means = lambda_means, lambda_probs = lambda_probs, ## Arguments to flowtrend() ylist = ylist, countslist = countslist, x = x, ## Additional arguments for flowtrend(). numclust = numclust, maxdev = maxdev, nrestart = nrestart, verbose = FALSE, seedtab = seedtab, niter = niter) } return(NULL) }, mc.cores = mc.cores) } 7.6 Summarizing the output Once the cross-validation is finished (and saved into many files called e.g. 1-1-1-1-cvscore.Rdata or 1-1-1-fit.Rdata), we can use cv_summary() to summarize the results. If you look closely, you’ll notice that cv_aggregate() is the workhorse. #&#39; Main function for summarizing the cross-validation results. #&#39; #&#39; @inheritParams cv_flowtrend #&#39; @param save If TRUE, save to \\code{file.path(destin, filename)}. #&#39; @param filename File name to save to. #&#39; #&#39; @return List containing summarized results from cross-validation. Here are #&#39; some objects in this list: \\code{bestres} is the the overall best model #&#39; chosen from the cross-validation; \\code{cvscoremat} is a 2d matrix of CV #&#39; scores from all pairs of regularization parameters; \\code{bestreslist} is a #&#39; list of all the best models (out of \\code{nrestart} EM replications) from the #&#39; each pair of lambda values. If \\code{isTRUE(save)}, nothing is returned. #&#39; #&#39; @export cv_summary &lt;- function(destin = &quot;.&quot;, save = FALSE, filename = &quot;summary.RDS&quot;){ load(file.path(destin,&#39;meta.Rdata&#39;)) ## This loads all the necessary things: nrestart, nfold, cv_gridsize stopifnot(exists(&quot;nrestart&quot;)) stopifnot(exists(&quot;nfold&quot;)) stopifnot(exists(&quot;cv_gridsize&quot;)) ## Get the results of the cross-validation. a = cv_aggregate(destin) cvscore.mat = a$cvscore.mat cvscore.mat.se = a$cvscore.mat.se min.inds = a$min.inds min.inds.1se = a$min.inds.1se ## Get results from refitting bestreslist = cv_aggregate_res(destin = destin) bestres = bestreslist[[paste0(min.inds[1] , &quot;-&quot;, min.inds[2])]] if(is.null(bestres)){ if(min.inds[1]==2 &amp; min.inds[2]==3) browser() stop(paste0(&quot;The model with lambda indices (&quot;, min.inds[1], &quot;,&quot;, min.inds[2], &quot;) is not available.&quot;)) } out = list(bestres = bestres, cvscore.mat = cvscore.mat, cvscore.mat.se = cvscore.mat.se, min.inds = min.inds, min.inds.1se = min.inds.1se, lambda_means = lambda_means, lambda_probs = lambda_probs, ## List of all best models for all lambda pairs. bestreslist = bestreslist, destin = destin) if(save){ saveRDS(out, file=file.path(destin, filename)) } return(out) } Because the metadata (meta.Rdata) file is a list object, and you want to prevent loading it to your function environment (lest it overwrite existing variables), we write a short helper function to load it. ##&#39; Load contents from the Rdata file |filename| and return a list. ##&#39; This is specifically for when the Rddata file you load contains a /named/ R ##&#39; list. ##&#39; ##&#39; This is basically trying to treat the Rdata file more like an RDS file that contains a list. ##&#39; ##&#39; @param filename Name of the Rdata file ##&#39; ##&#39; @return List object with contents of the Rdata file. ##&#39; @export load_Rdata &lt;- function(filename){ ## loads an RData file, and returns it load(filename, new_env &lt;- new.env()) obj = lapply(ls(envir = new_env), get) names(obj) = ls(new_env) return(obj) } #&#39; From the results saved in \\code{destin}, aggregate all |nrestart| files to retain only the &quot;best&quot; restart, and delete the rest. #&#39; #&#39; All meta information (|nfold|, |cv_gridsize|, |nrestart|, |lambda_means|, |lambda_probs|) comes from \\code{meta.Rdata}. #&#39; #&#39; @param destin Directory with cross-validation output. #&#39; #&#39; @export cv_makebest &lt;- function(destin){ ## Read the meta data (for |nfold|, |cv_gridsize|, |nrestart|, |lambda_means|, |lambda_probs|) load(file = file.path(destin, &#39;meta.Rdata&#39;), verbose = FALSE) ## This loads all the necessary things; just double-checking. stopifnot(exists(&quot;nrestart&quot;)) stopifnot(exists(&quot;nfold&quot;)) stopifnot(exists(&quot;cv_gridsize&quot;)) stopifnot(exists(c(&quot;lambda_probs&quot;))) stopifnot(exists(c(&quot;lambda_means&quot;))) ## Aggregate the results cvscore.array = array(NA, dim = c(cv_gridsize, cv_gridsize, nfold, nrestart)) cvscore.mat = matrix(NA, nrow = cv_gridsize, ncol = cv_gridsize) for(iprob in 1:cv_gridsize){ for(imu in 1:cv_gridsize){ for(ifold in 1:nfold){ print(c(iprob, imu, ifold)) ## If the &quot;best&quot; flowtrend object has already been created, do nothing. best_filename = make_best_cvscore_filename(iprob, imu, ifold) if(file.exists(file.path(destin, best_filename))){ next ## Otherwise, attempt to load from all |nrestart| replicates } else { objectives = load_all_objectives(destin, iprob, imu, ifold, nrestart) ## If all |nrestart| files exist, delete all files but the best model. if(all(!is.na(objectives))){ best_irestart = which(objectives == min(objectives)) %&gt;% .[1] ## If there is a tie, leave it. keep_only_best(destin, iprob, imu, ifold, nrestart, best_irestart) } else { print(paste0(&quot;iprob=&quot;, iprob, &quot; imu=&quot;, imu, &quot; ifold=&quot;, ifold, &quot; had objectives: &quot;, objectives)) } } } } } ## Also go over the &quot;refit&quot; files for(iprob in 1:cv_gridsize){ for(imu in 1:cv_gridsize){ ## If the &quot;best&quot; flowtrend object has already been created, do nothing. best_filename = make_best_refit_filename(iprob, imu) if(file.exists(file.path(destin, best_filename))){ ## ## Check if any more jobs have been done since before ## objectives = load_all_refit_objectives(destin, iprob, imu, nrestart) ## if(any(!is.na(objectives))){ ## nonmissing_irestart = which(!is.na(objectives))## == min(objectives)) ## ## keep_only_best_refit(destin, iprob, imu, nrestart, best_irestart) ## load(file.path(destin, best_filename)) ## } else { ## print(paste0(&quot;iprob=&quot;, iprob, &quot; imu=&quot;, imu, &quot; had /refit/ objectives: &quot;, objectives)) ## } ## } next ## Otherwise, attempt to load from all |nrestart| replicates } else { objectives = load_all_refit_objectives(destin, iprob, imu, nrestart) if(all(!is.na(objectives))){ best_irestart = which(objectives == min(objectives)) %&gt;% .[1] ## If there is a tie, leave it. keep_only_best_refit(destin, iprob, imu, nrestart, best_irestart) } else { print(paste0(&quot;iprob=&quot;, iprob, &quot; imu=&quot;, imu, &quot; had /refit/ objectives: &quot;, objectives)) } } } } } #&#39; Loading all objectives, with NA&#39;s for missing files load_all_objectives &lt;- function(destin, iprob, imu, ifold, nrestart){ objectives = sapply(1:nrestart, function(irestart){ filename = make_cvscore_filename(iprob, imu, ifold, irestart) tryCatch({ load(file.path(destin, filename), verbose = FALSE) return(objectives[length(objectives)]) }, error = function(e){ NA }) }) return(objectives) } #&#39; Loading all objectives, with NA&#39;s for missing files load_all_refit_objectives &lt;- function(destin, iprob, imu, nrestart){ objectives = sapply(1:nrestart, function(irestart){ filename = make_refit_filename(iprob, imu, irestart) ## filename = make_best_cvscore_filename(iprob, imu, ifold) tryCatch({ load(file.path(destin, filename), verbose = FALSE) return(res$objectives[length(res$objectives)]) }, error = function(e){ NA }) }) return(objectives) } #&#39; Keeping only the output files for the &quot;best&quot; restart, and deleting the rest. keep_only_best &lt;- function(destin, iprob, imu, ifold, nrestart, best_irestart){ for(irestart in 1:nrestart){ filename = make_cvscore_filename(iprob, imu, ifold, irestart) if(irestart == best_irestart){ best_filename = make_best_cvscore_filename(iprob, imu, ifold) file.rename(from = file.path(destin, filename), to = file.path(destin, best_filename)) } else { file.remove(file.path(destin, filename)) } } } #&#39; Keeping only the output files (among refit models) for the &quot;best&quot; restart, #&#39; and deleting the rest. keep_only_best_refit &lt;- function(destin, iprob, imu, nrestart, best_irestart){ for(irestart in 1:nrestart){ filename = make_refit_filename(iprob, imu, irestart) if(irestart == best_irestart){ best_filename = make_best_refit_filename(iprob, imu) file.rename(from = file.path(destin, filename), to = file.path(destin, best_filename)) } else { file.remove(file.path(destin, filename)) } } } #&#39; Aggregate CV scores from the results, saved in \\code{destin}. #&#39; #&#39; @param destin Directory with cross-validation output. #&#39; #&#39; @export cv_aggregate &lt;- function(destin){ ## ## Read the meta data (for |nfold|, |cv_gridsize|, |nrestart|, |lambda_means|, ## ## |lambda_probs|) load(file = file.path(destin, &#39;meta.Rdata&#39;), verbose = FALSE) ## This loads all the necessary things; just double-checking. stopifnot(exists(&quot;nrestart&quot;)) stopifnot(exists(&quot;nfold&quot;)) stopifnot(exists(&quot;cv_gridsize&quot;)) stopifnot(exists(c(&quot;lambda_probs&quot;))) stopifnot(exists(c(&quot;lambda_means&quot;))) ## Aggregate the results cvscore.array = array(NA, dim = c(cv_gridsize, cv_gridsize, nfold, nrestart)) cvscore.mat = matrix(NA, nrow = cv_gridsize, ncol = cv_gridsize) cvscore.mat.se = matrix(NA, nrow = cv_gridsize, ncol = cv_gridsize) for(iprob in 1:cv_gridsize){ for(imu in 1:cv_gridsize){ obj = matrix(NA, nrow=nfold, ncol=nrestart) for(ifold in 1:nfold){ ## If the &quot;best&quot; flowtrend object has already been created, use it. best_filename = make_best_cvscore_filename(iprob, imu, ifold) if(file.exists(file.path(destin, best_filename))){ load(file.path(destin, best_filename), verbose = FALSE) cvscore.array[iprob, imu, ifold, 1] = cvscore obj[ifold, 1] = objectives[length(objectives)] ## Otherwise, aggregate directly from the individual files } else { for(irestart in 1:nrestart){ filename = make_cvscore_filename(iprob, imu, ifold, irestart) tryCatch({ load(file.path(destin, filename), verbose = FALSE) cvscore.array[iprob, imu, ifold, irestart] = cvscore obj[ifold, irestart] = objectives[length(objectives)] }, error = function(e){}) } } } ## Pick out the CV scores with the *best* (lowest) objective value cvscores = cvscore.array[iprob, imu , , ] ## nfold x nrestart best.models = apply(obj, 1, function(myrow){ ind = which(myrow == min(myrow, na.rm=TRUE)) if(length(ind)&gt;1) ind = ind[1] ## Just choose one, if there is a tie. return(ind) }) %&gt;% as.numeric() ## best.models is the *best* model out of nrestarts, for each fold. ## final.cvscores[ifold,] has the |nrestart| final cv scores for that fold. ## cvscores[ifold, best.models[ifold]] is the best CV score for that fold. final.cvscores = sapply(1:nfold, function(ifold){ cvscores[ifold, best.models[ifold]] ## Why did we get rid of this? ## cvscores[ifold] }) cvscore.mat[iprob, imu] = mean(final.cvscores) cvscore.mat.se[iprob, imu] = sd(final.cvscores) } } ## Clean a bit cvscore.mat[which(is.nan(cvscore.mat), arr.ind = TRUE)] = NA ## ## Read the meta data (for |nfold|, |cv_gridsize|, |nrestart|) rownames(cvscore.mat) = signif(lambda_probs,3) colnames(cvscore.mat) = signif(lambda_means,3) ## Find the minimum mat = cvscore.mat min.inds = which(mat == min(mat, na.rm = TRUE), arr.ind = TRUE) ## Find the 1SE minimum index too mat = cvscore.mat ## sdmat = (out$cvscore.mat) ## This is fake ## sdmat[] = sd(out$cvscore.mat) ## This is fake sdmat = cvscore.mat.se ## This is fake upper = mat[min.inds] + sdmat[min.inds] possible_inds = which(mat &lt; upper, arr.ind=TRUE) ## ## Because there is no good heuristic, we&#39;ll just go up in both directions. ## my_ord = order(abs(possible_inds[,2] - possible_inds[,1])) ## possible_inds[my_ord, ] %&gt;% apply(2, which.max) ## Or even just stick with the ones on the diagonal direction. myrows = which(possible_inds[,2] == possible_inds[,1]) possible_inds = possible_inds[myrows, ,drop=FALSE] min.inds.1se = possible_inds[which.max(possible_inds[,1]),] ## Return the results out = list(cvscore.array = cvscore.array, cvscore.mat = cvscore.mat, cvscore.mat.se = cvscore.mat.se, lambda_means = lambda_means, lambda_probs = lambda_probs, min.inds = min.inds, min.inds.1se = min.inds.1se) return(out) } #&#39; Helper to aggregate CV results and obtain the |res| object, all saved in #&#39; |destin|. #&#39; #&#39; @inheritParams cv_aggregate #&#39; #&#39; @return List containing, for every (iprob, imu), the &quot;best&quot; estimated model #&#39; out of the |nrestart| replicates (best in the sense that it had the best #&#39; likelihood value out of the |nrestart| replicates.) cv_aggregate_res &lt;- function(destin){ load(file.path(destin, &quot;meta.Rdata&quot;)) res.list = list() for(iprob in 1:cv_gridsize){ for(imu in 1:cv_gridsize){ ## If the &quot;best&quot; object has already been created, use it. best_refit_filename = make_best_refit_filename(iprob, imu) if(file.exists(file.path(destin, best_refit_filename))){ load(file.path(destin, best_refit_filename), verbose = FALSE) bestres = res ## Otherwise, aggregate directly from the individual files } else { obj = rep(NA, nrestart) res.list.inner = list() for(irestart in 1:nrestart){ filename = make_refit_filename(iprob, imu, irestart) tryCatch({ load(file.path(destin, filename)) res.list.inner[[irestart]] = res obj[irestart] = res$objectives[length(res$objectives)] }, error = function(e){ NULL }) } if(!all(is.na(obj))){ bestres = res.list.inner[[which.min(obj)]] ## which.min? } } ## Add the &quot;best&quot; object to a list. res.list[[paste0(iprob, &quot;-&quot;, imu)]] = bestres } } return(res.list) } 7.7 CV on your own computer Using all this functionality, we’d like to be able to cross-validate on our own laptop, using cv_flowtrend(). Let’s try it out. (Note: We’re not running this code while building the package because it requires saving some output to an offline directory.) cv_gridsize = 3 l = 1 l_prob = 1 set.seed(332) ylist = gendat_1d(10, rep(100,10)) %&gt;% dt2ylist() plot_1d(ylist) folds = make_cv_folds(lapply(1:100, function(ii) cbind(runif(10))), nfold = 3, TT = length(ylist)) ## What is this doing? FISHY lambda_means = lambda_probs = logspace(min = 1E-5, max = 1, length = cv_gridsize) destin = &quot;~/repos/flowtrend/inst/test-output/cv-offline&quot; dir.create(destin, recursive=TRUE) A realistic application would start with a call to get_max_lambda(): ## This is where all the output is destin = &quot;~/repos/flowtrend/inst/test-output/cv-offline&quot; ## Get the maximum lambda values get_max_lambda(destin = destin, maxres_file = &quot;maxres.Rdata&quot;, ylist, countslist = NULL, numclust = 3, maxdev = 1, max_lambda_prob = 3000000, max_lambda_mean = 3000000, verbose = TRUE, l = 1, l_prob = 1) Next, with this maxres object, run the cross-validation. ## Loads &quot;maxres&quot; object load(file.path(destin, &quot;maxres.Rdata&quot;), verbose = TRUE) ## Run the cross-validation lambda_means = logspace(max = maxres$mean, length = 5) lambda_probs = logspace(max = maxres$prob, length = 5) for(refit in c(FALSE, TRUE)){ cv_flowtrend(ylist = ylist, countslist = NULL, destin = destin, lambda_means = lambda_means, lambda_probs = lambda_probs, l = 2, l_prob = 1, maxdev = 2, numclust = 3, nfold = 3, folds = folds, nrestart = 3, verbose = TRUE, refit = refit, save_meta = TRUE, blocksize = 1, mc.cores = 6) } cvres = cv_summary(destin = destin, save=TRUE) ## Plot the result oneplot = plot_1d(ylist = ylist) %&gt;% plot_1d_add_model(obj = cvres$bestres, idim=1, plot_band = TRUE) + geom_line(aes(x=time, y=mean, group = cluster), data = gendat_1d(10, ntlist = rep(100, 10), return_model = TRUE), linetype = &#39;dashed&#39;) oneplot %&gt;% ggsave(filename = file.path(destin, &quot;..&quot;, &quot;..&quot;, &quot;cv-offline-test.png&quot;), width = 5, height = 3) Cross-validated model estimates (on evenly spaced data). (The dashed black lines are the truth, and the solid colored lines are the estimated means.) 7.8 CV produces too many files? No problem Once all jobs are run, you can use cv_makebest() which goes over all the output, and (1) retains the best model out of the 3 replicates (random restarts), and (2) deletes the output from the remaining 2 of the 3 replicates. cv_summary() still works fine with the reduced number of files. ## This is where all the output is destin = &quot;~/repos/flowtrend/inst/test-output/cv-offline&quot; ## There are a lot of individual files here. list.files(destin) %&gt;% length() %&gt;% print() ## Clean the output directory to keep only the &quot;best&quot; results cv_makebest(destin = destin) ## There are much fewer individual files here. list.files(destin) %&gt;% length() %&gt;% print() 7.9 CV for unevenly spaced data Next, we test whether cv_flowtrend works for unevenly spaced points. (Note: We’re not running this code while building the package because it requires saving some output to an offline directory.) ## Setup l = 1 l_prob = 1 destin = &quot;inst/test-output/uneven-test&quot; dir.create(destin, recursive = TRUE) ## Generate data, and subset it at unevenly spaced x values. set.seed(332) TT = 50 nt = 100 ylist = gendat_1d(TT, ntlist = rep(nt, TT)) %&gt;% dt2ylist() x = sample(1:TT, size=TT/2, replace = FALSE) %&gt;% sort() ylist = ylist[x] ## Save the data datobj = list(ylist = ylist, x = x, countslist = NULL) saveRDS(datobj, file = file.path(destin, &quot;datobj.RDS&quot;)) nfold = 3 folds = make_cv_folds(ylist, nfold = nfold, TT = length(ylist)) lambda_means = lambda_probs = logspace(min = 1E-5, max = 1, length = 5) for(refit in c(FALSE, TRUE)){ cv_flowtrend(ylist = ylist, countslist = NULL, x = x, destin = destin, lambda_means = lambda_means, lambda_probs = lambda_probs, l = l, l_prob = l_prob, maxdev = 1, numclust = 3, nfold = nfold, nrestart = 3, niter = 100, folds = folds, verbose = TRUE, refit = refit, save_meta = TRUE, mc.cores = 4) } cvres = cv_summary(destin = destin, save=TRUE) ## Plot it! oneplot = plot_1d(ylist = ylist, x=x) %&gt;% plot_1d_add_model(obj = cvres$bestres, idim=1, plot_band = TRUE) + geom_line(aes(x=time, y=mean, group = cluster), data = gendat_1d(TT, ntlist = rep(nt, TT), return_model = TRUE), linetype = &#39;dashed&#39;) oneplot %&gt;% ggsave(filename = file.path(destin, &quot;..&quot;, &quot;..&quot;, &quot;uneven-spaced-data-test.png&quot;), width = 5, height = 3) Cross-validated model estimates on unevenly spaced data. (The dashed black lines are the truth, and the solid colored lines are the estimated means.) 7.10 Documenting the package and building We finish by running commands that will document, build, and install the package. It may also be a good idea to check the package from within this file. litr::document() # &lt;-- use instead of devtools::document() ## ℹ Updating flowtrend documentation ## Writing &#39;NAMESPACE&#39; ## ℹ Loading flowtrend ## ℹ Re-compiling flowtrend (debug build) ## ## ✖ Estep.R:5: @param requires two parts: an argument name and a description. ## ✖ Estep.R:7: @param requires two parts: an argument name and a description. ## ✖ Estep.R:8: @param requires two parts: an argument name and a description. ## ✖ Estep.R:9: @param requires two parts: an argument name and a description. ## ✖ Estep.R:10: @param requires two parts: an argument name and a description. ## ✖ Estep.R:11: @param requires two parts: an argument name and a description. ## ✖ Estep.R:12: @param requires two parts: an argument name and a description. ## ✖ Estep.R:17: @return requires a value. ## ✖ Mstep_mu.R:7: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:8: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:9: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:10: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:11: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:12: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:13: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:14: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:15: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:16: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:17: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:18: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:19: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:20: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:21: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:22: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:23: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:24: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:25: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:26: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:27: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:28: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:29: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:30: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:33: @return requires a value. ## ✖ Mstep_mu.R:36: @examples requires a value. ## ✖ Mstep_mu_cvxr.R:5: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu_cvxr.R:6: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu_cvxr.R:7: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu_cvxr.R:8: @param requires two parts: an argument name and a description. ## ✖ Mstep_sigma.R:13: @examples requires a value. ## ✖ etilde_mat.R:5: @param requires two parts: an argument name and a description. ## ✖ flowtrend.R:8: @return requires a value. ## ✖ flowtrend.R:11: @examples requires a value. ## ✖ flowtrend_once.R:22: @param requires two parts: an argument name and a description. ## ✖ flowtrend_once.R:23: @param requires two parts: an argument name and a description. ## ✖ flowtrend_once.R:24: @param requires two parts: an argument name and a description. ## ✖ flowtrend_once.R:25: @param requires two parts: an argument name and a description. ## ✖ flowtrend_once.R:31: @examples requires a value. ## ✖ gen_diff_mat.R:12: @examples requires a value. ## ✖ get_AB_mats.R:5: @param requires two parts: an argument name and a description. ## ✖ get_C_mat.R:4: @param requires two parts: an argument name and a description. ## ✖ la_admm_oneclust.R:5: @param requires two parts: an argument name and a description. ## ✖ la_admm_oneclust.R:8: @return requires a value. ## ✖ la_admm_oneclust.R:10: @examples requires a value. ## ✖ make_cvscore_filename.R:4: @param requires two parts: an argument name and a description. ## ✖ make_cvscore_filename.R:5: @param requires two parts: an argument name and a description. ## ✖ make_cvscore_filename.R:6: @param requires two parts: an argument name and a description. ## ✖ make_cvscore_filename.R:16: @param requires two parts: an argument name and a description. ## ✖ make_cvscore_filename.R:17: @param requires two parts: an argument name and a description. ## ✖ make_cvscore_filename.R:18: @param requires two parts: an argument name and a description. ## ✖ make_cvscore_filename.R:30: @param requires two parts: an argument name and a description. ## ✖ make_cvscore_filename.R:31: @param requires two parts: an argument name and a description. ## ✖ make_cvscore_filename.R:42: @param requires two parts: an argument name and a description. ## ✖ objective.R:5: @param requires two parts: an argument name and a description. ## ✖ objective.R:6: @param requires two parts: an argument name and a description. ## ✖ objective.R:7: @param requires two parts: an argument name and a description. ## ✖ objective.R:8: @param requires two parts: an argument name and a description. ## ✖ objective.R:9: @param requires two parts: an argument name and a description. ## ✖ objective.R:10: @param requires two parts: an argument name and a description. ## ✖ objective.R:11: @param requires two parts: an argument name and a description. ## ✖ objective.R:12: @param requires two parts: an argument name and a description. ## ✖ objective.R:13: @param requires two parts: an argument name and a description. ## ✖ objective.R:14: @param requires two parts: an argument name and a description. ## ✖ objective.R:15: @param requires two parts: an argument name and a description. ## ✖ objective.R:16: @param requires two parts: an argument name and a description. ## ✖ objective.R:17: @param requires two parts: an argument name and a description. ## ✖ objective.R:18: @param requires two parts: an argument name and a description. ## ✖ objective.R:21: @return requires a value. ## ✖ objective.R:24: @examples requires a value. ## ✖ plot_3d.R:15: @return requires a value. ## ✖ In topic &#39;U_update_W.Rd&#39;: Skipping; no name and/or title. ## ✖ In topic &#39;U_update_Z.Rd&#39;: Skipping; no name and/or title. ## ✖ In topic &#39;myschur.Rd&#39;: Skipping; no name and/or title. ## Writing &#39;NAMESPACE&#39; ## Writing &#39;Estep.Rd&#39; ## Writing &#39;Mstep_mu.Rd&#39; ## Writing &#39;Mstep_mu_cvxr.Rd&#39; ## Writing &#39;Mstep_prob.Rd&#39; ## Writing &#39;Mstep_sigma.Rd&#39; ## Writing &#39;matrix_function_solve_triangular_sylvester_barebonesC2.Rd&#39; ## Writing &#39;W_update_fused.Rd&#39; ## Writing &#39;projCmat.Rd&#39; ## Writing &#39;admm_oneclust.Rd&#39; ## Writing &#39;aug_lagr.Rd&#39; ## Writing &#39;calc_max_lambda.Rd&#39; ## Writing &#39;check_converge.Rd&#39; ## Writing &#39;check_converge_rel.Rd&#39; ## Writing &#39;cv_aggregate.Rd&#39; ## Writing &#39;cv_aggregate_res.Rd&#39; ## Writing &#39;cv_flowtrend.Rd&#39; ## Writing &#39;cv_makebest.Rd&#39; ## Writing &#39;load_all_objectives.Rd&#39; ## Writing &#39;load_all_refit_objectives.Rd&#39; ## Writing &#39;keep_only_best.Rd&#39; ## Writing &#39;keep_only_best_refit.Rd&#39; ## Writing &#39;cv_summary.Rd&#39; ## Writing &#39;dt2ylist.Rd&#39; ## Writing &#39;etilde_mat.Rd&#39; ## Writing &#39;flowtrend-package.Rd&#39; ## Writing &#39;flowtrend.Rd&#39; ## Writing &#39;flowtrend_once.Rd&#39; ## Writing &#39;form_symmetric_kl_distmat.Rd&#39; ## Writing &#39;gen_diff_mat.Rd&#39; ## Writing &#39;gen_tf_mat.Rd&#39; ## Writing &#39;gen_tf_mat_equalspace.Rd&#39; ## Writing &#39;gendat_1d.Rd&#39; ## Writing &#39;gendat_2d.Rd&#39; ## Writing &#39;gendat_3d.Rd&#39; ## Writing &#39;get_AB_mats.Rd&#39; ## Writing &#39;get_C_mat.Rd&#39; ## Writing &#39;get_best_match_from_kl.Rd&#39; ## Writing &#39;get_max_lambda.Rd&#39; ## Writing &#39;init_mn.Rd&#39; ## Writing &#39;init_sigma.Rd&#39; ## Writing &#39;interpolate_mn.Rd&#39; ## Writing &#39;interpolate_prob.Rd&#39; ## Writing &#39;la_admm_oneclust.Rd&#39; ## Writing &#39;loglik_tt.Rd&#39; ## Writing &#39;logspace.Rd&#39; ## Writing &#39;make_cv_folds.Rd&#39; ## Writing &#39;make_cv_folds_in_blocks.Rd&#39; ## Writing &#39;make_cvscore_filename.Rd&#39; ## Writing &#39;make_best_cvscore_filename.Rd&#39; ## Writing &#39;make_refit_filename.Rd&#39; ## Writing &#39;make_best_refit_filename.Rd&#39; ## Writing &#39;make_iilist.Rd&#39; ## Writing &#39;make_iimat.Rd&#39; ## Writing &#39;make_iimat_small.Rd&#39; ## Writing &#39;my_mfrow.Rd&#39; ## Writing &#39;objective.Rd&#39; ## Writing &#39;objective_per_cluster.Rd&#39; ## Writing &#39;one_job.Rd&#39; ## Writing &#39;one_job_refit.Rd&#39; ## Writing &#39;plot_1d.Rd&#39; ## Writing &#39;plot_1d_add_model.Rd&#39; ## Writing &#39;plot_1d_with_membership.Rd&#39; ## Writing &#39;plot_2d.Rd&#39; ## Writing &#39;plot_3d.Rd&#39; ## Writing &#39;plot_prob.Rd&#39; ## Writing &#39;predict_flowtrend.Rd&#39; ## Writing &#39;print_progress.Rd&#39; ## Writing &#39;reorder_clust.Rd&#39; ## Writing &#39;reorder_kl.Rd&#39; ## Writing &#39;softmax.Rd&#39; ## Writing &#39;symmetric_kl.Rd&#39; ## Writing &#39;pipe.Rd&#39; "],["testing-the-flowtrend-method.html", "8 Testing the flowtrend method 8.1 1d example 8.2 Testing monotonicity of objective values 8.3 2d example 8.4 Working with “binned” dataset 8.5 Unevenly spaced inputs (x)", " 8 Testing the flowtrend method 8.1 1d example First, we’ll generate data. set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), offset = 4) dt_model &lt;- gendat_1d(100, rep(100, 100), return_model = TRUE, offset=4) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() plot_1d(ylist) dt_model %&gt;% select(time, cluster, prob) %&gt;% ggplot() + geom_line(aes(x=time, y=prob, group=cluster, col = cluster)) Next, we fit a model (with hand-picked lambda values). set.seed(18) obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 1, numclust = 3, l = 2, l_prob = 1, lambda = .1, lambda_prob = .1, nrestart = 5) ## Also reorder the cluster labels of the truth, to match the fitted model. ord = obj$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing=TRUE) lookup &lt;- setNames(c(1:obj$numclust), ord) dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %&gt;% as.factor() ## Reorder the cluster lables of the fitted model. obj = reorder_clust(obj) The data and estimated model are shown here, along with the true means in dashed black lines. plot_1d(ylist = ylist, obj = obj, x = x) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model,## %&gt;% subset(time %ni% held_out), linetype = &quot;dashed&quot;, size=2, alpha = .7) Also, the estimated probabilities are shown here. plot_prob(obj=obj, x=x) + geom_line(aes(x = time, y = prob, group = cluster, color = cluster), data = dt_model, linetype = &quot;dashed&quot;) + facet_wrap(~cluster) 8.2 Testing monotonicity of objective values The objective value (that is, the penalized log likelihood) should be monotone decreasing across EM algorithm iterations. testthat::test_that(&quot;Objective value decreases over EM iterations.&quot;,{ glist = list() for(iseed in 1:5){ ## Generate synthetic data set.seed(iseed*100) dt &lt;- gendat_1d(100, rep(10, 100)) dt_model &lt;- gendat_1d(100, rep(10, 100), return_model = TRUE) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() ## Fit model obj &lt;- flowtrend_once(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = 0.05) ## Test objective monotonicity niter_end = length(obj$objective) testthat::expect_true(all(diff(obj$objective) &lt; 1E-3)) ## Make a plot g = ggplot(tibble(iter=1:niter_end, objective=obj$objectives)) + geom_point(aes(x=iter, y=objective)) + geom_line(aes(x=iter, y=objective)) + ggtitle(paste0(&quot;Seed=&quot;, iseed*100)) + xlab(&quot;EM iteration&quot;) glist[[iseed]] = g } title = cowplot::ggdraw() + cowplot::draw_label(&quot;Objective values over EM iterations&quot;, fontface=&#39;bold&#39;) main_plot = cowplot::plot_grid(plotlist = glist, ncol=5, nrow=1) cowplot::plot_grid(title, main_plot, ncol=1, rel_heights=c(0.1, 1)) %&gt;% print() }) ## Test passed 🎉 8.3 2d example Next, we try out flowtrend on a synthetic 2d data example. set.seed(100) TT = 100 dt &lt;- gendat_2d(TT, rep(100, times = TT)) x = 1:TT set.seed(10) obj &lt;- flowtrend(ylist = dt$ylist, x = x, maxdev = 3, numclust = 3, l = 2, l_prob = 2, lambda = 0.01, lambda_prob = .01, rho_init = 0.01, nrestart = 1) These are some snapshots at time \\(t=10\\) and \\(t=20\\). plot_2d(dt$ylist, obj = obj, tt = 10, bin = FALSE) + coord_fixed() + ylim(-6, 3) + xlim(-6, 3) plot_2d(dt$ylist, obj = obj, tt = 20, bin = FALSE) + coord_fixed() + ylim(-6, 3) + xlim(-6, 3) 8.4 Working with “binned” dataset Recall that “binning” means we will use binned frequency histogram estimates of the original particle-level dataset; this is useful when there are too many particles. ## library(flowmix) set.seed(10232) TT = 100 dt &lt;- gendat_2d(TT, rep(100, times = TT)) manual_grid = flowmix::make_grid(dt$ylist, gridsize = 20) ## Warning: replacing previous import &#39;RcppArmadillo::fastLmPure&#39; by &#39;RcppEigen::fastLmPure&#39; when loading &#39;flowmix&#39; ## Warning: replacing previous import &#39;RcppArmadillo::fastLm&#39; by &#39;RcppEigen::fastLm&#39; when loading &#39;flowmix&#39; binres = flowmix::bin_many_cytograms(dt$ylist, manual.grid = manual_grid) set.seed(100) obj = flowtrend(ylist = binres$ybin_list, countslist = binres$counts_list, maxdev = 2, numclust = 3, l = 2, l_prob = 2, lambda = .01, lambda_prob = .005, rho_init = .01, nrestart = 1) plot_2d(ylist = binres$ybin_list, countslist = binres$counts_list, obj = obj, tt = 10, bin = TRUE) + coord_fixed() ## Warning: Raster pixels are placed at uneven horizontal intervals and will be shifted ## ℹ Consider using `geom_tile()` instead. ## Warning: Raster pixels are placed at uneven horizontal intervals and will be shifted ## ℹ Consider using `geom_tile()` instead. plot_2d(ylist = binres$ybin_list, countslist = binres$counts_list, obj = obj, tt = 100, bin = TRUE) + coord_fixed() ## Warning: Raster pixels are placed at uneven horizontal intervals and will be shifted ## ℹ Consider using `geom_tile()` instead. Also plot the cluster probabilities: true_prob_long = dt$probs %&gt;% as_tibble() %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-time) plot_prob(obj) + geom_line(aes(x = time, y = value, group = name), data = true_prob_long, linetype = &quot;dashed&quot;) + ylim(c(0,1)) + ylab(&quot;&quot;) 8.5 Unevenly spaced inputs (x) Let’s try the EM algorithm out with unevenly spaced inputs. set.seed(100) dt &lt;- gendat_1d(TT=100, rep(10, times=100)) dt_model &lt;- gendat_1d(TT=100, rep(100, times=100), return_model = TRUE) ylist_orig = dt %&gt;% dt2ylist() ## Two ways of removing some time points ind_rm_list = list(seq(from=10, to=100, by=10), ind_rm = 30:50) ## Try both ways, and see that the objective values objlist = list() for(ii in 1:2){ ind_rm = ind_rm_list[[ii]] x = (1:100)[-ind_rm] ylist = ylist_orig[x] set.seed(100) obj &lt;- flowtrend_once(ylist = ylist, x = x, maxdev = 100, numclust = 3, l = 2, l_prob = 2, lambda = 0.1, lambda_prob = 0.1, admm_local_adapt = TRUE, rho_init = 0.01, verbose = FALSE) objlist[[ii]] = obj } ## Plot both results (and objectives) ii = 1 ind_rm = ind_rm_list[[ii]] x = (1:100)[-ind_rm] gg1 = plot_1d(obj = objlist[[1]], ylist = ylist_orig[x], x = x) + ggtitle(&#39;Randomly missing time points&#39;) all_objectives = objlist[[1]]$objectives gg2 = data.frame(iter = 1:length(all_objectives), objectives = all_objectives) %&gt;% ggplot(aes(x = iter, y = objectives)) + geom_point() + geom_line() + theme_minimal() + ggtitle(&quot;EM objectives&quot;) ii=2 ind_rm = ind_rm_list[[ii]] x = (1:100)[-ind_rm] gg3 = plot_1d(obj = objlist[[2]], ylist = ylist_orig[x], x = x) + ggtitle(&#39;Chunks of missing time points&#39;) all_objectives = objlist[[2]]$objectives gg4 = data.frame(iter = 1:length(all_objectives), objectives = all_objectives) %&gt;% ggplot(aes(x = iter, y = objectives)) + geom_point() + geom_line() + theme_minimal() + ggtitle(&quot;EM objectives&quot;) cowplot::plot_grid(gg1, gg2, gg3, gg4) ## %&gt;% ggsave(filename = file.path(&quot;inst&quot;, ## &quot;uneven-spaced-data-objectives.png&quot;), width = 5, height = 3) (The dashed black lines are the truth, and the solid colored lines are the estimated means.) "],["helpers-for-simulations.html", "9 Helpers for simulations 9.1 Two alternative clusterings 9.2 Evaluating performance: soft RAND index 9.3 Helpers in generating pseudo-real data 9.4 Miscellaneous helpers", " 9 Helpers for simulations There are two alternatives to consider when gating points in a series of cytograms. The first alternative (underfit_gmm()) is to estimate the clusterings to be identical across all time points. This is basically equivalent to collapsing all cytograms into one and clustering them. The second alternative (overfit_gmm()) is to estimate clusters in each cytogram, then connect the cytograms as much as one can, e.g., by finding similar clusters across time points and combining them. 9.1 Two alternative clusterings #&#39; Pool the entire series of cytograms, fit a GMM model, and re-aggregate parameters. #&#39; #&#39; @param ylist Data. #&#39; @param numclust Number of clusters. #&#39; @return #&#39; #&#39; @export underfit_gmm &lt;- function(ylist, numclust){ ## Pool all data TT &lt;- length(ylist) nt &lt;- sapply(ylist, nrow) ylist_pooled &lt;- do.call(rbind, ylist) %&gt;% as_tibble() colnames(ylist_pooled) = &quot;y&quot; ## Fit the GMM model gmm_pooled &lt;- mclust::Mclust(data = ylist_pooled, G = numclust, modelNames = &quot;V&quot;, verbose = FALSE) ## Memberships (soft- and hard-clustered) hard_mem = gmm_pooled$z %&gt;% apply(1, which.max) soft_mem = gmm_pooled$z %&gt;% apply(1, function(myrow){ sample(1:2, size = 1, replace = FALSE, prob=myrow) }) ## Put together in a table tab &lt;- tibble(y = ylist_pooled$y, soft_cluster = factor(soft_mem, levels = c(2,1)), hard_cluster = factor(hard_mem, levels = c(2,1)), time = rep(1:TT, times = nt)) tab_long = tab %&gt;% pivot_longer(-c(&quot;time&quot;, &quot;y&quot;), values_to = &quot;cluster&quot;, names_to = &quot;type&quot;) ## That&#39;s it! Return the results param_mat = tibble(time=1:TT, mn1 = gmm_pooled$parameters$mean[1], mn2 = gmm_pooled$parameters$mean[2], sd1 = gmm_pooled$parameters$variance$sigmasq[1], sd2 = gmm_pooled$parameters$variance$sigmasq[2], prob1 = gmm_pooled$parameters$pro[1], prob2 = gmm_pooled$parameters$pro[2]) mu = param_mat[,c(&quot;mn1&quot;, &quot;mn2&quot;)] %&gt;% as.matrix() prob = param_mat[,c(&quot;prob1&quot;, &quot;prob2&quot;)] %&gt;% as.matrix() sigma = param_mat[,c(&quot;sd1&quot;, &quot;sd2&quot;)] %&gt;% as.matrix() ## Soft membership memlist = tab_long %&gt;% subset(type == &quot;soft_cluster&quot;) %&gt;% group_by(time) %&gt;% group_split() %&gt;% lapply(function(a)pull(a, cluster)) ## Responsibilities soft_mem = gmm_pooled$z %&gt;% as_tibble() colnames(soft_mem) = c(&quot;clust1&quot;, &quot;clust2&quot;) resp_list = soft_mem %&gt;% add_column( time = rep(1:TT, times = nt)) %&gt;% group_by(time) %&gt;% group_split() %&gt;% lapply(select, c(&quot;clust1&quot;, &quot;clust2&quot;)) %&gt;% lapply(as.matrix) ## List of sigmas TT = nrow(sigma) sigma_list = lapply(1:TT, function(tt){ one_row = sigma[tt,] %&gt;% as.numeric() one_sigma = array(NA, dim = c(2,1,1)) one_sigma[,1,1] = one_row return(one_sigma) }) return(list(tab_long = tab_long, param_mat = param_mat, mu = mu, prob = prob, sigma = sigma, sigma_list = sigma_list, memlist = memlist, resp_list = resp_list, numclust = numclust)) } #&#39; Pool the entire series of cytograms, fit a GMM model, and re-aggregate parameters. #&#39; #&#39; @param ylist Data. #&#39; @param numclust Number of clusters (only works for 2). #&#39; @return #&#39; @export overfit_gmm &lt;- function(ylist, numclust = 2, reorder = TRUE){ ## Basic checks stopifnot(numclust == 2) ## Estimate individual GMM models gmm_list &lt;- lapply(ylist, gmm_each, numclust = numclust) ## Reorder the cluster memberships sequentially if(reorder){ gmm_list = match_clusters_gmm(gmm_list, numclust = 2) } ## Obtain the resulting data in long format tab_list &lt;- lapply(gmm_list, function(a) a$tab) TT = length(ylist) names(tab_list) = 1:TT tab_long = tab_list %&gt;% bind_rows(.id = &quot;time&quot;) %&gt;% pivot_longer(-c(&quot;time&quot;, &quot;y&quot;), values_to = &quot;cluster&quot;, names_to = &quot;type&quot;) memlist = tab_long %&gt;% subset(type == &quot;soft_cluster&quot;) %&gt;% group_by(time) %&gt;% group_split() %&gt;% lapply(function(a)pull(a, cluster)) ## Get the Gaussian distribution parameters param_list &lt;- lapply(gmm_list, function(a) a$param) param_mat = param_list %&gt;% bind_rows(.id = &quot;time&quot;) mu = param_mat[,c(&quot;mn1&quot;, &quot;mn2&quot;)] %&gt;% as.matrix() prob = param_mat[,c(&quot;prob1&quot;, &quot;prob2&quot;)] %&gt;% as.matrix() sigma = param_mat[,c(&quot;sd1&quot;, &quot;sd2&quot;)] %&gt;% .^2 %&gt;% as.matrix() ## Responsibilities resp_list = gmm_list %&gt;% lapply(function(a) a$resp) ## Make a list of each time points&#39; sigmas TT = nrow(sigma) sigma_list = lapply(1:TT, function(tt){ one_row = sigma[tt,] %&gt;% as.numeric() one_sigma = array(NA, dim = c(2,1,1)) one_sigma[,1,1] = one_row return(one_sigma) }) return(list(tab_long = tab_long, memlist = memlist, param_mat = param_mat, mu = mu, prob = prob, sigma = sigma, sigma_list = sigma_list, resp_list = resp_list, numclust = numclust)) } #&#39; Reordering function for gmm results. #&#39; #&#39; @param obj result of \\code{gmm_overfit()} or \\code{gmm_underfit()}. #&#39; @param new_order new ordering of clusters to use. #&#39; #&#39; @return reordered object. #&#39; @export reorder_gmm_fit &lt;- function(obj, new_order){ ## Setup numclust = max(new_order) ## Reorder everything obj$prob = obj$prob[,new_order] obj$mu = obj$mu[,new_order] obj$sigma = obj$sigma[,new_order] obj$memlist = lapply(obj$memlist, function(a){ a_copy = a for(iclust in 1:numclust){ a_copy[which(a == iclust)] = new_order[iclust] } return(a_copy) }) ## Reorder the responsibilities obj$resp_list = lapply(obj$resp_list, function(oneresp){ return(oneresp[,new_order]) }) return(obj) } #&#39; Sequentially permute a time series of GMMs. #&#39; #&#39; @param gmm_list Output of an lapply of gmm_each over ylist. #&#39; @param numclust Number of clusters. #&#39; #&#39; @return Same format as |gmm_list| match_clusters_gmm &lt;- function(gmm_list, numclust = 2){ ## Initialize some objects TT &lt;- length(gmm_list) ## Helper to permute one GMM my_reorder &lt;- function(one_gmm, new_order){ one_gmm_reordered = one_gmm one_gmm_reordered$tab = one_gmm_reordered$tab %&gt;% mutate(hard_cluster = hard_cluster %&gt;% plyr::revalue(replace=c(&quot;1&quot;=new_order[1],&quot;2&quot;=new_order[2]))) %&gt;% mutate(soft_cluster = soft_cluster %&gt;% plyr::revalue(replace=c(&quot;1&quot;=new_order[1],&quot;2&quot;=new_order[2]))) ## reorder the parameters param = one_gmm$param new_mns = param[,c(&quot;mn1&quot;, &quot;mn2&quot;)][new_order] %&gt;% as.numeric() new_sds = param[,c(&quot;sd1&quot;, &quot;sd2&quot;)][new_order] %&gt;% as.numeric() new_probs = param[,c(&quot;prob1&quot;, &quot;prob2&quot;)][new_order] %&gt;% as.numeric() one_gmm_reordered$param &lt;- data.frame(mn1 = new_mns[1], mn2 = new_mns[2], sd1 = new_sds[1], sd2 = new_sds[2], prob1 = new_probs[1], prob2 = new_probs[2]) ## if(all(new_order == 2:1)) browser() ## reorder the responsibilities one_gmm_reordered$resp = one_gmm$resp[,new_order] return(one_gmm_reordered) } ## Fill in the rest of the rows by sequentially matching clusters from t-1 to ## t, using the Hungarian Algorithm new_orders = matrix(NA, ncol = 2, nrow = TT) new_orders[1,] = c(1, 2) gmm_list_copy = gmm_list for(tt in 2:TT){ ## Find order param1 = gmm_list_copy[[tt-1]] %&gt;% .$param param2 = gmm_list[[tt]] %&gt;% .$param klmat &lt;- symmetric_kl_between_gaussians(param1 = param1, param2 = param2) new_order &lt;- RcppHungarian::HungarianSolver(klmat) %&gt;% .$pairs %&gt;% data.frame() %&gt;% arrange(.[,2]) %&gt;% .[,1] new_orders[tt,] = new_order ## Perform the reordering gmm_list_copy[[tt]] &lt;- my_reorder(gmm_list[[tt]], new_order) } return(gmm_list_copy) } #&#39; Between two sets of Gaussian mean/sd parameters, what is the symmetric KL #&#39; distance. #&#39; @param param1 list of mn1, mn2, sd1, sd2 #&#39; @param param2 another such list. #&#39; #&#39; @return (numclust x numclust) distance matrix. symmetric_kl_between_gaussians &lt;- function(param1, param2, numclust = 2){ ## First model&#39;s parameters mn_model1 = c(param1$mn1, param1$mn2) sd_model1 = c(param1$sd1, param1$sd2) ## Second model&#39;s parameters mn_model2 = c(param2$mn1, param2$mn2) sd_model2 = c(param2$sd1, param2$sd2) ## Fill out distance matrix dist_cs &lt;- matrix(0, ncol = numclust, nrow = numclust) for(iclust_1 in 1:numclust){ for(iclust_2 in 1:numclust){ dist_cs[iclust_2, iclust_1] &lt;- one_symmetric_kl(mu1 = mn_model1[iclust_1], mu2 = mn_model2[iclust_2], sd1 = sd_model1[iclust_1], sd2 = sd_model2[iclust_2]) } } rownames(dist_cs) &lt;- rep(&quot;C1&quot;, numclust) colnames(dist_cs) &lt;- rep(&quot;C2&quot;, numclust) return(dist_cs) } #&#39; Symmetric KL divergence between two Gaussian distributions. #&#39; @param mu1 Mean for distribution 1. #&#39; @param mu2 Mean for distribution 2. #&#39; @param sd1 Standard deviation for distribution 1. #&#39; @param sd2 Standard deviation for distribution 2. one_symmetric_kl &lt;- function(mu1, mu2, sd1, sd2){ stopifnot(length(mu1)==1 &amp; length(mu2) == 1 &amp; length(sd1) == 1 &amp; length(sd2) == 1) kl12 &lt;- log(sd2/sd1) + (sd1^2 + (mu1 - mu2)^2)/(2*sd2^2) - 1/2 kl21 &lt;- log(sd1/sd2) + (sd2^2 + (mu2 - mu1)^2)/(2*sd1^2) - 1/2 kl_sym &lt;- 0.5*(kl12 + kl21) return(kl_sym) } #&#39; Apply a gmm in each 1-dimensional cytogram. #&#39; #&#39; @param one_y One-column matrix. #&#39; @param numclust Number of clusters. #&#39; @return #&#39; @export gmm_each &lt;- function(one_y, numclust){ ## Basic check stopifnot(ncol(one_y) == 1) ## Do Gaussian Mixture model ## obj &lt;- mclust::Mclust(data = one_y, G = numclust, ## modelNames = &quot;V&quot;, verbose = FALSE) obj &lt;- my_mclust(one_y, numclust, FALSE) ## Memberships (soft- and hard-clustered) hard_mem = obj$z %&gt;% apply(1, which.max) soft_mem = obj$z %&gt;% apply(1, function(myrow){ sample(1:2, size = 1, replace = FALSE, prob=myrow) }) tab &lt;- tibble(y = as.numeric(one_y), soft_cluster = factor(soft_mem, levels = c(1,2)), hard_cluster = factor(hard_mem, levels = c(1,2))) ## parameter table mn1 = obj$parameters$mean[[1]] mn2 = obj$parameters$mean[[2]] sds = obj %&gt;% .$parameters %&gt;% .$variance %&gt;% .$sigmasq %&gt;% sqrt() sd1 = sds[1] sd2 = sds[2] prob1 = obj$parameters$pro[1] prob2 = obj$parameters$pro[2] ## ggplot(data.frame(x=as.numeric(one_y))) + ## geom_histogram(aes(x=x)) + ## geom_vline(xintercept = mn1, col = &#39;blue&#39;) + ## geom_vline(xintercept = mn1 + 2*sd1, col = &#39;skyblue&#39;) + ## geom_vline(xintercept = mn1 - 2*sd1, col = &#39;skyblue&#39;) + ## geom_vline(xintercept = mn2, col = &#39;red&#39;) + ## geom_vline(xintercept = mn2 - 2*sd2, col = &#39;orange&#39;) + ## geom_vline(xintercept = mn2 + 2*sd2, col = &#39;orange&#39;) param = tibble(mn1 = mn1, mn2 = mn2, sd1 = sd1, sd2 = sd2, prob1 = prob1, prob2 = prob2) resp = obj$z return(list(tab = tab, param = param, resp = resp))##, sigma_list = sigma_list)) } Here’s a function for soft-gating given a 2-column responsibility matrix. #&#39; Soft-gates a responsibility matrix by a bernoulli (or multinoulli) draw. #&#39; #&#39; @param oneresp A 2-column responsibility matrix #&#39; #&#39; @return #&#39; @export soft_gate_one_responsibility_matrix &lt;- function(oneresp){ ## Setup numclust = ncol(oneresp) vec = rep(0,numclust) ## Draw the 0-1 memberships zero_one_mat = apply(oneresp, 1, function(myrow){ draw = sample(1:numclust, size=1, prob=myrow) vec[draw]= 1 vec }) %&gt;% t() ## Check dimensions and return stopifnot(all(dim(zero_one_mat) == dim(oneresp))) return(zero_one_mat) } Here’s a useful function to create a flowtrend-like object but containing the “oracle” information about the cluster means, variances, and mixture probabilities. #&#39; Reformatting |datobj| to create a flowtrend-like object that contains #&#39; &quot;oracle&quot; information of the model that #&#39; generates the simulated pseudo-real data. #&#39; #&#39; @param datobj A data object. #&#39; #&#39; @return A flowtrend-like object containing mn, sigma, and prob. #&#39; #&#39; @export create_oracle &lt;- function(datobj){ TT = nrow(datobj$mns) dimdat = 1 numclust = ncol(datobj$mns) ## Make a fake object in a similar format as a |flowtrend| object fake_obj = list() fake_obj$mn = array(NA, dim = c(TT, dimdat,numclust)) fake_obj$mn[,1,] = datobj$mns fake_obj$prob = datobj$prob fake_obj$sigma = array(NA, dim = c(numclust, dimdat, dimdat)) fake_obj$sigma[1,1,1] = datobj$sd1^2 fake_obj$sigma[2,1,1] = datobj$sd2^2 fake_obj$numclust = numclust return(fake_obj) } 9.2 Evaluating performance: soft RAND index We will use a “soft” Rand index that replaces the regular Rand index: \\[ \\sum_{i, i&#39;} 1\\{ \\hat C_i = \\hat C_{i&#39;}, C_i^* \\neq C_{i&#39;}^*\\}.\\] which measures, for every pair of points \\(i\\) and \\(i&#39;\\), the number of times that the two clustering mechanisms disagree. Now, let’s say that the two mechanisms give probabilities: \\[\\hat \\gamma_{ik} = \\hat P(\\hat C_i = k),\\] \\[\\hat \\gamma^*_{ik} = \\hat P(\\hat C_i^* = k).\\] Then, the probability that the clustering is the same \\(P(C_i^* = C_{i&#39;}^*)\\) for the pair of points \\(i\\) and \\(i&#39;\\) is: \\[ (\\gamma_i^*)^T (\\gamma_{i&#39;}^*) = \\sum_{k=1} P(C_i = k) P(C_i^* = k) = P(C_i^* = C_{i&#39;}^*).\\] and the probaility they are different is: \\[ (\\gamma_i^*)^T (\\gamma_{i&#39;}^*) = \\sum_{k=1} P(C_i = k) P(C_i^* = k) = P(C_i^* = C_{i&#39;}^*).\\] So, we can measure the difference as: \\[\\sum_{i,i&#39;} (\\hat \\gamma_i^T \\hat \\gamma_{i&#39;})\\cdot(1- \\gamma^*_i^T \\gamma^*_{i&#39;}) \\] And when one of the clusterings is soft, we can still use a 0-1 vector as \\(\\gamma\\). #&#39; A &quot;soft&quot; version of a rand index between two sets of responsibility #&#39; (membership probability) matrices. Measures the /disagreement/ between the two clusterings. #&#39; #&#39; @param resp_list1 One list of responsibility matrices. #&#39; @param resp_list2 Another list of responsibility matrices. #&#39; @param times Optional; if you would like to isolate your attention to some specific times. #&#39; #&#39; @return A single soft rand index number #&#39; @export rand_old &lt;- function(resp_list1, resp_list2, times = NULL, prop = .25){ if(!is.null(times)) resp_list1 = resp_list1[times] if(!is.null(times)) resp_list2 = resp_list2[times] rand_onetime &lt;- function(resp1, resp2){ stopifnot(nrow(resp1) == nrow(resp2)) stopifnot(ncol(resp1) == ncol(resp2)) nt = nrow(resp1) ## Form the score mat11 = resp1 %*% t(resp1) mat22 = resp2 %*% t(resp2) mat11not = (1-mat11) mat22not = (1-mat22) ## Make the diagonals not matter anywhere diag(mat11) = 0 diag(mat22) = 0 diag(mat11not) = 0 diag(mat22not) = 0 a = mat11 * mat22 b = mat11not * mat22not c = mat11 * mat22not d = mat11not * mat22 return(c(a = sum(a), b = sum(b), c = sum(c), d = sum(d))) } abcd_over_time = mapply(rand_onetime, resp_list1, resp_list2) abcdmat = abcd_over_time %&gt;% t() abcd = abcdmat %&gt;% colSums() score = (abcd[&quot;a&quot;] + abcd[&quot;b&quot;])/ (sum(abcd)) return(score) } #&#39; Rand index between responsibilities. #&#39; #&#39; @param resp_list1 One list. #&#39; @param resp_list2 Another list. #&#39; @param times A subset of the time points (out of 1:length(resp_list1)) to #&#39; examine. #&#39; @param smaller If TRUE, use only a small (sampled) subset of the particles&#39; #&#39; responsibilities for calculating RAND. #&#39; @param prop How much to downsample; defaults to 0.1. #&#39; @export rand &lt;- function(resp_list1, resp_list2, times = NULL, smaller = TRUE, prop = 0.1){ ## Basic checks stopifnot(all(sapply(resp_list1, nrow) == sapply(resp_list2, nrow))) ## Subset the times if needed if(!is.null(times)) resp_list1 = resp_list1[times] if(!is.null(times)) resp_list2 = resp_list2[times] ## names(everything) ## list2env(everything,envir = environment()) ## resp_list1 = resp_oracle ## resp_list2 = true_resp if(smaller){ indslist = lapply(resp_list1, function(oneresp){ inds = sample(x=1:nrow(oneresp), size=ceiling(nrow(oneresp)*prop), replace=FALSE) %&gt;% sort() }) resp_list1 = mapply(function(a,b)a[b,,drop=FALSE], resp_list1, indslist, SIMPLIFY=FALSE) resp_list2 = mapply(function(a,b)a[b,,drop=FALSE], resp_list2, indslist, SIMPLIFY=FALSE) } ## Make each list into one long matrix resp1 = Reduce(rbind, resp_list1) resp2 = Reduce(rbind, resp_list2) ## resp1 = do.call(rbind, resp_list1)## %&gt;% bind_rows() ## resp2 = do.call(rbind, resp_list2)## %&gt;% bind_rows() stopifnot(nrow(resp1) == nrow(resp2)) stopifnot(ncol(resp1) == ncol(resp2)) nt = nrow(resp1) ## Form the score mat11 = resp1 %*% t(resp1) mat22 = resp2 %*% t(resp2) mat11not = (1-mat11) mat22not = (1-mat22) ## Make the diagonals not matter anywhere diag(mat11) = 0 diag(mat22) = 0 diag(mat11not) = 0 diag(mat22not) = 0 a = mat11 * mat22 b = mat11not * mat22not c = mat11 * mat22not d = mat11not * mat22 abcd = c(a = sum(a), b = sum(b), c = sum(c), d = sum(d)) score = (abcd[&quot;a&quot;] + abcd[&quot;b&quot;])/ (sum(abcd)) return(score) } #&#39; 2 x 2 contigency table from membership vectors. #&#39; #&#39; @param mem1 Membership vector. #&#39; @param mem2 Another membership vector. #&#39; #&#39; @return A 3x3 matrix containing (1) a 2 x 2 table in the first [1:2,1:2] #&#39; entries, and (2) row sums and column sums and total sums in the [,3], [3,] #&#39; entries. #&#39; @export make_contingency_table&lt;-function(mem1, mem2){ tab = table(mem1, mem2) tab = cbind(tab, rowSums(tab)) tab = rbind(tab, colSums(tab)) return(tab) } #&#39; Calculate RAND index from a contingency table. #&#39; #&#39; @param tab A matrix containing a 2 x 2 table in the first [1:2,1:2] #&#39; entries; e.g., from make_contingency_table(). #&#39; #&#39; @return Rand index. #&#39; @export get_rand_from_table &lt;- function(tab){ numer = sum(sapply(as.numeric(tab), function(nij) choose(nij, 2))) + tab[1,1] * tab[2,2] + tab[1,2] * tab[2,1] denom = (choose(sum(tab), 2)) ri = numer/denom return(ri) } #&#39; Faster rand index calculation from membership vectors. #&#39; #&#39; @param mem1 #&#39; @param mem2 #&#39; #&#39; @return RAND index. #&#39; @export rand_from_mems &lt;- function(mem1, mem2){ make_contingency_table(mem1, mem2) %&gt;% .[1:2,1:2] %&gt;% get_rand_from_table() } It’s also helpful to have a function that takes a list of discrete memberships (integers 1,..,K) and convert it to a list of responsibility matrices similar in format to obj$resp of a flowtrend object obj. #&#39; Convert list of memberships into a list of responsibilities. #&#39; #&#39; @param memlist List of memberships #&#39; #&#39; @return #&#39; @export memlist_to_respmat &lt;- function(memlist){ numclust = max(sapply(memlist, max)) respmats = lapply(memlist, function(mem){ respmat = lapply(mem, function(onemem){ onerow = rep(0, numclust)##c(0,0) onerow[onemem] = 1 return(onerow) }) %&gt;% do.call(rbind, .) return(respmat) }) return(respmats) } 9.3 Helpers in generating pseudo-real data Two clusters will be taken from an estimated flowtrend model fit on real data, downloaded from here: https://zenodo.org/records/6471995 to ./inst/data. 9.4 Miscellaneous helpers Lastly, there are some miscellaneous helpers that are useful when running actual jobs on a server. The first one is create_destin(), which creates a directory if it doesn’t already exist. #&#39; Creates a directory \\code{destin}, if it doesn&#39;t already exist. #&#39; #&#39; @param destin Destination directory. #&#39; #&#39; @return Nothing. #&#39; @export create_destin &lt;- function(destin){ if(!dir.exists(destin)){ dir.create(destin, recursive = TRUE) cat(&quot;Creating destin: &quot;, destin, fill=TRUE) } else { cat(&quot;All output goes out to destin: &quot;, destin, fill = TRUE) } } Another helper parse_args() helps R read in trailing arguments from the command line, like this: parse_args(args = commandArgs(trailingOnly = TRUE), verbose=TRUE) so that one can run jobs using a SLURM command such as: sbatch --export=summ=0 --array=1 run-3dreal.slurm from which the R script can access the summ=0 variable. #&#39; Parse command line arguments and assigns the values of them. |args| is meant #&#39; to just be additional command line arguments. #&#39; #&#39; @param args Argument. #&#39; #&#39; @return Nothing. #&#39; @export parse_args &lt;- function(args, verbose=FALSE){ args = sapply(args, strsplit, &quot;=&quot;) print(args) for(arg in args){ ## Check if the thing is integer all_numbers = str_detect(arg[2], &quot;^[:digit:]+$&quot;) ## Assign the variable if(all_numbers){ assign(arg[1], as.numeric(arg[2]), inherits = TRUE) } else { assign(arg[1], arg[2], inherits = TRUE) } if(verbose){ cat(arg[1], &quot;takes the value of&quot;, arg[2], &quot;from command line input&quot;, fill=TRUE) } print(&quot;===============================&quot;) } } "],["documenting-the-package-and-building-1.html", "10 Documenting the package and building", " 10 Documenting the package and building We finish by running commands that will document, build, and install the package. It may also be a good idea to check the package from within this file. litr::document() # &lt;-- use instead of devtools::document() ## ℹ Updating flowtrend documentation ## ℹ Loading flowtrend ## ℹ Re-compiling flowtrend (debug build) ## ## ✖ Estep.R:5: @param requires two parts: an argument name and a description. ## ✖ Estep.R:7: @param requires two parts: an argument name and a description. ## ✖ Estep.R:8: @param requires two parts: an argument name and a description. ## ✖ Estep.R:9: @param requires two parts: an argument name and a description. ## ✖ Estep.R:10: @param requires two parts: an argument name and a description. ## ✖ Estep.R:11: @param requires two parts: an argument name and a description. ## ✖ Estep.R:12: @param requires two parts: an argument name and a description. ## ✖ Estep.R:17: @return requires a value. ## ✖ Mstep_mu.R:7: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:8: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:9: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:10: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:11: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:12: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:13: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:14: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:15: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:16: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:17: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:18: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:19: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:20: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:21: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:22: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:23: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:24: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:25: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:26: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:27: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:28: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:29: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:30: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu.R:33: @return requires a value. ## ✖ Mstep_mu.R:36: @examples requires a value. ## ✖ Mstep_mu_cvxr.R:5: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu_cvxr.R:6: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu_cvxr.R:7: @param requires two parts: an argument name and a description. ## ✖ Mstep_mu_cvxr.R:8: @param requires two parts: an argument name and a description. ## ✖ Mstep_sigma.R:13: @examples requires a value. ## ✖ etilde_mat.R:5: @param requires two parts: an argument name and a description. ## ✖ flowtrend.R:8: @return requires a value. ## ✖ flowtrend.R:11: @examples requires a value. ## ✖ flowtrend_once.R:22: @param requires two parts: an argument name and a description. ## ✖ flowtrend_once.R:23: @param requires two parts: an argument name and a description. ## ✖ flowtrend_once.R:24: @param requires two parts: an argument name and a description. ## ✖ flowtrend_once.R:25: @param requires two parts: an argument name and a description. ## ✖ flowtrend_once.R:31: @examples requires a value. ## ✖ gen_diff_mat.R:12: @examples requires a value. ## ✖ get_AB_mats.R:5: @param requires two parts: an argument name and a description. ## ✖ get_C_mat.R:4: @param requires two parts: an argument name and a description. ## ✖ la_admm_oneclust.R:5: @param requires two parts: an argument name and a description. ## ✖ la_admm_oneclust.R:8: @return requires a value. ## ✖ la_admm_oneclust.R:10: @examples requires a value. ## ✖ make_cvscore_filename.R:4: @param requires two parts: an argument name and a description. ## ✖ make_cvscore_filename.R:5: @param requires two parts: an argument name and a description. ## ✖ make_cvscore_filename.R:6: @param requires two parts: an argument name and a description. ## ✖ make_cvscore_filename.R:16: @param requires two parts: an argument name and a description. ## ✖ make_cvscore_filename.R:17: @param requires two parts: an argument name and a description. ## ✖ make_cvscore_filename.R:18: @param requires two parts: an argument name and a description. ## ✖ make_cvscore_filename.R:30: @param requires two parts: an argument name and a description. ## ✖ make_cvscore_filename.R:31: @param requires two parts: an argument name and a description. ## ✖ make_cvscore_filename.R:42: @param requires two parts: an argument name and a description. ## ✖ match_clusters_gmm.R:108: @return requires a value. ## ✖ memlist_to_respmat.R:7: @return requires a value. ## ✖ objective.R:5: @param requires two parts: an argument name and a description. ## ✖ objective.R:6: @param requires two parts: an argument name and a description. ## ✖ objective.R:7: @param requires two parts: an argument name and a description. ## ✖ objective.R:8: @param requires two parts: an argument name and a description. ## ✖ objective.R:9: @param requires two parts: an argument name and a description. ## ✖ objective.R:10: @param requires two parts: an argument name and a description. ## ✖ objective.R:11: @param requires two parts: an argument name and a description. ## ✖ objective.R:12: @param requires two parts: an argument name and a description. ## ✖ objective.R:13: @param requires two parts: an argument name and a description. ## ✖ objective.R:14: @param requires two parts: an argument name and a description. ## ✖ objective.R:15: @param requires two parts: an argument name and a description. ## ✖ objective.R:16: @param requires two parts: an argument name and a description. ## ✖ objective.R:17: @param requires two parts: an argument name and a description. ## ✖ objective.R:18: @param requires two parts: an argument name and a description. ## ✖ objective.R:21: @return requires a value. ## ✖ objective.R:24: @examples requires a value. ## ✖ overfit_gmm.R:7: @return requires a value. ## ✖ plot_3d.R:15: @return requires a value. ## ✖ rand_old.R:148: @param requires two parts: an argument name and a description. ## ✖ soft_gate_one_responsibility_matrix.R:7: @return requires a value. ## ✖ underfit_gmm.R:7: @return requires a value. ## ✖ In topic &#39;U_update_W.Rd&#39;: Skipping; no name and/or title. ## ✖ In topic &#39;U_update_Z.Rd&#39;: Skipping; no name and/or title. ## ✖ In topic &#39;myschur.Rd&#39;: Skipping; no name and/or title. ## Writing &#39;NAMESPACE&#39; ## Writing &#39;Estep.Rd&#39; ## Writing &#39;Mstep_mu.Rd&#39; ## Writing &#39;Mstep_mu_cvxr.Rd&#39; ## Writing &#39;Mstep_prob.Rd&#39; ## Writing &#39;Mstep_sigma.Rd&#39; ## Writing &#39;matrix_function_solve_triangular_sylvester_barebonesC2.Rd&#39; ## Writing &#39;W_update_fused.Rd&#39; ## Writing &#39;projCmat.Rd&#39; ## Writing &#39;admm_oneclust.Rd&#39; ## Writing &#39;aug_lagr.Rd&#39; ## Writing &#39;calc_max_lambda.Rd&#39; ## Writing &#39;check_converge.Rd&#39; ## Writing &#39;check_converge_rel.Rd&#39; ## Writing &#39;create_destin.Rd&#39; ## Writing &#39;create_oracle.Rd&#39; ## Writing &#39;cv_aggregate.Rd&#39; ## Writing &#39;cv_aggregate_res.Rd&#39; ## Writing &#39;cv_flowtrend.Rd&#39; ## Writing &#39;cv_makebest.Rd&#39; ## Writing &#39;load_all_objectives.Rd&#39; ## Writing &#39;load_all_refit_objectives.Rd&#39; ## Writing &#39;keep_only_best.Rd&#39; ## Writing &#39;keep_only_best_refit.Rd&#39; ## Writing &#39;cv_summary.Rd&#39; ## Writing &#39;dt2ylist.Rd&#39; ## Writing &#39;etilde_mat.Rd&#39; ## Writing &#39;flowtrend-package.Rd&#39; ## Writing &#39;flowtrend.Rd&#39; ## Writing &#39;flowtrend_once.Rd&#39; ## Writing &#39;form_symmetric_kl_distmat.Rd&#39; ## Writing &#39;gen_diff_mat.Rd&#39; ## Writing &#39;gen_tf_mat.Rd&#39; ## Writing &#39;gen_tf_mat_equalspace.Rd&#39; ## Writing &#39;gendat_1d.Rd&#39; ## Writing &#39;gendat_2d.Rd&#39; ## Writing &#39;gendat_3d.Rd&#39; ## Writing &#39;get_AB_mats.Rd&#39; ## Writing &#39;get_C_mat.Rd&#39; ## Writing &#39;get_best_match_from_kl.Rd&#39; ## Writing &#39;get_max_lambda.Rd&#39; ## Writing &#39;init_mn.Rd&#39; ## Writing &#39;init_sigma.Rd&#39; ## Writing &#39;interpolate_mn.Rd&#39; ## Writing &#39;interpolate_prob.Rd&#39; ## Writing &#39;la_admm_oneclust.Rd&#39; ## Writing &#39;loglik_tt.Rd&#39; ## Writing &#39;logspace.Rd&#39; ## Writing &#39;make_cv_folds.Rd&#39; ## Writing &#39;make_cv_folds_in_blocks.Rd&#39; ## Writing &#39;make_cvscore_filename.Rd&#39; ## Writing &#39;make_best_cvscore_filename.Rd&#39; ## Writing &#39;make_refit_filename.Rd&#39; ## Writing &#39;make_best_refit_filename.Rd&#39; ## Writing &#39;make_iilist.Rd&#39; ## Writing &#39;make_iimat.Rd&#39; ## Writing &#39;make_iimat_small.Rd&#39; ## Writing &#39;match_clusters_gmm.Rd&#39; ## Writing &#39;symmetric_kl_between_gaussians.Rd&#39; ## Writing &#39;one_symmetric_kl.Rd&#39; ## Writing &#39;gmm_each.Rd&#39; ## Writing &#39;memlist_to_respmat.Rd&#39; ## Writing &#39;my_mfrow.Rd&#39; ## Writing &#39;objective.Rd&#39; ## Writing &#39;objective_per_cluster.Rd&#39; ## Writing &#39;one_job.Rd&#39; ## Writing &#39;one_job_refit.Rd&#39; ## Writing &#39;overfit_gmm.Rd&#39; ## Writing &#39;parse_args.Rd&#39; ## Writing &#39;plot_1d.Rd&#39; ## Writing &#39;plot_1d_add_model.Rd&#39; ## Writing &#39;plot_1d_with_membership.Rd&#39; ## Writing &#39;plot_2d.Rd&#39; ## Writing &#39;plot_3d.Rd&#39; ## Writing &#39;plot_prob.Rd&#39; ## Writing &#39;predict_flowtrend.Rd&#39; ## Writing &#39;print_progress.Rd&#39; ## Writing &#39;rand_old.Rd&#39; ## Writing &#39;rand.Rd&#39; ## Writing &#39;make_contingency_table.Rd&#39; ## Writing &#39;get_rand_from_table.Rd&#39; ## Writing &#39;rand_from_mems.Rd&#39; ## Writing &#39;reorder_clust.Rd&#39; ## Writing &#39;reorder_gmm_fit.Rd&#39; ## Writing &#39;reorder_kl.Rd&#39; ## Writing &#39;soft_gate_one_responsibility_matrix.Rd&#39; ## Writing &#39;softmax.Rd&#39; ## Writing &#39;symmetric_kl.Rd&#39; ## Writing &#39;underfit_gmm.Rd&#39; ## Writing &#39;pipe.Rd&#39; ## Temporarily here; This tricks litr::render(minimal_eval=TRUE) into running this code. litr::add_readme(&quot;../setup/readme-for-r-package.Rmd&quot;) ## adds readme to R package ## ✔ Writing &#39;README.Rmd&#39;. ## ✔ Adding &quot;^README\\\\.Rmd$&quot; to &#39;.Rbuildignore&#39;. ## ✔ Creating &#39;.git/hooks/&#39;. ## ✔ Writing &#39;.git/hooks/pre-commit&#39;. litr::add_pkgdown(&quot;../setup/_pkgdown.yml&quot;) ## adds pkgdown site ## ✔ Adding &quot;^_pkgdown\\\\.yml$&quot;, &quot;^docs$&quot;, and &quot;^pkgdown$&quot; to &#39;.Rbuildignore&#39;. ## Warning in readLines(con, warn = readLines.warn): incomplete final line found on &#39;./_pkgdown.yml&#39; ## ── Installing package flowtrend into temporary library ─────────────────────────────────────────────────────────────────────── ## ── Finished building pkgdown site for package flowtrend ────────────────────────────────────────────────────────────────────── "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
